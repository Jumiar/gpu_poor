{"NousResearch/Llama-2-13b-hf": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "google/flan-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "google/flan-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "PascalNotin/Tranception_Small": {"MSA_end": null, "MSA_filename": null, "MSA_start": null, "MSA_weight_file_name": null, "_name_or_path": "Tranception_Small", "activation_function": "squared_relu", "architectures": ["TranceptionLMHeadModel"], "attention_mode": "tranception", "attn_pdrop": 0.1, "bos_token_id": 1, "clustal_omega_location": null, "embd_pdrop": 0.1, "eos_token_id": 2, "full_protein_length": null, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "local_batch_size": 8, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12, "n_positions": 1024, "position_embedding": "grouped_alibi", "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "retrieval_aggregation_mode": null, "retrieval_inference_weight": 0.6, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "scoring_window": "optimal", "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "tokenizer": null, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 25}, "bigscience/bloom-560m": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "architectures": ["BloomForCausalLM"], "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 1024, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 1, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "distilgpt2": {"_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 6, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "hf-internal-testing/tiny-random-gpt2": {"activation_function": "gelu_new", "attention_probs_dropout_prob": 0.1, "attn_pdrop": 0.1, "bos_token_id": 98, "embd_pdrop": 0.1, "eos_token_id": 98, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "initializer_range": 0.02, "intermediate_size": 37, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 512, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5, "n_positions": 512, "pad_token_id": 98, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.11.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1000}, "tiiuae/falcon-7b": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 65024}, "t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "bigscience/bloomz-1b1": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "architectures": ["BloomForCausalLM"], "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 1536, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 1, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "gpt2-medium": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "mrm8488/t5-base-finetuned-common_gen": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "total_flos": 1903112143994880, "vocab_size": 32128}, "lmsys/fastchat-t5-3b-v1.0": {"_name_or_path": "flant5_3b_fp16", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32110}, "gpt2-xl": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1600, "n_head": 25, "n_layer": 48, "n_positions": 1024, "output_past": true, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "meta-llama/Llama-2-7b-hf": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "hf-internal-testing/tiny-random-t5": {"bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 0.002, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_num_buckets": 8, "transformers_version": "4.11.0.dev0", "use_cache": true, "vocab_size": 1103}, "EleutherAI/pythia-6.9b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "databricks/dolly-v2-3b": {"_name_or_path": "EleutherAI/pythia-2.8b", "architectures": ["GPTNeoXForCausalLM"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.25.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50280}, "hf-internal-testing/tiny-random-GPTNeoXForCausalLM": {"architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.1, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 32, "initializer_range": 0.02, "intermediate_size": 37, "is_decoder": true, "layer_norm_eps": 1e-05, "max_position_embeddings": 512, "model_type": "gpt_neox", "num_attention_heads": 4, "num_hidden_layers": 5, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "use_parallel_residual": true, "vocab_size": 1024}, "meta-llama/Llama-2-7b-chat-hf": {"_name_or_path": "meta-llama/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "microsoft/DialoGPT-medium": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_layer": 24, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "vocab_size": 50257}, "google/mt5-base": {"_name_or_path": "/home/patrick/hugging_face/t5/mt5-base", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 250112}, "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ": {"_name_or_path": "/workspace/models/llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "hf-internal-testing/tiny-random-BloomModel": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomModel"], "attention_dropout": 0.1, "bos_token_id": 1, "dtype": "float32", "eos_token_id": 2, "gradient_checkpointing": false, "hidden_dropout": 0.1, "hidden_size": 32, "id2label": {"0": "LABEL_0", "1": "LABEL_1", "2": "LABEL_2"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2}, "layer_norm_epsilon": 1e-05, "model_type": "bloom", "n_head": 4, "n_layer": 5, "n_positions": 512, "pad_token_id": 3, "pretraining_tp": 1, "seq_length": 7, "slow_but_exact": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "google/flan-t5-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.24.0.dev0", "use_cache": true, "vocab_size": 32128}, "lmsys/vicuna-7b-v1.1": {"_name_or_path": "/content/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "huggyllama/llama-7b": {"_name_or_path": "/home/sgugger/tmp/llama/llama-7b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "mrm8488/t5-base-finetuned-summarize-news": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "google/flan-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "tiiuae/falcon-40b-instruct": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.26.0", "use_cache": true, "vocab_size": 65024}, "ramsrigouthamg/t5_sentence_paraphraser": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "use_cache": true, "vocab_size": 32128}, "flexudy/t5-base-multi-sentence-doctor": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "lewtun/tiny-random-mt5": {"_name_or_path": "tiny-model", "architectures": ["MT5Model"], "d_ff": 1024, "d_kv": 4, "d_model": 16, "decoder_attention_heads": 4, "decoder_ffn_dim": 4, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 4, "num_layers": 2, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.22.0.dev0", "use_cache": true, "vocab_size": 250112}, "gpt2-large": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_layer": 36, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "valhalla/t5-base-e2e-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_de": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_fr": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_ro": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}}, "vocab_size": 32102}, "sshleifer/tiny-gpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 2, "n_head": 2, "n_layer": 2, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "fxmarty/tiny-llama-fast-tokenizer": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 16, "initializer_range": 0.02, "intermediate_size": 64, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 2, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "decapoda-research/llama-7b-hf": {"architectures": ["LLaMAForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "intermediate_size": 11008, "initializer_range": 0.02, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "stabilityai/StableBeluga2": {"_name_or_path": "/fsx/dakota/orca/tmp_orca", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "syzymon/long_llama_3b": {"architectures": ["LongLlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "longllama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_longllama.LongLlamaConfig", "AutoModel": "modeling_longllama.LongLlamaModel", "AutoModelForCausalLM": "modeling_longllama.LongLlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_longllama.LongLlamaForSequenceClassification"}, "mem_dtype": "bfloat16", "mem_positionals": true, "mem_layers": [6, 12, 18]}, "NousResearch/Llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/Llama-2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "tiiuae/falcon-7b-instruct": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 65024}, "google/flan-t5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.24.0.dev0", "use_cache": true, "vocab_size": 32128}, "meta-llama/Llama-2-13b-chat-hf": {"_name_or_path": null, "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "petals-team/StableBeluga2": {"_name_or_path": "/fsx/dakota/orca/tmp_orca", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "meta-llama/Llama-2-70b-chat-hf": {"_name_or_path": "meta-llama/Llama-2-70b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "EleutherAI/gpt-neox-20b": {"architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.19.0.dev0", "use_cache": true, "vocab_size": 50432}, "hf-internal-testing/tiny-random-GPTBigCodeForCausalLM": {"activation_function": "relu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": false, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "is_decoder": true, "layer_norm_epsilon": 1e-05, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1021, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": false, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "nferruz/ProtGPT2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 50257}, "philschmid/flan-t5-xxl-sharded-fp16": {"_name_or_path": "google/flan-t5-xxl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dense_act_fn": "gelu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.25.0.dev0", "use_cache": true, "vocab_size": 32128}, "HuggingFaceM4/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 16, "initializer_range": 0.02, "intermediate_size": 64, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 2, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "hf-internal-testing/tiny-random-BloomForCausalLM": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 1, "dtype": "float32", "eos_token_id": 2, "gradient_checkpointing": false, "hidden_dropout": 0.1, "hidden_size": 32, "id2label": {"0": "LABEL_0", "1": "LABEL_1", "2": "LABEL_2"}, "initializer_range": 0.02, "is_decoder": true, "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2}, "layer_norm_epsilon": 1e-05, "model_type": "bloom", "n_head": 4, "n_layer": 5, "n_positions": 512, "pad_token_id": 3, "pretraining_tp": 1, "seq_length": 7, "slow_but_exact": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "hf-internal-testing/tiny-random-GPT2LMHeadModel": {"_name_or_path": "temp/dummy/gpt2/GPT2LMHeadModel", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "is_decoder": true, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "Vamsi/T5_Paraphrase_Paws": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "lmsys/vicuna-7b-v1.3": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "meta-llama/Llama-2-13b-hf": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "ybelkada/tiny-random-T5ForConditionalGeneration-calibrated": {"_name_or_path": "hf-internal-testing/tiny-random-T5ForConditionalGeneration", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.2, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32100}, "prithivida/parrot_paraphraser_on_T5": {"_name_or_path": "paraphrase/checkpoint-19329", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.5.1", "use_cache": true, "vocab_size": 32128}, "hf-internal-testing/tiny-random-GPTBigCodeModel": {"activation_function": "relu", "architectures": ["GPTBigCodeModel"], "attention_softmax_in_fp32": false, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1021, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": false, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "hkunlp/instructor-large": {"_name_or_path": "/scratch/acd13578qu/metatrain_models/enhanced_large/checkpoint-300/", "architectures": ["T5EncoderModel"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.20.0.dev0", "use_cache": true, "vocab_size": 32128}, "lmsys/vicuna-7b-v1.5": {"_name_or_path": "vicuna-7b-v1.5", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "fabiochiu/t5-small-medium-title-generation": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 32128}, "TheBloke/Llama-2-7b-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "skt/kogpt2-base-v2": {"_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 1, "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "pad_token_id": 3, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.3.3", "use_cache": true, "vocab_size": 51200, "author": "Heewon Jeon(madjakarta@gmail.com)", "created_date": "2021-04-28", "license": "CC-BY-NC-SA 4.0"}, "google/t5-v1_1-base": {"_name_or_path": "/home/patrick/hugging_face/t5/t5-v1_1-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "Maykeye/TinyLLama-v0": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 64, "initializer_range": 0.02, "intermediate_size": 256, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 16, "num_hidden_layers": 8, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "TheBloke/Llama-2-13B-chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "tiiuae/falcon-40b": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 65024}, "sonoisa/t5-base-japanese-title-generation": {"_name_or_path": "sonoisa/t5-base-japanese", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 0, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "eos_token_ids": [1], "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "model_type": "t5", "n_positions": 512, "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "pad_token_id": 0, "relative_attention_num_buckets": 32, "transformers_version": "4.2.2", "use_cache": true, "vocab_size": 32128}, "Rostlab/prot_t5_xl_uniref50": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "use_cache": true, "vocab_size": 128}, "TheBloke/vicuna-7B-v1.3-GPTQ": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "daryl149/llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "stabilityai/StableBeluga-7B": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "meta-llama/Llama-2-70b-hf": {"_name_or_path": "meta-llama/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/MythoMax-L2-13B-GPTQ": {"_name_or_path": "mythomax-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "czurita/nsql-llama-2-7B-sharded-bf16-2GB": {"_name_or_path": "NumbersStation/nsql-llama-2-7B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 2, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "vennify/t5-base-grammar-correction": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.9.2", "use_cache": true, "vocab_size": 32128}, "czearing/story-to-title": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.20.1", "use_cache": true, "vocab_size": 32128}, "google/byt5-large": {"_name_or_path": "/home/patrick/t5/byt5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3840, "d_kv": 64, "d_model": 1536, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 16, "num_layers": 36, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "transformers_version": "4.7.0.dev0", "use_cache": true, "vocab_size": 384}, "HuggingFaceH4/starchat-beta": {"_name_or_path": "data/starcoderplus-ift-v4.1", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": true, "validate_runner_input": true, "vocab_size": 49156}, "codellama/CodeLlama-34b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "openlm-research/open_llama_13b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "optimum/t5-small": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 32128}, "t5-3b": {"architectures": ["T5WithLMHeadModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "humarin/chatgpt_paraphraser_on_T5_base": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}, "paraphrase": {"early_stopping": true, "max_length": 128, "num_beams": 5, "prefix": "paraphrase: "}}, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32128}, "Gustavosta/MagicPrompt-Stable-Diffusion": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.23.0.dev0", "use_cache": true, "vocab_size": 50257}, "bigscience/bloomz-7b1": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 4096, "n_inner": null, "n_layer": 30, "num_attention_heads": 32, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "transformers_version": "4.21.0.dev0", "unk_token_id": 0, "use_cache": true, "vocab_size": 250880}, "trl-internal-testing/tiny-random-GPTNeoXForCausalLM": {"_name_or_path": "hf-internal-testing/tiny-random-GPTNeoXForCausalLM", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.1, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 32, "initializer_range": 0.02, "intermediate_size": 37, "is_decoder": true, "layer_norm_eps": 1e-05, "max_position_embeddings": 512, "model_type": "gpt_neox", "num_attention_heads": 4, "num_hidden_layers": 5, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.1", "type_vocab_size": 16, "use_cache": true, "use_parallel_residual": true, "vocab_size": 1024}, "NousResearch/Yarn-Llama-2-7b-64k": {"_name_or_path": "/fsx/proj-trlx/conceptofmind/scaled-rope-private/save_model/final/yarn-llama-2-7b-3e-5-64k-400steps", "architectures": ["LlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModelForCausalLM": "modeling_llama_together_yarn.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 65536, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 16.0, "original_max_position_embeddings": 4096, "type": "yarn", "finetuned": true}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "khhuang/zerofec-qa2claim-t5-base": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 32128}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3": {"_name_or_path": "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "attention_probs_dropout_prob": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_dropout_prob": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 65024}, "khhuang/zerofec-daqa-t5-base": {"_name_or_path": "allenai/unifiedqa-v2-t5-base-1251000", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 32128}, "declare-lab/flan-alpaca-gpt4-xl": {"_name_or_path": "google/flan-t5-xl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32128}, "codellama/CodeLlama-7b-hf": {"_name_or_path": "codellama/CodeLlama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32016}, "lmsys/vicuna-13b-v1.3": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-13b-v1.3", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "Rostlab/prot_t5_xl_half_uniref50-enc": {"_name_or_path": "Rostlab/prot_t5_xl_half_uniref50-enc", "architectures": ["T5EncoderModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "torch_dtype": "float16", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 128}, "google/mt5-small": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "vocab_size": 250112}, "Salesforce/safety-flan-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "patrickvonplaten/t5-tiny-random": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 256, "d_kv": 8, "d_model": 64, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "max_length": 10, "model_type": "t5", "n_positions": 512, "num_heads": 2, "num_layers": 2, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "google/flan-ul2": {"_name_or_path": "/home/younes_huggingface_co/code/ul2/flan-ul2", "architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_kv": 256, "d_model": 4096, "decoder_start_token_id": 0, "dense_act_fn": "silu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-silu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 32, "num_heads": 16, "num_layers": 32, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "bfloat16", "transformers_version": "4.25.0.dev0", "use_cache": true, "vocab_size": 32128}, "EleutherAI/pythia-70m": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 512, "initializer_range": 0.02, "intermediate_size": 2048, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 6, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "bigscience/mt0-large": {"_name_or_path": "google/mt5-large", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "stevhliu/my_awesome_billsum_model": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "EleutherAI/pythia-70m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 512, "initializer_range": 0.02, "intermediate_size": 2048, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 6, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "lmsys/vicuna-13b-v1.5": {"_name_or_path": "vicuna-13b-v1.5", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "PAIXAI/Astrid-1B": {"_name_or_path": "EleutherAI/pythia-1b-deduped", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 0, "classifier_dropout": 0.1, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout_prob": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "Phind/Phind-CodeLlama-34B-Python-v1": {"_name_or_path": "/fsx/codellama-34b-python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "pszemraj/flan-t5-large-grammar-synthesis": {"_name_or_path": "pszemraj/flan-t5-large-grammar-synthesis-v6", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "min_length": 8, "model_type": "t5", "n_positions": 512, "no_repeat_ngram_size": 4, "num_beams": 2, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "use_cache": true, "vocab_size": 32128}, "Voicelab/vlt5-base-keywords": {"_name_or_path": "Voicelab/vlt5-base-keywords", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.22.1", "use_cache": true, "vocab_size": 50048}, "togethercomputer/Llama-2-7B-32K-Instruct": {"architectures": ["LlamaForCausalLM"], "auto_map": {"AutoModelForCausalLM": "togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 32768, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "huggyllama/llama-65b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "ai-forever/ruGPT-3.5-13B": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.27.1", "use_cache": true, "vocab_size": 50272}, "Einmalumdiewelt/T5-Base_GNAD": {"_name_or_path": "Einmalumdiewelt/T5-Base_GNAD", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.22.0.dev0", "use_cache": true, "vocab_size": 32100}, "google/t5-v1_1-xl": {"_name_or_path": "/home/patrick/t5/t5-v1_1-xl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "Austism/chronos-hermes-13b": {"_name_or_path": "E:/TextGenWebUI/text-generation-webui/models/NousResearch_Nous-Hermes-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32001}, "upstage/SOLAR-0-70b-16bit": {"_name_or_path": "output/llama2-70b-alpaca_gpt4/last", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "bigscience/bloom-7b1": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 4096, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 32, "n_inner": null, "n_layer": 30, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 1, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "unk_token_id": 0, "use_cache": true, "vocab_size": 250880}, "nlpai-lab/kullm-polyglot-12.8b-v2": {"_name_or_path": "/data/persuade/01_KuAlpaca/models/lora-alpaca-merged-ep5", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 40, "num_steps": "global_step301000", "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "codellama/CodeLlama-13b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32016}, "hf-internal-testing/tiny-random-GPT2Model": {"_name_or_path": "temp/dummy/gpt2/GPT2Model", "activation_function": "gelu", "architectures": ["GPT2Model"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "Gryphe/MythoMax-L2-13b": {"_name_or_path": "mythomax-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "openlm-research/open_llama_3b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/Llama-2-70B-chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "trl-internal-testing/dummy-GPT2-correct-vocab": {"_name_or_path": "trl-internal-testing/tiny-random-GPT2LMHeadModel", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "is_decoder": true, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 50257}, "charsiu/g2p_multilingual_byT5_small_100": {"_name_or_path": "google/byt5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_kv": 64, "d_model": 1472, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 4, "num_heads": 6, "num_layers": 12, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.21.1", "use_cache": true, "vocab_size": 384}, "EleutherAI/pythia-160m": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 12, "num_hidden_layers": 12, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "ElnaggarLab/ankh-base": {"_name_or_path": "ElnaggarLab/protx-base-1gspan-partreconstruction-20mlmp-encl48-decl24-ramd128-ranb64", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.0, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 12, "num_layers": 48, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 64, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.21.1", "use_cache": true, "vocab_size": 144}, "trl-internal-testing/tiny-random-GPT2LMHeadModel": {"_name_or_path": "hf-internal-testing/tiny-random-GPT2LMHeadModel", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "is_decoder": true, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.1", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "openlm-research/open_llama_7b_v2": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.1", "use_cache": true, "vocab_size": 32000}, "TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ": {"_name_or_path": "llama2-13b-orca-v2-8k-3166", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 2.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32016, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "codellama/CodeLlama-7b-Instruct-hf": {"_name_or_path": "codellama/CodeLlama-7b-Instruct-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32016}, "WizardLM/WizardCoder-Python-34B-V1.0": {"_name_or_path": "codellama/CodeLlama-34b-Python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "pszemraj/grammar-synthesis-small": {"_name_or_path": "pszemraj/grammar-synthesis-small-WIP", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "early_stopping": true, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "min_length": 8, "model_type": "t5", "no_repeat_ngram_size": 4, "num_beams": 2, "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.20.1", "use_cache": false, "vocab_size": 32128}, "TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ": {"_name_or_path": "/workspace/process/lora_base/meta-llama_Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "openlm-research/open_llama_3b_v2": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "IDEA-CCNL/Wenzhong-GPT2-110M": {"activation_function": "gelu_fast", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "tokenizer_class": "GPT2TokenizerFast", "transformers_version": "4.12.3", "use_cache": true, "vocab_size": 50304}, "microsoft/DialoGPT-small": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "vocab_size": 50257}, "trl-internal-testing/tiny-random-BloomForCausalLM": {"_name_or_path": "hf-internal-testing/tiny-random-BloomForCausalLM", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 1, "dtype": "float32", "eos_token_id": 2, "gradient_checkpointing": false, "hidden_dropout": 0.1, "hidden_size": 32, "id2label": {"0": "LABEL_0", "1": "LABEL_1", "2": "LABEL_2"}, "initializer_range": 0.02, "is_decoder": true, "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2}, "layer_norm_epsilon": 1e-05, "model_type": "bloom", "n_head": 4, "n_layer": 5, "n_positions": 512, "pad_token_id": 3, "pretraining_tp": 1, "seq_length": 7, "slow_but_exact": true, "torch_dtype": "float32", "transformers_version": "4.25.1", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "trl-internal-testing/tiny-random-T5ForConditionalGeneration": {"_name_or_path": "hf-internal-testing/tiny-random-T5ForConditionalGeneration", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 1302}, "hf-internal-testing/tiny-random-onnx-mt5": {"_name_or_path": "hf-internal-testing/tiny-random-mt5", "architectures": ["MT5Model"], "d_ff": 1024, "d_kv": 4, "d_model": 16, "decoder_attention_heads": 4, "decoder_ffn_dim": 4, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 4, "num_layers": 2, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.22.0.dev0", "use_cache": true, "vocab_size": 250112}, "NousResearch/Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "trl-internal-testing/tiny-random-MT5ForConditionalGeneration": {"_name_or_path": "stas/mt5-tiny-random", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 256, "d_kv": 8, "d_model": 64, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 4, "num_layers": 8, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 5100}, "fxmarty/tiny-testing-gpt2-remote-code": {"_name_or_path": "hf-internal-testing/tiny-random-gpt2", "activation_function": "gelu_new", "architectures": ["GPT2CustomLMHeadModel"], "attention_probs_dropout_prob": 0.1, "attn_pdrop": 0.1, "auto_map": {"AutoModelForCausalLM": "modeling_gpt2.GPT2CustomLMHeadModel"}, "bos_token_id": 98, "embd_pdrop": 0.1, "eos_token_id": 98, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "initializer_range": 0.02, "intermediate_size": 37, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 512, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5, "n_positions": 512, "pad_token_id": 98, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.1", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1000}, "castorini/monot5-base-msmarco-10k": {"_name_or_path": "model.ckpt-1009900.index", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "use_cache": true, "vocab_size": 32128}, "microsoft/DialoGPT-large": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_layer": 36, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "vocab_size": 50257}, "bigscience/bloomz-560m": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "architectures": ["BloomForCausalLM"], "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 1024, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 1, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "Open-Orca/OpenOrca-Platypus2-13B": {"_name_or_path": "Open-Orca/OpenOrcaxOpenChat-Preview2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32002}, "google/byt5-small": {"_name_or_path": "/home/patrick/t5/byt5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_kv": 64, "d_model": 1472, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 4, "num_heads": 6, "num_layers": 12, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "transformers_version": "4.7.0.dev0", "use_cache": true, "vocab_size": 384}, "bigscience/bloom-1b7": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "architectures": ["BloomForCausalLM"], "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 2048, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 2, "seq_length": 4096, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "elinas/chronos-13b-v2": {"_name_or_path": "/workspace/axolotl/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "google/t5-efficient-tiny": {"_name_or_path": "t5-efficient-tiny", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 256, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 4, "num_heads": 4, "num_layers": 4, "pad_token_id": 0, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.17.0.dev0", "use_cache": true, "vocab_size": 32128}, "bigscience/bloom-1b1": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "architectures": ["BloomForCausalLM"], "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 1536, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 1, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "EleutherAI/polyglot-ko-1.3b": {"_name_or_path": "./polyglot-ko-1.3b/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "bigscience/bloom-3b": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "architectures": ["BloomForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 2560, "n_inner": null, "n_layer": 30, "num_attention_heads": 32, "offset_alibi": 100, "pretraining_tp": 4, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "TinyPixel/Llama-2-7B-bf16-sharded": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "edumunozsala/llama-2-7b-int4-python-code-20k": {"_name_or_path": "NousResearch/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "yahma/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "beomi/KoAlpaca-Polyglot-12.8B": {"_name_or_path": "KoAlpaca-Polyglot-12.8B/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 40, "num_steps": "global_step301000", "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30003}, "stanfordnlp/backpack-gpt2": {"architectures": ["BackpackGPT2LMHeadModel"], "auto_map": {"AutoConfig": "configuration_backpack_gpt2.BackpackGPT2Config", "AutoModelForCausalLM": "modeling_backpack_gpt2.BackpackGPT2LMHeadModel"}, "activation_function": "gelu_new", "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 512, "num_senses": 16, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": true, "scale_attn_weights": true, "sense_intermediate_scale": 4, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 50264}, "prithivida/grammar_error_correcter_v1": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.6.1", "use_cache": true, "vocab_size": 32128}, "lvkaokao/llama2-7b-hf-chat-lora-v3": {"_name_or_path": "llama2-7b-chat-finetuned-ours-orca-1w", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "google/t5-v1_1-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "TheBloke/gpt4-alpaca-lora_mlp-65B-GPTQ": {"_name_or_path": "/workspace/models/huggyllama_llama-65b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "desc_act": true, "group_size": -1, "model_file_base_name": "model", "quant_method": "gptq"}}, "google/mt5-large": {"_name_or_path": "/home/patrick/hugging_face/t5/mt5-large", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "vocab_size": 250112}, "EleutherAI/pythia-2.8b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "cyberagent/open-calm-7b": {"_name_or_path": "open-calm-7b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52224}, "lvwerra/gpt2-imdb": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "output_past": true, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50257}, "WizardLM/WizardLM-13B-V1.2": {"_name_or_path": "//workspaceblobstore/caxu/llama_new/Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": false, "vocab_size": 32000}, "KoboldAI/GPT-NeoX-20B-Erebus": {"architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.1, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0.1, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.19.0.dev0", "use_cache": true, "max_length": 2048, "rep_pen": 1.07, "vocab_size": 50432, "welcome": "## Warning: This model has a very heavy NSFW bias and is not suitable for use by minors!\n\nYou are currently running story-writing model `Erebus, version 1 (20B).`\n\n This model is made by [Mr. Seeker](https://www.patreon.com/mrseeker) with help of ProudNoob and the KoboldAI team.\n\n### How to use this model\n\nErebus is designed to generate short stories and novels. Use the author's note to give it a certain genre to follow, use memory to give an overview of the story and use World Information to give specific details about the characters. To start off, give the AI an idea of what you are writing about by setting the scene. Give the AI around 10 sentences that make your story interesting to read. Introduce your character, describe the world, blow something up, or let the AI use its creative mind.", "antemplate": "[Genre: <|>]"}, "togethercomputer/RedPajama-INCITE-Instruct-3B-v1": {"_name_or_path": "/root/fm/models/rp_3b_800b_real_fp16", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "aditi2222/automatic_title_generation": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.10.0", "use_cache": true, "vocab_size": 32128}, "shibing624/chinese-alpaca-plus-13b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 49954}, "optimum/gpt2": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "pad_token_id": 0, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 50257}, "togethercomputer/LLaMA-2-7B-32K": {"architectures": ["LlamaForCausalLM"], "auto_map": {"AutoModelForCausalLM": "modeling_flash_llama.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 32768, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "amazon/FalconLite": {"_name_or_path": "/home/ubuntu/model/long-falcon-40b-oa-4bit", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 65040}, "EleutherAI/polyglot-ko-5.8b": {"_name_or_path": "./polyglot-ko-5.8b/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 28, "num_steps": "global_step320000", "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "databricks/dolly-v2-7b": {"_name_or_path": "EleutherAI/pythia-6.9b", "architectures": ["GPTNeoXForCausalLM"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.25.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50280}, "snrspeaks/t5-one-line-summary": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.6.1", "use_cache": true, "vocab_size": 32128}, "lmsys/vicuna-33b-v1.3": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-33b-v1.3", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "TheBloke/OpenOrca-Platypus2-13B-GPTQ": {"_name_or_path": "Open-Orca/OpenOrcaxOpenChat-Preview2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32002, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/Llama-2-13B-Chat-fp16": {"_name_or_path": null, "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "sdadas/mt5-base-translator-pl-en": {"_name_or_path": "original/mt5-base-translator-pl-en", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 250100}, "TheBloke/Llama-2-7b-chat-fp16": {"_name_or_path": "meta-llama/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "bigcode/gpt_bigcode-santacoder": {"activation_function": "gelu_pytorch_tanh", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "multi_query": true, "attn_pdrop": 0.1, "bos_token_id": 49152, "embd_pdrop": 0.1, "eos_token_id": 49152, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt_bigcode", "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24, "n_positions": 2048, "resid_pdrop": 0.1, "runner_max_sequence_length": null, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 49280}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "attention_probs_dropout_prob": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_dropout_prob": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 65024}, "TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ": {"_name_or_path": "airoboros-l2-70b-gpt4-2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "lmsys/vicuna-13b-v1.5-16k": {"_name_or_path": "vicuna-13b-v1.5-16k/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "max_sequence_length": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "bigcode/santacoder": {"activation_function": "gelu_fast", "architectures": ["GPT2LMHeadCustomModel"], "attention_head_type": "multiquery", "attn_pdrop": 0.1, "auto_map": {"AutoConfig": "configuration_gpt2_mq.GPT2CustomConfig", "AutoModelForCausalLM": "modeling_gpt2_mq.GPT2LMHeadCustomModel"}, "bos_token_id": 49152, "embd_pdrop": 0.1, "eos_token_id": 49152, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 49280}, "togethercomputer/RedPajama-INCITE-Chat-3B-v1": {"_name_or_path": "/root/fm/models/rp_3b_800b_real_fp16", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "ai-forever/mGPT": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 2048, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 2048, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.10.3", "use_cache": true, "vocab_size": 100000}, "openlm-research/open_llama_7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "decapoda-research/llama-13b-hf": {"architectures": ["LLaMAForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "intermediate_size": 13824, "initializer_range": 0.02, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "OpenAssistant/codellama-13b-oasst-sft-v10": {"_name_or_path": "OpenAssistant/codellama-13b-oasst-sft-v10", "architectures": ["LlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "bos_token_id": 32021, "eos_token_id": 32022, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32032}, "rinna/bilingual-gpt-neox-4b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 2, "classifier_dropout": 0.1, "eos_token_id": 3, "hidden_act": "gelu", "hidden_dropout": 0.1, "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rope_scaling": null, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 65536}, "KoboldAI/LLaMA2-13B-Holomax-GPTQ": {"_name_or_path": "KoboldAI_LLaMA2-13B-Holomax", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "quantization_config": {"batch_size": 1, "bits": 4, "block_name_to_quantize": "model.layers", "damp_percent": 0.1, "dataset": ["[Write a detailed adventure story:]"], "desc_act": false, "disable_exllama": true, "group_size": 128, "model_seqlen": 4096, "module_name_preceding_first_block": ["model.embed_tokens"], "pad_token_id": null, "quant_method": "gptq", "sym": true, "tokenizer": null, "true_sequential": true, "use_cuda_fp16": true}, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": false, "vocab_size": 32000}, "MBZUAI/LaMini-T5-61M": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.27.0", "use_cache": true, "vocab_size": 32128}, "google/t5-v1_1-xxl": {"_name_or_path": "/home/patrick/t5/t5-v1_1-xxl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "EleutherAI/pythia-1.4b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "stabilityai/StableBeluga-13B": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "tiiuae/falcon-rw-1b": {"alibi": true, "apply_residual_connection_post_layernorm": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_falcon.FalconConfig", "AutoModel": "modeling_falcon.FalconModel", "AutoModelForSequenceClassification": "modeling_falcon.FalconForSequenceClassification", "AutoModelForTokenClassification": "modeling_falcon.FalconForTokenClassification", "AutoModelForQuestionAnswering": "modeling_falcon.FalconForQuestionAnswering", "AutoModelForCausalLM": "modeling_falcon.FalconForCausalLM"}, "bias": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "falcon", "multi_query": false, "new_decoder_architecture": false, "num_attention_heads": 32, "num_hidden_layers": 24, "parallel_attn": false, "torch_dtype": "bfloat16", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 50304}, "ClueAI/ChatYuan-large-v2": {"_name_or_path": "ClueAI/ChatYuan-large-v2", "architectures": ["T5ForConditionalGeneration"], "auto_map": {"AutoModel": "modeling_t5.T5ForConditionalGeneration"}, "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 32128}, "af1tang/personaGPT": {"_name_or_path": "/model/", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.10.0", "use_cache": true, "vocab_size": 50263}, "google/t5-large-lm-adapt": {"_name_or_path": "./", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 32128}, "vilsonrodrigues/falcon-7b-instruct-sharded": {"alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_falcon.FalconConfig", "AutoModel": "modeling_falcon.FalconModel", "AutoModelForSequenceClassification": "modeling_falcon.FalconForSequenceClassification", "AutoModelForTokenClassification": "modeling_falcon.FalconForTokenClassification", "AutoModelForQuestionAnswering": "modeling_falcon.FalconForQuestionAnswering", "AutoModelForCausalLM": "modeling_falcon.FalconForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "falcon", "multi_query": true, "new_decoder_architecture": false, "num_attention_heads": 71, "num_hidden_layers": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 65024}, "petals-team/falcon-rw-1b": {"alibi": true, "apply_residual_connection_post_layernorm": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_falcon.FalconConfig", "AutoModel": "modeling_falcon.FalconModel", "AutoModelForSequenceClassification": "modeling_falcon.FalconForSequenceClassification", "AutoModelForTokenClassification": "modeling_falcon.FalconForTokenClassification", "AutoModelForQuestionAnswering": "modeling_falcon.FalconForQuestionAnswering", "AutoModelForCausalLM": "modeling_falcon.FalconForCausalLM"}, "bias": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "falcon", "multi_query": false, "new_decoder_architecture": false, "num_attention_heads": 32, "num_hidden_layers": 24, "parallel_attn": false, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 50304}, "bigscience/T0_3B": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.8.1", "use_cache": true, "vocab_size": 32128}, "TheTravellingEngineer/llama2-7b-hf-guanaco": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "Salesforce/codet5-base": {"_name_or_path": "/content/drive/MyDrive/CodeT5/pretrained_models/codet5_base", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_factor": 1.0, "is_encoder_decoder": true, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.10.2", "use_cache": true, "vocab_size": 32100}, "EleutherAI/pythia-2.8b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "The-Face-Of-Goonery/Huginn-13b-v1.2": {"_name_or_path": "Huginnv1.2-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "FredZhang7/distilgpt2-stable-diffusion-v2": {"_name_or_path": "distilgpt2", "_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6, "n_positions": 1024, "pad_token_id": 50256, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 76, "temperature": 0.7, "top_k": 4, "repetition_penalty": 1.2, "early_stopping": true}}, "torch_dtype": "float32", "transformers_version": "4.21.2", "use_cache": true, "vocab_size": 50257}, "WizardLM/WizardCoder-15B-V1.0": {"_name_or_path": "bigcode/starcoder", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "validate_runner_input": true, "vocab_size": 49153}, "EleutherAI/pythia-410m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "huggyllama/llama-13b": {"_name_or_path": "/home/sgugger/tmp/llama/llama-13b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "ybelkada/falcon-7b-sharded-bf16": {"_name_or_path": "tiiuae/falcon-7b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "tiiuae/falcon-7b--configuration_RW.RWConfig", "AutoModel": "tiiuae/falcon-7b--modelling_RW.RWModel", "AutoModelForCausalLM": "tiiuae/falcon-7b--modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "tiiuae/falcon-7b--modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "tiiuae/falcon-7b--modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "tiiuae/falcon-7b--modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 65024}, "MingZhong/unieval-sum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.17.0.dev0", "use_cache": true, "vocab_size": 32100}, "NousResearch/Nous-Hermes-Llama2-13b": {"_name_or_path": "output/hermes-llama2-4k/checkpoint-2259", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32032}, "csebuetnlp/mT5_multilingual_XLSum": {"_name_or_path": "google/mt5-base", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "length_penalty": 0.6, "max_length": 84, "model_type": "mt5", "no_repeat_ngram_size": 2, "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "use_cache": true, "vocab_size": 250112}, "hkunlp/instructor-xl": {"_name_or_path": "/home2/huggingface/outputs/xl_30000_fever/checkpoint-300/", "architectures": ["T5EncoderModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.20.0.dev0", "use_cache": true, "vocab_size": 32128}, "h2oai/h2ogpt-4096-llama2-13b-chat": {"_name_or_path": "h2oai/h2ogpt-4096-llama2-13b-chat", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "HuggingFaceH4/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 16, "initializer_range": 0.02, "intermediate_size": 64, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 2, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "EleutherAI/polyglot-ko-12.8b": {"_name_or_path": "./polyglot-ko-12.8b/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 40, "num_steps": "global_step301000", "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "databricks/dolly-v2-12b": {"_name_or_path": "EleutherAI/pythia-12b", "architectures": ["GPTNeoXForCausalLM"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.25.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50280}, "mrm8488/t5-base-finetuned-span-sentiment-extraction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "WizardLM/WizardLM-70B-V1.0": {"_name_or_path": "/home/aiscuser/Llama-2-70b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "codellama/CodeLlama-34b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Salesforce/codet5-base-multi-sum": {"architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "id2label": {"0": "LABEL_0"}, "initializer_factor": 1.0, "is_encoder_decoder": true, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 256, "min_length": 1, "no_repeat_ngram_size": 3, "num_beams": 5}}, "transformers_version": "4.5.0", "use_cache": true, "vocab_size": 32100}, "MBZUAI/LaMini-T5-738M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/t5-large-distil-v1_checkpoint/checkpoint-12500", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32128}, "codellama/CodeLlama-13b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32016}, "h2oai/h2ogpt-4096-llama2-7b-chat": {"_name_or_path": "h2oai/h2ogpt-4096-llama2-7b-chat", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5": {"_name_or_path": ".saved/oasst-sft-3-pythia-12b-reference_2kpre/checkpoint-4000/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50288}, "bigscience/bloom": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "architectures": ["BloomForCausalLM"], "attention_softmax_in_fp32": true, "pad_token_id": 3, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 14336, "n_layer": 70, "num_attention_heads": 112, "pretraining_tp": 4, "slow_but_exact": false, "transformers_version": "4.21.0", "use_cache": true, "vocab_size": 250880}, "TigerResearch/tigerbot-13b-chat": {"_name_or_path": "/mnt/nfs/yechen/models/tigerbot-13b-2h-sft-20g-mix0.0-group/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 8, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 60928}, "TheBloke/airoboros-l2-70B-gpt4-1.4.1-GPTQ": {"_name_or_path": "airoboros-l2-70b-gpt4-1.4.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "Austism/chronos-hermes-13b-v2": {"_name_or_path": "E:/TextGenWebUI/text-generation-webui/models/Nous-Hermes-Llama2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32032}, "snrspeaks/KeyPhraseTransformer": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.16.2", "use_cache": true, "vocab_size": 32128}, "Rocketknight1/tiny-random-falcon-7b": {"_name_or_path": "falcon_convert/repos/falcon-7b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 1136, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "falcon", "multi_query": true, "new_decoder_architecture": false, "num_attention_heads": 71, "num_hidden_layers": 2, "num_kv_heads": 71, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 65024}, "hf-internal-testing/tiny-random-T5Model": {"_name_or_path": "tiny_models/t5/T5Model", "architectures": ["T5Model"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32100}, "sambanovasystems/BLOOMChat-176B-v1": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": false, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 14336, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 112, "n_layer": 70, "pad_token_id": 3, "pretraining_tp": 4, "return_dict": false, "slow_but_exact": false, "transformers_version": "4.25.0", "use_cache": true, "vocab_size": 250880}, "huggyllama/llama-30b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "elyza/ELYZA-japanese-Llama-2-7b-instruct": {"_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b-instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "lcw99/t5-base-korean-text-summary": {"_name_or_path": "./Models/mt5-base-korean-text-summary", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.22.1", "use_cache": true, "vocab_size": 50358}, "it5/it5-base-news-summarization": {"_name_or_path": ".", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.15.0", "use_cache": true, "vocab_size": 32103, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 142, "min_length": 56, "no_repeat_ngram_size": 3, "num_beams": 4}}}, "uer/gpt2-chinese-cluecorpussmall": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "embd_pdrop": 0.1, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "output_past": true, "resid_pdrop": 0.1, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 320}}, "tokenizer_class": "BertTokenizer", "vocab_size": 21128}, "t5-11b": {"architectures": ["T5WithLMHeadModel"], "d_ff": 65536, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 128, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "KoboldAI/LLaMA2-13B-Holomax": {"_name_or_path": "/home/mixer/koboldai/models/MythoMax-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "TheBloke/Llama-2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "bigscience/bloomz-3b": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "architectures": ["BloomForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 2560, "n_inner": null, "n_layer": 30, "num_attention_heads": 32, "offset_alibi": 100, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "lmsys/vicuna-7b-v1.5-16k": {"_name_or_path": "vicuna-7b-v1.5-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 16384, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "sonoisa/t5-base-japanese": {"_name_or_path": "/content/drive/MyDrive/T5_models/oscar_cc100_wikipedia_ja", "architectures": ["T5Model"], "bos_token_id": 0, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "eos_token_ids": [1], "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "model_type": "t5", "n_positions": 512, "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "pad_token_id": 0, "relative_attention_num_buckets": 32, "transformers_version": "4.2.2", "use_cache": true, "vocab_size": 32128}, "line-corporation/japanese-large-lm-3.6b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 3072, "initializer_range": 0.02, "intermediate_size": 12288, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 30, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": false, "vocab_size": 51200}, "TheBloke/Llama-2-7B-32K-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "auto_map": {"AutoModelForCausalLM": "togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 32768, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-410m": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "NousResearch/Llama-2-70b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/falcon-7b-instruct-GPTQ": {"_name_or_path": "/workspace/process/falcon7b-instruct/HF", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 65024, "quantization_config": {"bits": 4, "group_size": 64, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "eachadea/vicuna-13b-1.1": {"_name_or_path": "/Users/tdo/Desktop/13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "beomi/llama-2-ko-7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 46336}, "TheBloke/falcon-40b-instruct-GPTQ": {"_name_or_path": "/workspace/process/falcon40b-instruct/HF", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 65024, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/Llama-2-13B-GPTQ": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "garage-bAInd/Platypus2-70B-instruct": {"_name_or_path": "upstage/Llama-2-70b-instruct-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "rajkumarrrk/gpt2-fine-tuned-on-imdb-positive-reviews": {"_name_or_path": "lvwerra/gpt2-imdb", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "output_past": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 50257}, "cerebras/Cerebras-GPT-13B": {"_name_or_path": "/cb/scrap/cynthia/Cerebras-GPT-13B", "activation_function": "gelu", "architectures": ["GPT2Model"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 5120, "n_head": 40, "n_inner": 20480, "n_layer": 40, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.27.2", "use_cache": true, "vocab_size": 50257}, "rinna/japanese-gpt2-medium": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 1, "embd_pdrop": 0.1, "eos_token_id": 2, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24, "n_positions": 1024, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.8.2", "use_cache": true, "vocab_size": 32000}, "bigscience/T0pp": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.8.1", "use_cache": true, "vocab_size": 32128}, "Phind/Phind-CodeLlama-34B-v1": {"_name_or_path": "/fsx/codellama-34b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "beomi/kykim-gpt3-kor-small_based_on_gpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 3, "embd_pdrop": 0.1, "eos_token_id": 3, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 2048, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "use_cache": true, "vocab_size": 42000}, "Pi3141/DialoGPT-medium-elon-3": {"_name_or_path": "output-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 50257}, "EleutherAI/pythia-1b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "ai-forever/rugpt3large_based_on_gpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 2048, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50257}, "jondurbin/airoboros-l2-13b-gpt4-m2.0": {"_name_or_path": "airoboros-l2-13b-gpt4-m2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "codellama/CodeLlama-13b-Python-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "AUTOMATIC/promptgen-lexart": {"_name_or_path": "distilgpt2", "_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 50257}, "Salesforce/codet5-small": {"_name_or_path": "/content/drive/MyDrive/CodeT5/pretrained_models/codet5_small", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_factor": 1.0, "is_encoder_decoder": true, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.10.2", "use_cache": true, "vocab_size": 32100}, "h2oai/h2ogpt-oig-oasst1-512-6_9b": {"_name_or_path": "h2oai/h2ogpt-oig-oasst1-512-6_9b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "rinna/japanese-gpt-neox-3.6b-instruction-ppo": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "eos_token_id": 3, "hidden_act": "gelu", "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 32000}, "prithivida/informal_to_formal_styletransfer": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.7.0", "use_cache": true, "vocab_size": 32128}, "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ": {"_name_or_path": "/workspace/models/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "desc_act": false, "group_size": 128, "model_file_base_name": "model", "quant_method": "gptq"}}, "matsuo-lab/weblab-10b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4864, "initializer_range": 0.02, "intermediate_size": 19456, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 38, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50277}, "succinctly/text2image-prompt-generator": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.20.1", "use_cache": true, "vocab_size": 50257}, "TheBloke/Llama-2-7B-Chat-GGML": {"model_type": "llama"}, "TheBloke/Llama-2-70B-fp16": {"_name_or_path": "meta-llama/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "sentence-transformers/gtr-t5-large": {"_name_or_path": "models/gtr-t5-large", "architectures": ["T5EncoderModel"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2": {"_name_or_path": "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "attention_probs_dropout_prob": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_dropout_prob": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.29.0", "use_cache": true, "vocab_size": 65024}, "togethercomputer/RedPajama-INCITE-Base-3B-v1": {"_name_or_path": "rp_3b_800b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "rinna/bilingual-gpt-neox-4b": {"architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 2, "classifier_dropout": 0.1, "eos_token_id": 3, "hidden_act": "gelu", "hidden_dropout": 0.1, "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 65536}, "TheBloke/Llama-2-13B-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "pankajmathur/orca_mini_v3_70b": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "OpenAssistant/llama2-13b-orca-8k-3319": {"_name_or_path": "/mnt/data/ikka/Open-Assistant/model/model_training/llama2_13b_orca_8k_2/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 8192, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 2.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32016}, "TheBloke/StableBeluga-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "defog/sqlcoder": {"_name_or_path": "defog/starcoder-finetune-v3-easy", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "validate_runner_input": true, "vocab_size": 49152}, "WizardLM/WizardCoder-Python-13B-V1.0": {"_name_or_path": "codellama/CodeLlama-13b-Python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "stabilityai/stablelm-tuned-alpha-3b": {"_name_or_path": "3b-tuned-84k", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "cyberagent/open-calm-small": {"_name_or_path": "open-calm-small", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 12, "num_hidden_layers": 12, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52096}, "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ": {"_name_or_path": "/workspace/process/wizard-vicuna-30b/HF/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/WizardLM-70B-V1.0-GPTQ": {"_name_or_path": "/home/aiscuser/Llama-2-70b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "bigscience/bigscience-small-testing": {"_name_or_path": "/home/younes/Desktop/Work/data/megatron-debug/", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomModel"], "attention_dropout": 0.1, "bias_dropout_fusion": true, "bos_token_id": 0, "dtype": "bfloat16", "eos_token_id": 0, "hidden_dropout": 0.1, "hidden_size": 64, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 8, "n_inner": null, "n_layer": 2, "pretraining_tp": 2, "seq_length": 20, "skip_bias_add": true, "torch_dtype": "bfloat16", "transformers_version": "4.18.0", "use_cache": false, "vocab_size": 250880, "slow_but_exact": true}, "cyberagent/open-calm-1b": {"_name_or_path": "open-calm-1b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52096}, "lamini/lamini_docs_finetuned": {"_name_or_path": "EleutherAI/pythia-70m", "architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.0, "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout": 0.0, "hidden_size": 512, "initializer_range": 0.02, "intermediate_size": 2048, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 6, "rope_scaling": null, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "EnglishVoice/t5-base-uk-to-us-english": {"_name_or_path": "upload", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 32128}, "codellama/CodeLlama-7b-Python-hf": {"_name_or_path": "codellama/CodeLlama-7b-Python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/WizardLM-13B-V1.2-GPTQ": {"_name_or_path": "//workspaceblobstore/caxu/llama_new/Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-160m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 12, "num_hidden_layers": 12, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "jphme/Llama-2-13b-chat-german": {"_name_or_path": "jphme/Llama-2-13b-chat-german", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "EleutherAI/pythia-12b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "Salesforce/codet5p-220m": {"_name_or_path": "Salesforce/codet5p-220m", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float16", "transformers_version": "4.21.3", "use_cache": true, "vocab_size": 32100}, "google/mt5-xl": {"_name_or_path": "/home/patrick/t5/mt5-xl", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 250112}, "cerebras/Cerebras-GPT-111M": {"model_type": "gpt2", "attn_pdrop": 0.0, "scale_attn_weights": true, "resid_pdrop": 0.0, "n_inner": 3072, "n_embd": 768, "layer_norm_epsilon": 1e-05, "n_positions": 2048, "activation_function": "gelu", "n_head": 12, "n_layer": 10, "tie_word_embeddings": true, "vocab_size": 50257, "embd_pdrop": 0.0}, "google/t5-v1_1-large": {"_name_or_path": "/home/patrick/hugging_face/t5/t5-v1_1-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "TheBloke/vicuna-7B-v1.5-GPTQ": {"_name_or_path": "vicuna-7b-v1.5", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "chavinlo/alpaca-native": {"_name_or_path": "decapoda-research/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32001}, "kimnt93/kmv-7b-03": {"_name_or_path": "/models/kmv-7b-03", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "NumbersStation/nsql-llama-2-7B": {"_name_or_path": "nsql-llama-2-7B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 2, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "cerebras/Cerebras-GPT-1.3B": {"model_type": "gpt2", "attn_pdrop": 0.0, "scale_attn_weights": true, "resid_pdrop": 0.0, "n_inner": 8192, "n_embd": 2048, "layer_norm_epsilon": 1e-05, "n_positions": 2048, "activation_function": "gelu", "n_head": 16, "n_layer": 24, "tie_word_embeddings": true, "vocab_size": 50257, "embd_pdrop": 0.0}, "trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab": {"_name_or_path": "trl-internal-testing/tiny-random-T5ForConditionalGeneration", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32128}, "akreal/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 16, "initializer_range": 0.02, "intermediate_size": 64, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 2, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "akreal/tiny-random-BloomForCausalLM": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 1, "dtype": "float32", "eos_token_id": 2, "gradient_checkpointing": false, "hidden_dropout": 0.1, "hidden_size": 32, "id2label": {"0": "LABEL_0", "1": "LABEL_1", "2": "LABEL_2"}, "initializer_range": 0.02, "is_decoder": true, "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2}, "layer_norm_epsilon": 1e-05, "model_type": "bloom", "n_head": 4, "n_layer": 5, "n_positions": 512, "pad_token_id": 3, "pretraining_tp": 1, "seq_length": 7, "slow_but_exact": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "NousResearch/Nous-Hermes-llama-2-7b": {"_name_or_path": "output/hermes-llama2-4k/checkpoint-2259", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "ai-forever/rugpt3small_based_on_gpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 2048, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50264}, "VMware/open-llama-7b-v2-open-instruct": {"_name_or_path": "/home/gollapudit/peft/open_llama_7b_v2_open_instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "robertmyers/targon-7b": {"_name_or_path": "output/targon-7b-v118", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 32000, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32001}, "TheBloke/Nous-Hermes-13B-GPTQ": {"_name_or_path": "elinas/llama-13b-hf-transformers-4.29", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ": {"_name_or_path": "WizardLM-33B-V1.0-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/WizardLM-7B-uncensored-GPTQ": {"_name_or_path": "/workspace/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "desc_act": false, "group_size": 128, "model_file_base_name": "model", "quant_method": "gptq"}}, "ramsrigouthamg/t5_paraphraser": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "tinkoff-ai/ruDialoGPT-medium": {"activation_function": "gelu_new", "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 2048, "n_special": 0, "output_past": true, "predict_special_tokens": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.15.0", "use_cache": true, "vocab_size": 50261}, "OpenAssistant/falcon-7b-sft-mix-2000": {"_name_or_path": "/home/ubuntu/Open-Assistant/output_dir/oaast-falcon7b-stage2-top2/checkpoint-2000/", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 65040}, "bigcode/tiny_starcoder_py": {"_name_or_path": "/fsx/bigcode/tinystarcoder/saves/large-model", "activation_function": "gelu_pytorch_tanh", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "multi_query": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 20, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "validate_runner_input": true, "vocab_size": 49152}, "rinna/japanese-gpt-1b": {"activation_function": "gelu_fast", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 2, "embd_pdrop": 0.1, "eos_token_id": 3, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "use_cache": true, "vocab_size": 44928}, "TheBloke/orca_mini_v3_70B-GPTQ": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "UBC-NLP/turjuman": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "transformers_version": "4.5.1", "use_cache": true, "vocab_size": 110080}, "h2oai/h2ogpt-4096-llama2-70b-chat": {"_name_or_path": "h2oai/h2ogpt-4096-llama2-70b-chat", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "Phind/Phind-CodeLlama-34B-v2": {"_name_or_path": "/fsx/Phind-CodeLlama-34B-v1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct": {"_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 45043}, "iarfmoose/t5-base-question-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32102}, "TheBloke/Llama-2-7B-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "mrm8488/t5-base-finetuned-emotion": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "hkunlp/instructor-base": {"_name_or_path": "/scratch/acd14245px/metatrain_models/enhanced_large/0103_base_fever_40000/checkpoint-200/", "architectures": ["T5EncoderModel"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.20.0.dev0", "use_cache": true, "vocab_size": 32128}, "fxmarty/onnx-tiny-random-gpt2-without-merge": {"_name_or_path": "hf-internal-testing/tiny-random-GPT2Model", "activation_function": "gelu", "architectures": ["GPT2Model"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.28.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "fxmarty/onnx-tiny-random-gpt2-with-merge": {"_name_or_path": "hf-internal-testing/tiny-random-GPT2Model", "activation_function": "gelu", "architectures": ["GPT2Model"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.28.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "microsoft/GODEL-v1_1-large-seq2seq": {"_name_or_path": "GODEL-v1_1-large-seq2seq", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 32102}, "rinna/japanese-gpt-neox-3.6b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "eos_token_id": 3, "hidden_act": "gelu", "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 32000}, "cyberagent/open-calm-3b": {"_name_or_path": "open-calm-3b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52224}, "eachadea/vicuna-7b-1.1": {"_name_or_path": "/Users/tdo/Desktop/7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "beomi/KoAlpaca-Polyglot-5.8B": {"_name_or_path": "beomi/KoAlpaca-Polyglot-5.8B", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 28, "num_steps": "global_step320000", "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "grammarly/coedit-large": {"_name_or_path": "google/flan-t5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 32100}, "TheBloke/Platypus2-70B-Instruct-GPTQ": {"_name_or_path": "upstage/Llama-2-70b-instruct-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "codellama/CodeLlama-34b-Python-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "noamwies/llama-test-gqa-with-better-transformer": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 128, "initializer_range": 0.02, "intermediate_size": 344, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 8, "num_hidden_layers": 2, "num_key_value_heads": 4, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 2000}, "bigscience/bloomz-7b1-mt": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 4096, "n_inner": null, "n_layer": 30, "num_attention_heads": 32, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "transformers_version": "4.21.0.dev0", "unk_token_id": 0, "use_cache": true, "vocab_size": 250880}, "Salesforce/codet5p-770m": {"_name_or_path": "Salesforce/codet5p-770m", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float16", "transformers_version": "4.21.3", "use_cache": true, "vocab_size": 32100}, "OpenAssistant/pythia-12b-sft-v8-7k-steps": {"_name_or_path": ".saved/pythia-12b-sft8/checkpoint-7000/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50288}, "augtoma/qCammel-70-x": {"_name_or_path": "qCammel-70", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "NousResearch/Llama-2-13b-chat-hf": {"_name_or_path": null, "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "plguillou/t5-base-fr-sum-cnndm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32102}, "WeOpenML/PandaLM-7B-v1": {"_name_or_path": "/ssdwork/wyd/test/llm/output/llama-7b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32001}, "VMware/open-llama-7b-open-instruct": {"_name_or_path": "/home/gollapudit/peft/open_llama_open_instruct_v1.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "pankajmathur/orca_mini_v3_7b": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "google/t5-xl-lm-adapt": {"_name_or_path": "./", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 32128}, "LinkSoul/Chinese-Llama-2-7b": {"_name_or_path": "./llama2/hf_llama2/7B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "line-corporation/japanese-large-lm-3.6b-instruction-sft": {"_name_or_path": "/vsfs/deepspeed_chat/base-models/line-gpt-japanese-3.6b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "classifier_dropout": 0.1, "end_token_id": 2, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 3072, "initializer_range": 0.02, "intermediate_size": 12288, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 30, "pad_token_id": 2, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": true, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 51200}, "OpenAssistant/oasst-sft-1-pythia-12b": {"_name_or_path": "/home/ubuntu/Open-Assistant/model/model_training/.saved_models/oasst-sft-1_12b/checkpoint-7500/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.26.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50288}, "ehartford/WizardLM-7B-Uncensored": {"_name_or_path": "/workspace/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32001}, "upstage/llama-30b-instruct-2048": {"_name_or_path": "upstage/llama-30b-instruct-2048", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "cyberagent/open-calm-large": {"_name_or_path": "open-calm-large", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 1536, "initializer_range": 0.02, "intermediate_size": 6144, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52096}, "Gryphe/MythoLogic-L2-13b": {"_name_or_path": "MythoLogic-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "eenzeenee/t5-small-korean-summarization": {"_name_or_path": "paust/pko-t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "max_length": 128, "model_type": "t5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 50358}, "google/t5-xxl-lm-adapt": {"_name_or_path": "./", "architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 32128}, "mywateriswet/ShuanBot": {"_name_or_path": "output-small", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 50257}, "hf-internal-testing/tiny-random-bloom": {"_name_or_path": "/home/younes/Desktop/Work/data/megatron-debug/", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomModel"], "attention_dropout": 0.1, "bias_dropout_fusion": true, "bos_token_id": 0, "dtype": "bfloat16", "eos_token_id": 0, "hidden_dropout": 0.1, "hidden_size": 64, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 8, "n_inner": null, "n_layer": 2, "pretraining_tp": 2, "seq_length": 20, "skip_bias_add": true, "torch_dtype": "bfloat16", "transformers_version": "4.18.0.dev0", "use_cache": false, "vocab_size": 250880, "slow_but_exact": true}, "TheBloke/Llama-2-13B-chat-GGML": {"model_type": "llama"}, "decapoda-research/llama-30b-hf": {"architectures": ["LLaMAForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "intermediate_size": 17920, "initializer_range": 0.02, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": -1, "rms_norm_eps": 1e-06, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "lmsys/longchat-7b-v1.5-32k": {"_name_or_path": "longchat-v2-7b-32k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 32768, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_condense_ratio": 8, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32000}, "ziqingyang/chinese-alpaca-2-7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 55296}, "nlpai-lab/kullm-polyglot-5.8b-v2": {"_name_or_path": "models/kullm-5.8b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 28, "num_steps": "global_step320000", "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "google/byt5-base": {"_name_or_path": "/home/patrick/t5/byt5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3968, "d_kv": 64, "d_model": 1536, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 6, "num_heads": 12, "num_layers": 18, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "transformers_version": "4.7.0.dev0", "use_cache": true, "vocab_size": 384}, "stabilityai/stablelm-tuned-alpha-7b": {"_name_or_path": "7b-tuned-69k", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 48, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "PygmalionAI/pygmalion-1.3b": {"_name_or_path": "EleutherAI/pythia-1.3b-deduped", "architectures": ["GPTNeoXForCausalLM"], "bad_words_ids": [[434, 15694, 66, 27, 209], [15362], [1713], [1713, 64], [1713, 876], [2016, 251, 857, 75, 9194, 35478], [2391], [20340], [33021], [2391, 1051], [5638], [2391, 20340], [5638, 537], [1559, 2345], [1559, 7849], [1559, 17379], [25321, 4611]], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.25.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "stanford-crfm/BioMedLM": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 28895, "embd_pdrop": 0.1, "eos_token_id": 28895, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 2560, "n_head": 20, "n_inner": null, "n_layer": 32, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.21.3", "use_cache": false, "vocab_size": 28896}, "PY007/TinyLlama-1.1B-step-50K-105b": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 5632, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 22, "num_key_value_heads": 4, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "georgesung/llama2_7b_chat_uncensored": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "bigscience/mt0-small": {"_name_or_path": "google/mt5-small", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "TheBloke/WizardCoder-15B-1.0-GPTQ": {"_name_or_path": "bigcode/starcoder", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "validate_runner_input": true, "vocab_size": 49153, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "google/t5-base-lm-adapt": {"_name_or_path": "./", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 32128}, "OpenAssistant/falcon-40b-sft-top1-560": {"_name_or_path": "/home/ubuntu/Open-Assistant/output_dir/falcon40b_oasst_top1/checkpoint-560/", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 65040}, "TheBloke/WizardLM-30B-Uncensored-GPTQ": {"_name_or_path": "/workspace/WizardLM-30B-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/WizardCoder-Python-34B-V1.0-GPTQ": {"_name_or_path": "codellama/CodeLlama-34b-Python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "garage-bAInd/Camel-Platypus2-70B": {"_name_or_path": "augtoma/qCammel-70-x", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "DeepFloyd/t5-v1_1-xxl": {"_name_or_path": "google/t5-v1_1-xxl", "architectures": ["T5EncoderModel"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.21.1", "use_cache": true, "vocab_size": 32128}, "EleutherAI/pythia-1b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "TheBloke/CodeLlama-7B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "kfkas/Llama-2-ko-7b-Chat": {"_name_or_path": "beomi/llama-2-ko-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 46336}, "valhalla/t5-small-qa-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"translation_en_to_fr": {"early_stopping": true, "length_penalty": 1.0, "max_length": 32, "num_beams": 4, "prefix": ""}}, "vocab_size": 32102}, "FlagAlpha/Llama2-Chinese-13b-Chat": {"_name_or_path": "/mnt/data/zhangzheng/data/other_model/models--meta-llama--Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Open-Orca/OpenOrcaxOpenChat-Preview2-13B": {"_name_or_path": "imone/LLaMA2_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32002}, "trl-internal-testing/tiny-random-LlamaForCausalLM": {"_name_or_path": "HuggingFaceM4/tiny-random-LlamaForCausalLM", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 16, "initializer_range": 0.02, "intermediate_size": 64, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 2, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "abhishek/llama-2-7b-hf-small-shards": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "togethercomputer/RedPajama-INCITE-7B-Base": {"_name_or_path": "rp_800b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "Salesforce/codegen25-7b-multi": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 50256, "eos_token_id": 50256, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 51200}, "fabiochiu/t5-base-tag-generation": {"_name_or_path": "fabiochiu/t5-base-tag-generation", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 32128}, "MBZUAI/LaMini-Flan-T5-248M": {"_name_or_path": "google/flan-t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 32128}, "bigscience/bloomz-1b7": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "architectures": ["BloomForCausalLM"], "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "pad_token_id": 3, "unk_token_id": 0, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 2048, "n_inner": null, "n_layer": 24, "num_attention_heads": 16, "offset_alibi": 100, "pretraining_tp": 2, "seq_length": 4096, "skip_bias_add": true, "skip_bias_add_qkv": false, "transformers_version": "4.20.0", "use_cache": true, "vocab_size": 250880}, "valhalla/t5-base-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "max_length": 32, "num_beams": 4, "prefix": ""}, "translation_en_to_fr": {"early_stopping": true, "max_length": 32, "num_beams": 4, "prefix": ""}, "translation_en_to_de": {"early_stopping": true, "max_length": 32, "num_beams": 4, "prefix": ""}, "translation_en_to_ro": {"early_stopping": true, "max_length": 32, "num_beams": 4, "prefix": ""}}, "vocab_size": 32101}, "Wi/gptp": {"_name_or_path": "kyryl0s/gpt2-uk-xxs", "activation_function": "gelu_new", "architectures": ["GPTPModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "line_by_line": false, "model_type": "gpt2", "n_ctx": 64, "n_embd": 128, "n_head": 4, "n_inner": null, "n_layer": 4, "n_positions": 64, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.21.2", "use_cache": true, "vocab_size": 1000}, "medalpaca/medalpaca-7b": {"_name_or_path": "../llama-7b-hf/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32001}, "yentinglin/Taiwan-LLaMa-v1.0": {"_name_or_path": "output/zh_tw_LLAMA2-chat-sft-0809/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "rinna/japanese-gpt-neox-small": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "eos_token_id": 3, "hidden_act": "gelu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 12, "num_hidden_layers": 12, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "use_cache": true, "use_parallel_residual": false, "vocab_size": 44416}, "TheBloke/llama2_7b_chat_uncensored-GPTQ": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-1.4b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "daryl149/llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "flax-community/gpt-2-spanish": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.0, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.9.0.dev0", "use_cache": true, "vocab_size": 50257}, "KoboldAI/LLAMA2-13B-Holodeck-1": {"_name_or_path": "mrseeker/llama2-13b-pike-v2-2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "jondurbin/airoboros-13b-gpt4-1.4": {"_name_or_path": "airoboros-13b-gpt4-1.4", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "mrm8488/t5-base-finetuned-question-generation-ap": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128, "xla_device": true}, "OpenBuddy/openbuddy-llama2-13b-v8.1-fp16": {"_name_or_path": "final", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 37632}, "EleutherAI/pythia-6.9b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "tscholak/3vnuv1vf": {"_name_or_path": "/safekeeping/t5-1.1.lm100k/t5.1.1.lm100k.large/pytorch_model/", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "diversity_penalty": null, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "model_type": "t5", "num_beams": 4, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.8.2", "use_cache": true, "vocab_size": 32102}, "OpenAssistant/llama2-70b-oasst-sft-v10": {"_name_or_path": "OpenAssistant/llama2-70b-oasst-sft-v10", "architectures": ["LlamaForCausalLM"], "bos_token_id": 32005, "eos_token_id": 32006, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.1", "use_cache": true, "vocab_size": 32007}, "TheBloke/vicuna-13B-v1.5-16K-GPTQ": {"_name_or_path": "vicuna-13b-v1.5-16k/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "max_sequence_length": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "OpenAssistant/falcon-7b-sft-top1-696": {"_name_or_path": "/home/ubuntu/Open-Assistant/output_dir/oaast-falcon7b-top1-2/", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 65040}, "sentence-transformers/sentence-t5-large": {"_name_or_path": "models/sentence-t5-large", "architectures": ["T5EncoderModel"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "TheBloke/Nous-Hermes-Llama2-GPTQ": {"_name_or_path": "output/hermes-llama2-4k/checkpoint-2259", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32032, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "mesolitica/finetune-translation-t5-super-super-tiny-standard-bahasa-cased": {"_name_or_path": "finetune-t5-super-super-tiny-standard-bahasa-cased/checkpoint-1420000", "architectures": ["T5ForConditionalGeneration"], "d_ff": 512, "d_kv": 64, "d_model": 128, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "inputs_length": 512, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 2, "num_heads": 6, "num_layers": 2, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.21.0.dev0", "use_cache": true, "vocab_size": 32100}, "Henk717/spring-dragon": {"_name_or_path": "NousResearch_Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "adventure": true, "vocab_size": 32000}, "openchat/openchat_v3.2": {"_name_or_path": "imone/LLaMA2_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": false, "vocab_size": 32002}, "WizardLM/WizardMath-70B-V1.0": {"_name_or_path": "/model_weights/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32002}, "potsawee/t5-large-generation-squad-QuestionAnswer": {"_name_or_path": "t5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.26.1", "use_cache": true, "vocab_size": 32128}, "TheBloke/Phind-CodeLlama-34B-v2-GPTQ": {"_name_or_path": "/fsx/codellama-34b-wizardcoder/checkpoint-1000/codellama-34b-wizardcoder-checkpoint-1000", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "pankajmathur/orca_mini_3b": {"_name_or_path": "openlm-research/open_llama_3b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000}, "fffrrt/ruGPT-3.5-13B-GPTQ": {"_name_or_path": "ai-forever/ruGPT-3.5-13B", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 50272}, "kykim/gpt3-kor-small_based_on_gpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 3, "embd_pdrop": 0.1, "eos_token_id": 3, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 2048, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "use_cache": true, "vocab_size": 42000}, "PAIXAI/Astrid-1B-CPU": {"_name_or_path": "EleutherAI/pythia-1b-deduped", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 0, "classifier_dropout": 0.1, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout_prob": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "ElnaggarLab/ankh-large": {"_name_or_path": "agemagician/protx-large-1gspan-partreconstruction-20mlmp-encl48-decl24-ramd128-ranb64-dmodel1536-resume-17", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3840, "d_kv": 64, "d_model": 1536, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.0, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 48, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 64, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.22.0", "use_cache": true, "vocab_size": 144}, "togethercomputer/RedPajama-INCITE-7B-Chat": {"_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-7B-v1", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "ramsrigouthamg/t5_squad_v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "KETI-AIR/ke-t5-base": {"_name_or_path": "hf/ke-t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "pad_token_id": 0, "relative_attention_num_buckets": 32, "transformers_version": "4.4.2", "use_cache": true, "vocab_size": 64128}, "sentence-transformers/gtr-t5-base": {"_name_or_path": "models/gtr-t5-base", "architectures": ["T5EncoderModel"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "ramsrigouthamg/t5-large-paraphraser-diverse-high-quality": {"_name_or_path": "t5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "use_cache": true, "vocab_size": 32128}, "rinna/japanese-gpt2-small": {"_name_or_path": "rinna/japanese-gpt2-small", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 1, "embd_pdrop": 0.1, "eos_token_id": 2, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.6.1", "use_cache": true, "vocab_size": 32000}, "rinna/bilingual-gpt-neox-4b-instruction-ppo": {"architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 2, "classifier_dropout": 0.1, "eos_token_id": 3, "hidden_act": "gelu", "hidden_dropout": 0.1, "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rope_scaling": null, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 65536}, "ramsrigouthamg/t5_boolean_questions": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "philschmid/flan-t5-base-samsum": {"_name_or_path": "google/flan-t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 32128}, "google/t5-small-lm-adapt": {"_name_or_path": "./", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.13.0.dev0", "use_cache": true, "vocab_size": 32128}, "matsuo-lab/weblab-10b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4864, "initializer_range": 0.02, "intermediate_size": 19456, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 38, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "stabilityai/stablecode-completion-alpha-3b-4k": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 49152}, "IDEA-CCNL/Ziya-LLaMA-7B-Reward": {"_name_or_path": "IDEA-CCNL/Ziya-LLaMA-7B-Reward", "architectures": ["LlamaRewardModel"], "auto_map": {"AutoModelForSequenceClassification": "modeling_llama_rm.LlamaRewardModel"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32001}, "ichitaka/falcon-40b-instruct-8bit": {"_name_or_path": "tiiuae/falcon-40b-instruct", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "tiiuae/falcon-40b-instruct--configuration_RW.RWConfig", "AutoModelForCausalLM": "tiiuae/falcon-40b-instruct--modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "quantization_config": {"bnb_4bit_compute_dtype": "float32", "bnb_4bit_quant_type": "fp4", "bnb_4bit_use_double_quant": false, "llm_int8_enable_fp32_cpu_offload": false, "llm_int8_has_fp16_weight": false, "llm_int8_skip_modules": null, "llm_int8_threshold": 6.0, "load_in_4bit": false, "load_in_8bit": true}, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 65024}, "TheBloke/WizardCoder-Python-13B-V1.0-GPTQ": {"_name_or_path": "codellama/CodeLlama-13b-Python-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "togethercomputer/Pythia-Chat-Base-7B": {"_name_or_path": "togethercomputer/Pythia-Chat-Base-7B-v0.16", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.21.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "TheBloke/wizardLM-7B-HF": {"_name_or_path": "/workspace/victor123_WizardLM/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32001}, "AUTOMATIC/promptgen-majinai-unsafe": {"_name_or_path": "distilgpt2", "_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 50257}, "pinkmanlove/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "lmsys/longchat-13b-16k": {"_name_or_path": "longchat_13b_16K", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_sequence_length": 16384, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "rope_condense_ratio": 8}, "togethercomputer/RedPajama-INCITE-7B-Instruct": {"_name_or_path": "togethercomputer/RedPajama-INCITE-7B-Instruct", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "lmsys/vicuna-13b-v1.1": {"_name_or_path": "/workspace/llama-13B-HF", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "Salesforce/codet5-large": {"architectures": ["T5WithLMHeadModel"], "bos_token_id": 1, "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 32100}, "FredZhang7/anime-anything-promptgen-v2": {"_name_or_path": "./config.json", "_num_labels": 1, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6, "n_positions": 1024, "pad_token_id": 50256, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 76, "temperature": 0.9, "top_k": 4, "repetition_penalty": 1.2, "early_stopping": true}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 50257}, "Salesforce/xgen-7b-8k-inst": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 50256, "eos_token_id": 50256, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 51200}, "jojo0217/step3_mk7": {"_name_or_path": "jojo0217/ChatSKKU12.8BSFT", "architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.0, "bos_token_id": 0, "classifier_dropout": 0.1, "dropout": 0.0, "end_token_id": 2, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout": 0.0, "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 40, "num_steps": "global_step301000", "pad_token_id": 0, "rope_scaling": null, "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30008}, "EleutherAI/pythia-14m": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 128, "initializer_range": 0.02, "intermediate_size": 512, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 4, "num_hidden_layers": 6, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "cerebras/Cerebras-GPT-590M": {"model_type": "gpt2", "attn_pdrop": 0.0, "scale_attn_weights": true, "resid_pdrop": 0.0, "n_inner": 6144, "n_embd": 1536, "layer_norm_epsilon": 1e-05, "n_positions": 2048, "activation_function": "gelu", "n_head": 12, "n_layer": 18, "tie_word_embeddings": true, "vocab_size": 50257, "embd_pdrop": 0.0}, "dbmdz/german-gpt2": {"_name_or_path": "./", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.0, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 50265}, "KoboldAI/GPT-NeoX-20B-Skein": {"architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.1, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0.1, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.19.0.dev0", "use_cache": true, "adventure": true, "rep_pen": 1.03, "max_length": 2048, "vocab_size": 50432}, "beomi/polyglot-ko-12.8b-safetensors": {"_name_or_path": "EleutherAI/polyglot-ko-12.8b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 40, "num_steps": "global_step301000", "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "sentence-transformers/sentence-t5-base": {"_name_or_path": "models/sentence-t5-base", "architectures": ["T5EncoderModel"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "decapoda-research/llama-65b-hf": {"architectures": ["LLaMAForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 8192, "intermediate_size": 22016, "initializer_range": 0.02, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": -1, "rms_norm_eps": 1e-05, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32000}, "mesolitica/finetune-translation-t5-small-standard-bahasa-cased": {"_name_or_path": "finetune-t5-small-standard-bahasa-cased/checkpoint-2370000", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "inputs_length": 1024, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 1024, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.21.2", "use_cache": true, "vocab_size": 32100}, "marcsun13/bloom-1b7_with_lm_head": {"_name_or_path": "bigscience/bloom-1b7", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 16, "n_inner": null, "n_layer": 24, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 2, "seq_length": 4096, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "unk_token_id": 0, "use_cache": true, "vocab_size": 250880}, "MBZUAI/LaMini-Flan-T5-783M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/flan-t5-large-distil-v2_checkpoint/checkpoint-12500", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32128}, "medalpaca/medalpaca-13b": {"_name_or_path": "decapoda-research/llama-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32001}, "JulesBelveze/t5-small-headline-generator": {"_name_or_path": "checkpoint-14000/", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.20.1", "use_cache": true, "vocab_size": 32100}, "Michau/t5-base-en-generate-headline": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "TheBloke/Falcon-180B-Chat-GPTQ": {"_name_or_path": "/workspace/process/tiiuae_falcon-180b-chat/gptq/gptq-4bit-128g-actorder_True/", "alibi": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 14848, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_position_embeddings": 2048, "model_type": "falcon", "multi_query": true, "new_decoder_architecture": true, "num_attention_heads": 232, "num_hidden_layers": 80, "num_kv_heads": 8, "parallel_attn": true, "quantization_config": {"batch_size": 1, "bits": 4, "block_name_to_quantize": null, "damp_percent": 0.1, "dataset": null, "desc_act": true, "disable_exllama": true, "group_size": 128, "model_seqlen": null, "module_name_preceding_first_block": null, "pad_token_id": null, "quant_method": "gptq", "sym": true, "tokenizer": null, "true_sequential": true, "use_cuda_fp16": false}, "rope_scaling": null, "rope_theta": 10000.0, "torch_dtype": "float16", "transformers_version": "4.33.0", "use_cache": true, "vocab_size": 65024}, "Salesforce/xgen-7b-8k-base": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 50256, "eos_token_id": 50256, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 51200}, "ai-forever/ruT5-base": {"_name_or_path": "/home/jovyan/models/t5/t5_base_org", "_num_labels": 2, "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.5.1", "use_cache": true, "vocab_size": 32128}, "KRAFTON/KORani-v3-13B": {"_name_or_path": "decapoda-research/llama-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32001}, "bigscience/mt0-xxl-mt": {"_name_or_path": "google/mt5-xxl", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "garage-bAInd/Stable-Platypus2-13B": {"_name_or_path": "stabilityai/StableBeluga-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TheBloke/Wizard-Vicuna-13B-Uncensored-HF": {"_name_or_path": "/workspace/models/ehartford_Wizard-Vicuna-13B-Uncensored/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-oasst1-512-12b": {"_name_or_path": "EleutherAI/pythia-12b-deduped", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "Parth/result": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "declare-lab/flan-alpaca-large": {"_name_or_path": "google/flan-t5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.1", "use_cache": true, "vocab_size": 32128}, "sdadas/mt5-base-translator-en-pl": {"_name_or_path": "original/mt5-base-translator-en-pl", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 250100}, "ziqingyang/chinese-llama-2-7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 55296}, "NousResearch/Nous-Hermes-13b": {"_name_or_path": "elinas/llama-13b-hf-transformers-4.29", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32001}, "pragmatic-programs/listener-suffix-idx-300k": {"_name_or_path": "listener-specs-suffix-idx/checkpoint-9375", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_kv": 64, "d_model": 1472, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 4, "num_heads": 6, "num_layers": 12, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 384}, "jinaai/jina-embedding-l-en-v1": {"_name_or_path": "tmp/", "architectures": ["T5EncoderModel"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32128}, "stabilityai/stablelm-base-alpha-3b": {"_name_or_path": "/fsx/ckpts/3b_tok=neox_data=pilev2-recontam=p3_lower-code_bs=8m_tp=4_pp=1_init=wang-small-init/global_step84000_hf", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "razent/SciFive-base-Pubmed_PMC": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 1024, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "use_cache": true, "vocab_size": 32128}, "uer/gpt2-chinese-poem": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "embd_pdrop": 0.1, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "output_past": true, "resid_pdrop": 0.1, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 120}}, "tokenizer_class": "BertTokenizer", "vocab_size": 22557}, "openchat/openchat_v3.1": {"_name_or_path": "imone/LLaMA2_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": false, "vocab_size": 32002}, "IDEA-CCNL/Ziya-LLaMA-13B-v1": {"_name_or_path": "path/to/your/checkpoints", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 39424}, "Sao10K/Mythical-Destroyer-V2-L2-13B": {"_name_or_path": "Mythical-Destroyer-V2-L2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.1", "use_cache": true, "vocab_size": 32000}, "juierror/text-to-sql-with-table-schema": {"_name_or_path": "./results/checkpoint-53000", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 32128}, "MingZhong/unieval-fact": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.17.0.dev0", "use_cache": true, "vocab_size": 32100}, "TheBloke/vicuna-13B-v1.5-GPTQ": {"_name_or_path": "vicuna-13b-v1.5", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "cerebras/Cerebras-GPT-256M": {"model_type": "gpt2", "attn_pdrop": 0.0, "scale_attn_weights": true, "resid_pdrop": 0.0, "n_inner": 4352, "n_embd": 1088, "layer_norm_epsilon": 1e-05, "n_positions": 2048, "activation_function": "gelu", "n_head": 17, "n_layer": 14, "tie_word_embeddings": true, "vocab_size": 50257, "embd_pdrop": 0.0}, "declare-lab/flan-alpaca-base": {"_name_or_path": "google/flan-t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.1", "use_cache": true, "vocab_size": 32128}, "ehartford/WizardLM-1.0-Uncensored-Llama2-13b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "aubmindlab/aragpt2-base": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50, "num_beams": 5, "top_p": 0.95, "repetition_penalty": 3.0, "no_repeat_ngram_size": 3}}, "use_cache": true, "vocab_size": 64000}, "valhalla/t5-small-e2e-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_de": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_fr": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}, "translation_en_to_ro": {"early_stopping": true, "length_penalty": 1.5, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "generate questions: "}}, "vocab_size": 32102}, "elinas/llama-7b-hf-transformers-4.29": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "lmsys/vicuna-13b-delta-v1.1": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-13b-v1.1-fp16/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "garage-bAInd/Platypus2-7B": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "PKU-Alignment/beaver-7b-v1.0-cost": {"architectures": ["LlamaModelForScore"], "bias": false, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 32000, "rms_norm_eps": 1e-06, "score_dim": 1, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32001}, "allenai/unifiedqa-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b": {"_name_or_path": "openlm-research/open_llama_7b", "architectures": ["LlamaForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 1, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 2, "hidden_act": "silu", "hidden_dropout_prob": 0.0, "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "JackFram/llama-160m": {"_name_or_path": "JackFram/llama-160m", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 12, "num_hidden_layers": 12, "pad_token_id": 1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "daryl149/llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "akreal/tiny-random-t5": {"bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 0.002, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_num_buckets": 8, "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 99}, "cyberagent/open-calm-medium": {"_name_or_path": "open-calm-medium", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "use_parallel_residual": false, "vocab_size": 52096}, "The-Face-Of-Goonery/Huginn-13b-FP16": {"_name_or_path": "C:/Users/caleb/Desktop/AI stuff/oobabooga_windows/text-generation-webui/models/converted_model", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "facebook/tart-full-flan-t5-xl": {"_name_or_path": "google/flan-t5-large", "architectures": ["EncT5ForSequenceClassification"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "finetuning_task": "ce", "id2label": {"0": 0, "1": 1}, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "label2id": {"0": 0, "1": 1}, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "problem_type": "single_label_classification", "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 32128}, "csebuetnlp/banglat5_banglaparaphrase": {"_name_or_path": "csebuetnlp/banglat5", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.11.0.dev0", "use_cache": true, "vocab_size": 32100}, "FlagAlpha/Llama2-Chinese-7b-Chat": {"_name_or_path": "meta-llama/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "jerryjalapeno/Llama-2-1b-0-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 16, "num_hidden_layers": 24, "num_key_value_heads": 16, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "NousResearch/Redmond-Puffin-13B": {"_name_or_path": "output/puffin-v1.3-4k-sharegpt-llama-2-13b/checkpoint-550", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32032}, "bigscience/bloomz": {"apply_residual_connection_post_layernorm": false, "attention_dropout": 0.0, "architectures": ["BloomForCausalLM"], "attention_softmax_in_fp32": true, "seq_length": 2048, "pad_token_id": 3, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_embed": 14336, "n_layer": 70, "num_attention_heads": 112, "pretraining_tp": 4, "slow_but_exact": false, "transformers_version": "4.21.0", "use_cache": true, "vocab_size": 250880}, "allenai/unifiedqa-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "WizardLM/WizardMath-7B-V1.0": {"_name_or_path": "/model_weights/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "pragmatic-programs/speaker-prefix-idx-300k": {"_name_or_path": "speaker-specs-prefix-idx/checkpoint-9375", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_kv": 64, "d_model": 1472, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 4, "num_heads": 6, "num_layers": 12, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 384}, "TheBloke/CodeLlama-13B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "TheBloke/Upstage-Llama-2-70B-instruct-v2-GPTQ": {"_name_or_path": "output/llama2-70b-alpaca_gpt4/last", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "pinkmanlove/llama-13b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "VietAI/envit5-translation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50048}, "cerebras/Cerebras-GPT-2.7B": {"model_type": "gpt2", "attn_pdrop": 0.0, "scale_attn_weights": true, "resid_pdrop": 0.0, "n_inner": 10240, "n_embd": 2560, "layer_norm_epsilon": 1e-05, "n_positions": 2048, "activation_function": "gelu", "n_head": 32, "n_layer": 32, "tie_word_embeddings": true, "vocab_size": 50257, "embd_pdrop": 0.0}, "Open-Orca/LlongOrca-7B-16k": {"_name_or_path": "conceptofmind/LLongMA-2-7b-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32003}, "hf-internal-testing/tiny-random-T5ForConditionalGeneration": {"_name_or_path": "tiny_models/t5/T5ForConditionalGeneration", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32100}, "juierror/flan-t5-text2sql-with-schema-v2": {"_name_or_path": "./model/checkpoint-1905", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32101}, "BeIR/query-gen-msmarco-t5-base-v1": {"_name_or_path": "model_raw/t5-base/model.ckpt-1004000", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.2.2", "use_cache": true, "vocab_size": 32128}, "conceptofmind/LLongMA-2-13b-16k": {"_name_or_path": "NousResearch/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32000}, "NousResearch/Yarn-Llama-2-13b-128k": {"_name_or_path": "/fsx/proj-trlx/conceptofmind/scaled-rope-private/save_model/final/yarn-llama-2-13b-3e-5-64k-600steps-32", "architectures": ["LlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModelForCausalLM": "modeling_llama_together_yarn.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 131072, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 32.0, "original_max_position_embeddings": 4096, "type": "yarn", "finetuned": true}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "wangrongsheng/MiniGPT-4-LLaMA": {"_name_or_path": "./llama-13b-hf/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32001}, "hf-internal-testing/tiny-random-GPT2ForSequenceClassification": {"_name_or_path": "temp/dummy/gpt2/GPT2ForSequenceClassification", "activation_function": "gelu", "architectures": ["GPT2ForSequenceClassification"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "zenham/wail_m_e4_16h_2k": {"_name_or_path": "microsoft/DialoGPT-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 50257}, "h2oai/h2ogpt-4096-llama2-7b": {"_name_or_path": "h2oai/h2ogpt-4096-llama2-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "ai-forever/FRED-T5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.8.0", "use_cache": true, "vocab_size": 50364}, "FreedomIntelligence/phoenix-inst-chat-7b": {"_name_or_path": "phoenix-inst-chat-7b", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 4096, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 32, "n_inner": null, "n_layer": 30, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "unk_token_id": 0, "use_cache": true, "vocab_size": 250880}, "castorini/monot5-base-msmarco": {"_num_labels": 2, "architectures": ["T5ForConditionalGeneration"], "bos_token_id": null, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "do_sample": false, "dropout_rate": 0.1, "early_stopping": false, "eos_token_id": 1, "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "initializer_factor": 1.0, "is_decoder": false, "is_encoder_decoder": true, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "layer_norm_epsilon": 1e-06, "length_penalty": 1.0, "max_length": 20, "min_length": 0, "model_type": "t5", "n_positions": 512, "no_repeat_ngram_size": 0, "num_beams": 1, "num_heads": 12, "num_layers": 12, "num_return_sequences": 1, "output_attentions": false, "output_hidden_states": false, "output_past": true, "pad_token_id": 0, "prefix": null, "pruned_heads": {}, "relative_attention_num_buckets": 32, "repetition_penalty": 1.0, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "torchscript": false, "use_bfloat16": false, "vocab_size": 32128}, "minlik/chinese-alpaca-plus-7b-merged": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 49954}, "joaogante/tiny-random-gpt2-with-generation-config": {"activation_function": "gelu_new", "attention_probs_dropout_prob": 0.1, "attn_pdrop": 0.1, "bos_token_id": 98, "embd_pdrop": 0.1, "eos_token_id": 98, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "initializer_range": 0.02, "intermediate_size": 37, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 512, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5, "n_positions": 512, "pad_token_id": 98, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.11.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1000}, "neulab/gpt2-finetuned-wikitext103": {"_name_or_path": "models/finetune_gpt2_wikitext103", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.14.1", "use_cache": true, "vocab_size": 50257}, "jarradh/llama2_70b_chat_uncensored": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TigerResearch/tigerbot-13b-base": {"_name_or_path": "/mnt/nfs/yechen/models/tigerbot-13b-llama2-base-2h-hf-25000", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 8, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 60928}, "rinna/japanese-gpt-neox-3.6b-instruction-sft-v2": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "eos_token_id": 3, "hidden_act": "gelu", "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 32000}, "bofenghuang/vigogne-2-7b-chat": {"_name_or_path": "outputs/chat/llama-2-7b-ft-chat-max2048-packing2048-ep3-bs128-lr1e4-gpt3_5data-merged", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/stable-vicuna-13B-HF": {"_name_or_path": "/workspace/huggyllama_llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32001}, "aiplanet/effi-13b": {"_name_or_path": "meta-llama/Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-33b-gpt4-m2.0": {"_name_or_path": "llama-30b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "num_key_value_heads": 52, "pad_token_id": -1, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TheBloke/orca_mini_v3_13B-GPTQ": {"_name_or_path": "TheBloke/Llama-2-13B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "HuggingFaceH4/starchat-alpha": {"_name_or_path": "/fsx/h4/checkpoints/starchat", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "validate_runner_input": true, "vocab_size": 49156}, "WizardLM/WizardMath-13B-V1.0": {"_name_or_path": "/model_weights/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "upstage/Llama-2-70b-instruct": {"_name_or_path": "upstage/Llama-2-70b-instruct-1024", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "anushehchaudry/llama-2-tiny-random": {"_name_or_path": "summarization_policy_new/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "end_token_id": 2, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8, "initializer_range": 0.02, "intermediate_size": 32, "is_decoder": true, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 2, "num_hidden_layers": 1, "num_key_value_heads": 2, "pad_token_id": 2, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0", "use_cache": false, "vocab_size": 32000}, "fangloveskari/ORCA_LLaMA_70B_QLoRA": {"_name_or_path": "/home/jovyan/.cache/huggingface/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "HyperbeeAI/Tulpar-7b-v0": {"_name_or_path": "HyperbeeAI/Tulpar-7b-v0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TheBloke/Llama-2-70B-Chat-fp16": {"_name_or_path": "meta-llama/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "csebuetnlp/mT5_m2m_crossSum_enhanced": {"_name_or_path": "google/mt5-base", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "length_penalty": 0.6, "max_length": 84, "model_type": "mt5", "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"langid_map": {"amharic": [35, "\u2581<extra_id_64>"], "arabic": [4, "\u2581<extra_id_95>"], "azerbaijani": [7, "\u2581<extra_id_92>"], "bengali": [42, "\u2581<extra_id_57>"], "burmese": [33, "\u2581<extra_id_66>"], "chinese_simplified": [40, "\u2581<extra_id_59>"], "chinese_traditional": [44, "\u2581<extra_id_55>"], "english": [30, "\u2581<extra_id_69>"], "french": [10, "\u2581<extra_id_89>"], "gujarati": [27, "\u2581<extra_id_72>"], "hausa": [43, "\u2581<extra_id_56>"], "hindi": [21, "\u2581<extra_id_78>"], "igbo": [9, "\u2581<extra_id_90>"], "indonesian": [1, "\u2581<extra_id_98>"], "japanese": [37, "\u2581<extra_id_62>"], "kirundi": [0, "\u2581<extra_id_99>"], "korean": [29, "\u2581<extra_id_70>"], "kyrgyz": [5, "\u2581<extra_id_94>"], "marathi": [13, "\u2581<extra_id_86>"], "nepali": [20, "\u2581<extra_id_79>"], "oromo": [41, "\u2581<extra_id_58>"], "pashto": [34, "\u2581<extra_id_65>"], "persian": [23, "\u2581<extra_id_76>"], "pidgin": [14, "\u2581<extra_id_85>"], "portuguese": [39, "\u2581<extra_id_60>"], "punjabi": [17, "\u2581<extra_id_82>"], "russian": [36, "\u2581<extra_id_63>"], "scottish_gaelic": [24, "\u2581<extra_id_75>"], "serbian_cyrillic": [28, "\u2581<extra_id_71>"], "serbian_latin": [11, "\u2581<extra_id_88>"], "sinhala": [31, "\u2581<extra_id_68>"], "somali": [19, "\u2581<extra_id_80>"], "spanish": [3, "\u2581<extra_id_96>"], "swahili": [18, "\u2581<extra_id_81>"], "tamil": [32, "\u2581<extra_id_67>"], "telugu": [22, "\u2581<extra_id_77>"], "thai": [6, "\u2581<extra_id_93>"], "tigrinya": [16, "\u2581<extra_id_83>"], "turkish": [15, "\u2581<extra_id_84>"], "ukrainian": [2, "\u2581<extra_id_97>"], "urdu": [38, "\u2581<extra_id_61>"], "uzbek": [8, "\u2581<extra_id_91>"], "vietnamese": [12, "\u2581<extra_id_87>"], "welsh": [26, "\u2581<extra_id_73>"], "yoruba": [25, "\u2581<extra_id_74>"]}}, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 250112}, "TheBloke/Genz-70b-GPTQ": {"_name_or_path": "budecosystem/genz-70b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "cerebras/Cerebras-GPT-6.7B": {"model_type": "gpt2", "n_embd": 4096, "tie_word_embeddings": true, "n_positions": 2048, "vocab_size": 50257, "n_layer": 32, "resid_pdrop": 0.0, "layer_norm_epsilon": 1e-05, "n_head": 32, "scale_attn_weights": true, "attn_pdrop": 0.0, "n_inner": 16384, "activation_function": "gelu", "embd_pdrop": 0.0}, "ziqingyang/chinese-alpaca-2-13b": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 55296}, "google/t5-small-ssm-nq": {"_name_or_path": "/home/patrick/hugging_face/t5/t5-small-ssm-nq", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "vocab_size": 32128}, "EleutherAI/polyglot-ko-3.8b": {"_name_or_path": "./polyglot-ko-3.8b/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 2, "hidden_act": "gelu", "hidden_size": 3072, "initializer_range": 0.02, "intermediate_size": 12288, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 24, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.5, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 30080}, "kashif/stack-llama-2": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "line-corporation/japanese-large-lm-1.7b": {"vocab_size": 51200, "n_positions": 2048, "n_embd": 2304, "n_layer": 24, "n_head": 24, "n_inner": 9216, "activation_function": "gelu", "resid_pdrop": 0.1, "embd_pdrop": 0.1, "attn_pdrop": 0.1, "layer_norm_epsilon": 1e-05, "initializer_range": 0.02, "summary_type": "cls_index", "summary_use_proj": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "scale_attn_weights": true, "use_cache": true, "scale_attn_by_inverse_layer_idx": false, "reorder_and_upcast_attn": false, "bos_token_id": 1, "eos_token_id": 2, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": null, "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": true, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "chunk_size_feed_forward": 0, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["GPT2LMHeadModel"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "pad_token_id": null, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "", "transformers_version": "4.28.1", "n_ctx": 2048, "gradient_checkpointing": false, "model_type": "gpt2"}, "microsoft/codereviewer": {"_name_or_path": "/content/drive/MyDrive/CodeT5/pretrained_models/codet5_base", "add_token_id": 32101, "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "del_token_id": 32102, "dropout_rate": 0.1, "end_token_id": 32104, "eos_token_id": 2, "feed_forward_proj": "relu", "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_factor": 1.0, "is_encoder_decoder": true, "keep_token_id": 32100, "label2id": {"LABEL_0": 0}, "lang_id": {"<c>": 32213, "<c_plus_plus>": 32215, "<c_sharp>": 32214, "<en>": 32206, "<go>": 32212, "<java>": 32208, "<javascript>": 32209, "<php>": 32211, "<python>": 32207, "<ruby>": 32210}, "lang_tokens": ["<en>", "<python>", "<java>", "<javascript>", "<ruby>", "<php>", "<go>", "<c>", "<c_sharp>", "<c_plus_plus>"], "layer_norm_epsilon": 1e-06, "mask_token_id": 4, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "start_token_id": 32103, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.15.0", "use_cache": true, "vocab_size": 32216}, "TheBloke/guanaco-7B-HF": {"_name_or_path": "/workspace/models/huggyllama_llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "circulus/Llama-2-7b-orca-v1": {"_name_or_path": "circulus/Llama-2-7b-orca-v1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "FlagAlpha/Atom-7B": {"_name_or_path": "/home/user/data/data/base_model/AtoM-7B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 2, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.29.0", "use_cache": true, "vocab_size": 65000}, "Tap-M/Luna-AI-Llama2-Uncensored": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "K024/mt5-zh-ja-en-trimmed": {"_name_or_path": "K024/mt5-zh-ja-en-trimmed", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.15.0", "use_cache": true, "vocab_size": 85292}, "deep-learning-analytics/automatic-title-generation": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.10.0", "use_cache": true, "vocab_size": 32128}, "luodian/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "stabilityai/stablelm-base-alpha-7b": {"_name_or_path": "/fsx/ckpts/7b_tok=neox_data=pilev2-recontam_lower-code_bs=8m_tp=4_pp=1_init=wang-small-init/global_step69000_hf", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 48, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "OpenLemur/lemur-70b-chat-v1": {"_name_or_path": "/home/ubuntu/sft-models/lemur-70b-10k/sysdatav1/epoch_1_hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32005}, "rahular/varta-t5": {"_name_or_path": "t5_1_1_base_1M_pt", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 128128}, "rinna/japanese-gpt-neox-3.6b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 2, "eos_token_id": 3, "hidden_act": "gelu", "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 32000}, "garage-bAInd/Platypus-30B": {"_name_or_path": "./llama30B_hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "WizardLM/WizardCoder-Python-7B-V1.0": {"_name_or_path": "/workspaceblobstore/caxu/trained_models/7Bcodellamapython_v1_2048_e3_2e_5/checkpoint-195", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "chavinlo/gpt4-x-alpaca": {"_name_or_path": "./output", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32001}, "sentence-transformers/gtr-t5-xl": {"_name_or_path": "models/gtr-t5-3b", "architectures": ["T5EncoderModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "wangrongsheng/MiniGPT-4-LLaMA-7B": {"_name_or_path": "./llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32001}, "EleutherAI/pythia-12b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "unicamp-dl/translation-pt-en-t5": {"_name_or_path": "pt-en/", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "bigscience/mt0-base": {"_name_or_path": "google/mt5-base", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "Pirr/pythia-13b-deduped-green_devil": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50277, "welcome": "Let your fantasies run wild on our story-writing model `Green Devil`, created by Mr. Seeker for [Pirr](https://www.pirr.me/).\n\nAre you playing on colab? We also have an app for [iPhone](https://apps.apple.com/us/app/pirr-ai/id1638980697), [Android](https://play.google.com/store/apps/details?id=me.pirr.app) and [web](https://www.pirr.app). Do share your best stories on our [Discord](https://discord.gg/CXGDNAuDsw)", "antemplate": "[Genre: <|>]"}, "trl-internal-testing/tiny-random-GPT2Model": {"_name_or_path": "hf-internal-testing/tiny-random-GPT2Model", "activation_function": "gelu", "architectures": ["GPT2Model"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.1", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "MBZUAI/LaMini-GPT-1.5B": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/gpt2-xl", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48, "n_positions": 1024, "output_past": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": false, "vocab_size": 50258}, "Universal-NER/UniNER-7B-all": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "TheBloke/koala-13B-HF": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "Rostlab/prot_t5_xl_bfd": {"architectures": ["T5WithLMHeadModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "vocab_size": 128}, "Voicelab/trurl-2-13b": {"_name_or_path": "/data/models/output/voicelab-llama2-13b-v4.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "explosion-testing/llama2-kv-sharing": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 256, "initializer_range": 0.02, "intermediate_size": 512, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 5, "num_key_value_heads": 1, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 1024}, "inpars/monot5-3b-inpars-v2-nq-promptagator": {"_name_or_path": "castorini/monot5-3b-msmarco-10k", "architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.6.1", "use_cache": true, "vocab_size": 32128}, "upstage/llama-65b-instruct": {"_name_or_path": "upstage/llama-65b-instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "microsoft/CodeGPT-small-py": {"_num_labels": 2, "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 2, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "output_past": true, "pad_token_id": 1, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50001}, "VietAI/vit5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.21.3", "use_cache": true, "vocab_size": 36096}, "TheBloke/CodeUp-Llama-2-13B-Chat-HF-GPTQ": {"_name_or_path": "meta-llama/Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/CodeLlama-34B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "elyza/ELYZA-japanese-Llama-2-7b": {"_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-7b": {"_name_or_path": "/workspace/airoboros-7b/checkpoint-225", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "TheBloke/CodeLlama-7B-Python-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "FlagAlpha/Llama2-Chinese-13b-Chat-4bit": {"_name_or_path": "/mnt/data/zhangzheng/data/llama2/sft_13b_chat/checkpoint-600_merge", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/CodeLlama-13B-Python-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "Enoch/llama-65b-hf": {"architectures": ["LLaMAForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "intermediate_size": 22016, "initializer_range": 0.02, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": 0, "rms_norm_eps": 1e-05, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "chargoddard/platypus-2-22b-relora": {"_name_or_path": "chargoddard/platypus2-22b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4098, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "togethercomputer/GPT-NeoXT-Chat-Base-20B": {"_name_or_path": "togethercomputer/GPT-NeoXT-Chat-Base-20B", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.21.1", "use_cache": true, "vocab_size": 50432}, "porkorbeef/Llama-2-13b-sf": {"_name_or_path": "./output/Llama-2-13b-chat-hf/truthfulqa_arc_2023_08_15_17:08:06/checkpoint-10", "architectures": ["LlamaModel"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "ehartford/Wizard-Vicuna-13B-Uncensored": {"_name_or_path": "/workspace/models/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "doas/test5": {"_name_or_path": "./output/Llama-2-13b-chat-hf/truthfulqa_gpt4_2023_08_14_11:45:59/checkpoint-9", "architectures": ["LlamaModel"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "klosax/open_llama_3b_350bt_preview": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "Writer/camel-5b-hf": {"_name_or_path": "./camel-5b", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.01, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 4096, "n_head": 32, "n_inner": 16384, "n_layer": 24, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 50258}, "Filosofas/DialoGPT-medium-PALPATINE2": {"_name_or_path": "microsoft/DialoGPT-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 50257}, "nomic-ai/gpt4all-falcon": {"_name_or_path": "tiiuae/falcon-7b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 65024}, "reciprocate/llama2-7b-gsm8k": {"_name_or_path": "checkpoints/-Llama-2-7b-hf-20230828-221255/checkpoint_702", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000}, "TheBloke/CodeLlama-13B-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "pankajmathur/orca_mini_v3_13b": {"_name_or_path": "TheBloke/Llama-2-13B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "projecte-aina/aguila-7b": {"_name_or_path": "/bscdata/models/falcon_7b_balanced_tokenizer_fp16/", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 50256, "eos_token_id": 50256, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "pad_token_id": 50256, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "max_position_embeddings": 2048, "vocab_size": 50257}, "TheBloke/WizardLM-13B-V1.1-GPTQ": {"_name_or_path": "/scratch/AzureBlobStorage_CODE/scratch/workspaceblobstore/caxu/model_weights/13B_sg_alpaca_prefix_mt_451k_step700_v11_768_e3_2e_5/checkpoint-700", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "MBZUAI/LaMini-GPT-124M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": false, "vocab_size": 50258}, "google/mt5-xxl": {"_name_or_path": "/home/patrick/t5/mt5-xxl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "vocab_size": 250112}, "MaRiOrOsSi/t5-base-finetuned-question-answering": {"_name_or_path": "./results/t5-base/checkpoint-11", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 32128}, "satvikag/chatbot": {"_name_or_path": "output-small", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "transformers_version": "4.6.1", "use_cache": true, "vocab_size": 50257}, "LMFlow/Robin-7b-v2": {"_name_or_path": "/home/rpan/lmflow-tmp/robin-7b-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "daryl149/llama-2-70b-chat-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "acrastt/Puma-3B": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "TheBloke/orca_mini_v3_7B-GPTQ": {"_name_or_path": "TheBloke/Llama-2-7B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "static_groups": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "taeminlee/kogpt2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "output_past": true, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50000}, "NousResearch/Llama-2-70b-chat-hf": {"_name_or_path": "meta-llama/Llama-2-70b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 8, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "rinna/japanese-gpt2-xsmall": {"_name_or_path": "rinna/japanese-gpt2-xsmall", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 1, "embd_pdrop": 0.1, "eos_token_id": 2, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 512, "n_head": 8, "n_inner": 2304, "n_layer": 6, "n_positions": 1024, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.8.2", "use_cache": true, "vocab_size": 32000}, "ziqingyang/chinese-llama-2-13b": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 55296}, "hf-internal-testing/tiny-random-t5-v1.1": {"bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 0.002, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_num_buckets": 8, "tie_word_embeddings": false, "transformers_version": "4.11.0.dev0", "use_cache": true, "vocab_size": 1103}, "pankajmathur/Lima_Unchained_70b": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "chargoddard/llama2-22b-blocktriangular": {"_name_or_path": "/workspace/llama2-22b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4098, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "TheBloke/CodeLlama-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}, "pad_token_id": 0}, "BeIR/query-gen-msmarco-t5-large-v1": {"_name_or_path": "model_raw/t5-large/model.ckpt-1004700", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.2.2", "use_cache": true, "vocab_size": 32128}, "TheBloke/CodeLlama-13B-Instruct-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "acrastt/Marx-3B": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "PygmalionAI/pygmalion-2-13b": {"_name_or_path": "PygmalionAI/pygmalion-2-13b", "architectures": ["LlamaForCausalLM"], "badwordsids": "[[29961], [14352], [24630], [29962], [11759], [15974], [5519], [25473], [18899], [25901], [7110], [9341], [13531], [518], [9310], [2636], [3366], [21069], [11970], [23098], [16733], [21298], [18173], [10846], [3816], [28513], [15625], [23192], [28166], [10062], [1385], [11724], [3108], [15555], [10834], [10370], [14330], [1822], [12436], [5262], [17094], [10725], [17077], [11424], [4197], [24406], [13359], [17531], [24566], [23076], [4514], [13192], [19942], [16261], [7072], [6024], [1402], [1839], [2033], [13970], [850], [5913], [28895], [5387], [8308], [24927], [5691], [12940], [19997], [18959], [11287], [16862], [4638], [22322], [29861], [21251], [14704], [17548], [12452], [17288], [23160], [24960], [8219], [18024], [5539], [7464], [27865], [29588], [20068], [19660], [27706], [22896], [24264], [12258], [2314], [4400], [5586], [12622], [6796], [7226], [21939], [18456], [14178], [21540], [21945], [14664], [16215], [10338], [17361], [7503], [13769], [26073], [9601], [26909], [7961], [8999], [20840], [16272], [21545], [3199], [10514], [5159], [22689], [6525], [20526], [27077], [18017]]", "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "shibing624/chinese-alpaca-plus-7b-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 49954}, "TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ": {"_name_or_path": "imone/LLaMA2_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32002, "pretraining_tp": 1, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "syzymon/long_llama_3b_instruct": {"architectures": ["LongLlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_longllama.LongLlamaConfig", "AutoModel": "modeling_longllama.LongLlamaModel", "AutoModelForCausalLM": "modeling_longllama.LongLlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_longllama.LongLlamaForSequenceClassification"}, "bos_token_id": 1, "eos_token_id": 2, "gradient_checkpoint_every_ith": 1, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "last_context_length": 1024, "max_position_embeddings": 2048, "mem_attention_grouping": null, "mem_dtype": "bfloat16", "mem_layers": [6, 12, 18], "mem_positionals": true, "model_type": "longllama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_attention": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0", "use_cache": true, "vocab_size": 32000}, "bofenghuang/vigogne-2-7b-instruct": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Gustavosta/MagicPrompt-Dalle": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.23.0.dev0", "use_cache": true, "vocab_size": 50257}, "muchad/idt5-qa-qg": {"_name_or_path": "muchad/idt5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.4.2", "use_cache": true, "vocab_size": 30002}, "TheBloke/vicuna-13b-v1.3.0-GPTQ": {"_name_or_path": "/home/ubuntu/model_weights/vicuna-13b-hao-sharegpt-20230515", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TigerResearch/tigerbot-13b-base-v1": {"_name_or_path": "tigerbot-13b-base/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 8, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 60928}, "ehartford/WizardLM-13B-Uncensored": {"_name_or_path": "/workspace/llama-13b-hf/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32001}, "clibrain/Llama-2-7b-ft-instruct-es": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "google/t5_xxl_true_nli_mixture": {"_name_or_path": "/usr/local/google/home/roeeaharoni/Downloads/32911087-smfpsv/checkpoint_1050000_hf", "architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 128, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 32128}, "unikei/t5-base-split-and-rephrase": {"_name_or_path": "unikei/t5-base-split-and-rephrase", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "max_length": 256, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {}, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": true, "vocab_size": 32128}, "microsoft/Promptist": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "stas/mt5-tiny-random": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 256, "d_kv": 8, "d_model": 64, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 4, "num_layers": 8, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "transformers_version": "4.6.0.dev0", "use_cache": true, "vocab_size": 5100}, "AIDC-ai-business/Luban-13B": {"_name_or_path": "/mnt/data/open_model/OpenOrcaxOpenChat-Preview2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32002}, "microsoft/GODEL-v1_1-base-seq2seq": {"_name_or_path": "GODEL-v1_1-base-seq2seq", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 32102}, "CalderaAI/30B-Lazarus": {"_name_or_path": "30B-Lazarus", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "acrastt/Marx-3B-V2": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-4096-llama2-70b": {"_name_or_path": "h2oai/h2ogpt-4096-llama2-70b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "ajibawa-2023/scarlett-33b": {"_name_or_path": ".", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-70b-2.1": {"_name_or_path": "airoboros-l2-70b-2.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "rubentito/vt5-base-spdocvqa": {"_name_or_path": "rubentito/t5-base-mpdocvqa", "architectures": ["HF_VT5"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "hidden_dropout_prob": 0.1, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_eps": 1e-12, "layer_norm_epsilon": 1e-06, "max_2d_position_embeddings": 1024, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.27.2", "use_cache": true, "visual_module_config": {"finetune": false, "model": "dit", "model_weights": "microsoft/dit-base-finetuned-rvlcdip"}, "vocab_size": 32128}, "aisquared/dlite-v2-774m": {"_name_or_path": "gpt2-large", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float16", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "elyza/ELYZA-japanese-Llama-2-7b-fast": {"_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b-fast", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 45043}, "quantumaikr/llama-2-70b-fb16-korean": {"_name_or_path": "llama-2-70b-fb16-korean", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "TheBloke/CodeLlama-34B-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "microsoft/DialogRPT-updown": {"activation_function": "gelu_new", "architectures": ["GPT2ForSequenceClassification"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257, "num_labels": 1, "pad_token_id": 50256}, "TheBloke/CodeLlama-34B-Instruct-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "garage-bAInd/Platypus2-13B": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "trl-internal-testing/tiny-BloomForCausalLM-correct-vocab": {"_name_or_path": "trl-internal-testing/tiny-random-BloomForCausalLM", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 1, "dtype": "float32", "eos_token_id": 2, "gradient_checkpointing": false, "hidden_dropout": 0.1, "hidden_size": 32, "id2label": {"0": "LABEL_0", "1": "LABEL_1", "2": "LABEL_2"}, "initializer_range": 0.02, "is_decoder": true, "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2}, "layer_norm_epsilon": 1e-05, "model_type": "bloom", "n_head": 4, "n_layer": 5, "n_positions": 512, "pad_token_id": 3, "pretraining_tp": 1, "seq_length": 7, "slow_but_exact": true, "torch_dtype": "float32", "transformers_version": "4.27.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 250880}, "TheBloke/Llama-2-7B-GGML": {"model_type": "llama"}, "TheBloke/Wizard-Vicuna-7B-Uncensored-HF": {"_name_or_path": "/workspace/models/ehartford_Wizard-Vicuna-7B-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "wenge-research/yayi-7b-llama2": {"_name_or_path": "wenge-research/yayi-7b-llama2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32005}, "coffeeee/nsfw-story-generator2": {"_name_or_path": "gpt2-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 50257}, "jondurbin/airoboros-33b-gpt4-2.0": {"_name_or_path": "airoboros-33b-gpt4-2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "num_key_value_heads": 52, "pad_token_id": -1, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "totally-not-an-llm/EverythingLM-13b-16k": {"_name_or_path": "/home/pikal/Coding/everythinglm/LLongMA-2-13b-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32000}, "datificate/gpt2-small-spanish": {"_name_or_path": "/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "use_cache": true, "vocab_size": 50257}, "mrm8488/t5-base-finetuned-wikiSQL": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128, "xla_device": true}, "bofenghuang/vigogne-2-13b-instruct": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "OpenAssistant/stablelm-7b-sft-v7-epoch-3": {"_name_or_path": ".saved/stablelm-7b-sft-1/checkpoint-12099/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 4096, "model_type": "gpt_neox", "num_attention_heads": 48, "num_hidden_layers": 16, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50288}, "bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16": {"_name_or_path": "../../../basemodels/llama-33b-xctx-PI-s16384-t8192", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 16384, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "rope_scaling": {"factor": 8.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "flozi00/codellama-34b-german-assistant-v1": {"_name_or_path": "Phind/Phind-CodeLlama-34B-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000}, "WizardLM/WizardCoder-1B-V1.0": {"_name_or_path": "bigcode/starcoderbase-1b", "activation_function": "gelu_pytorch_tanh", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": false, "validate_runner_input": true, "vocab_size": 49153}, "upstage/llama-30b-instruct": {"_name_or_path": "upstage/llama-30b-instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "ehartford/dolphin-llama2-7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Open-Orca/LlongOrca-13B-16k": {"_name_or_path": "conceptofmind/LLongMA-2-13b-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": false, "use_flash_attention": false, "vocab_size": 32004}, "NousResearch/Nous-Hermes-Llama2-70b": {"_name_or_path": "NousResearch/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32001}, "ml6team/mt5-small-german-query-generation": {"_name_or_path": "google/mt5-small", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 250112}, "bigscience/mt0-xxl": {"_name_or_path": "google/mt5-xxl", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 10240, "d_kv": 64, "d_model": 4096, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 64, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "EleutherAI/pythia-2.8b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "TheBloke/wizardLM-7B-GPTQ": {"_name_or_path": "/vc_data_blob/caxu/Llama-X/model_weights/checkpoint-2400", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "conceptofmind/LLongMA-2-7b-16k": {"_name_or_path": "NousResearch/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32000}, "lmsys/vicuna-7b-delta-v1.1": {"_name_or_path": "/home/haozhang/stage_weights/vicuna-7b-new-token/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "bofenghuang/vigogne-7b-instruct": {"_name_or_path": "huggyllama/llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "csebuetnlp/banglat5_nmt_en_bn": {"_name_or_path": "data_and_model/model", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "length_penalty": 0.6, "max_length": 128, "model_type": "t5", "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.9.2", "use_cache": true, "vocab_size": 32128}, "trl-internal-testing/tiny-random-T5Model": {"_name_or_path": "hf-internal-testing/tiny-random-T5Model", "architectures": ["T5Model"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 1302}, "OpenBuddy/openbuddy-llama2-70b-v10.1-bf16": {"_name_or_path": "openbuddy-llama2-70b-v10.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 37632}, "TheBloke/wizard-vicuna-13B-GPTQ": {"_name_or_path": "wizard_vicuna_13b_600_step", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "desc_act": false, "group_size": 128, "model_file_base_name": "model", "quant_method": "gptq"}}, "JosephusCheung/Guanaco": {"_name_or_path": "Guanaco", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "openchat/opencoderplus": {"_name_or_path": "/data/one/starcoderplus_with_eot", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": true, "validate_runner_input": true, "vocab_size": 49153}, "jacobmorrison/tk-instruct-large-lora-experiments": {"_name_or_path": "google/t5-large-lm-adapt", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 32100}, "PygmalionAI/metharme-1.3b": {"_name_or_path": "/home/alpin/ramdisk/trained-models/metharme-1.3b", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "TheBloke/orca_mini_13B-GPTQ": {"_name_or_path": "openlm-research/open_llama_13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-70m-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 512, "initializer_range": 0.02, "intermediate_size": 2048, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 6, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50304}, "project-baize/baize-v2-7b": {"_name_or_path": "../RL/Baize-v2-2-iter", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "line-corporation/japanese-large-lm-1.7b-instruction-sft": {"_name_or_path": "/vsfs/deepspeed_chat/base-models/line-gpt-japanese", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 1, "embd_pdrop": 0.1, "end_token_id": 2, "eos_token_id": 2, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 2304, "n_head": 24, "n_inner": 9216, "n_layer": 24, "n_positions": 2048, "pad_token_id": 2, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 51200}, "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GPTQ": {"_name_or_path": "WizardLM-13B-V1.0-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 8192, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoModel": "modelling_llama.LlamaModel", "AutoModelForCausalLM": "modelling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modelling_llama.LlamaForSequenceClassification"}, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/llama-2-70b-Guanaco-QLoRA-fp16": {"_name_or_path": "/workspace/process/lora_base/meta-llama_Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "MBZUAI/LaMini-Flan-T5-77M": {"_name_or_path": "google/flan-t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.27.0", "use_cache": true, "vocab_size": 32128}, "csebuetnlp/banglat5_nmt_bn_en": {"_name_or_path": "data_and_model/model", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "length_penalty": 0.6, "max_length": 128, "model_type": "t5", "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.9.2", "use_cache": true, "vocab_size": 32128}, "Trelis/Llama-2-7b-chat-hf-function-calling-v2": {"_name_or_path": "meta-llama/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "ehartford/Wizard-Vicuna-7B-Uncensored": {"_name_or_path": "/workspace/models/llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "llSourcell/medllama2_7b": {"_name_or_path": "doctorGPT", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "WizardLM/WizardLM-13B-V1.1": {"_name_or_path": "/scratch/AzureBlobStorage_CODE/scratch/workspaceblobstore/caxu/model_weights/13B_sg_alpaca_prefix_mt_451k_step700_v11_768_e3_2e_5/checkpoint-700", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": false, "vocab_size": 32001}, "Gryphe/MythoMix-L2-13b": {"_name_or_path": "mythomix-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "TheBloke/StableBeluga2-70B-GPTQ": {"_name_or_path": "/fsx/dakota/orca/tmp_orca", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "VietAI/vit5-large-vietnews-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 256, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarization: "}}, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 36096}, "adasnew/t5-small-xsum": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 32128}, "Intel/t5-small-xsum-int8-dynamic": {"_name_or_path": "adasnew/t5-small-xsum", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "int8", "transformers_version": "4.21.1", "use_cache": true, "vocab_size": 32100}, "daspartho/prompt-extend": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 128, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 52000}, "EleutherAI/pythia-160m-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 12, "num_hidden_layers": 12, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50304}, "Ar4ikov/gpt2-650k-stable-diffusion-prompt-generator": {"_name_or_path": "Ar4ikov/gpt2-650k-stable-diffusion-prompt-generator", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": true, "vocab_size": 50257}, "ehartford/WizardLM-Uncensored-Falcon-7b": {"_name_or_path": "/workspace/models/WizardLM-Uncensored-falcon-7b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM"}, "bias": false, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 65025}, "CobraMamba/mamba-gpt-3b-v3": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TheBloke/llama2_70b_chat_uncensored-GPTQ": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "ai-forever/FRED-T5-1.7B": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1536, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 24, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "transformers_version": "4.8.0", "use_cache": true, "vocab_size": 50364}, "MBZUAI/LaMini-Cerebras-590M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/Cerebras-GPT-590M", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 1536, "n_head": 12, "n_inner": 6144, "n_layer": 18, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": false, "vocab_size": 50258}, "mrm8488/llama-2-coder-7b": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "guardrail/llama-2-7b-guanaco-instruct-sharded": {"_name_or_path": "mlabonne/llama-2-7b-guanaco", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "rinna/bilingual-gpt-neox-4b-8k": {"architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.1, "bos_token_id": 2, "classifier_dropout": 0.1, "eos_token_id": 3, "hidden_act": "gelu", "hidden_dropout": 0.1, "hidden_size": 2816, "initializer_range": 0.02, "intermediate_size": 11264, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 22, "num_hidden_layers": 36, "rope_scaling": {"type": "linear", "factor": 4.0}, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "use_cache": true, "use_parallel_residual": false, "vocab_size": 65536}, "mrm8488/falcoder-7b": {"_name_or_path": "ybelkada/falcon-7b-sharded-bf16", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "tiiuae/falcon-7b--configuration_RW.RWConfig", "AutoModel": "tiiuae/falcon-7b--modelling_RW.RWModel", "AutoModelForCausalLM": "tiiuae/falcon-7b--modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "tiiuae/falcon-7b--modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "tiiuae/falcon-7b--modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "tiiuae/falcon-7b--modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 4544, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWebModel", "multi_query": true, "n_head": 71, "n_layer": 32, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 65024}, "circulus/Llama-2-13b-orca-v1": {"_name_or_path": "circulus/Llama-2-13b-orca-v1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "allenai/tk-instruct-3b-def": {"_name_or_path": "google/t5-xl-lm-adapt", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.17.0.dev0", "use_cache": true, "vocab_size": 32100}, "pierreguillou/gpt2-small-portuguese": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50257}, "junelee/wizard-vicuna-13b": {"_name_or_path": "wizard_vicuna_13b_600_step", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "castorini/monot5-3b-msmarco-10k": {"_name_or_path": "3B/", "architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 32128}, "TheBloke/Llama-2-70B-Chat-GGML": {"model_type": "llama"}, "TheBloke/CodeLlama-7B-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32016, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "yeontaek/llama-2-13B-ensemble-v5": {"_name_or_path": "Open-Orca/OpenOrca-Platypus2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32002}, "ybelkada/flan-t5-xl-sharded-bf16": {"_name_or_path": "google/flan-t5-xl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32128}, "WizardLM/WizardCoder-3B-V1.0": {"_name_or_path": "bigcode/starcoderbase-3b", "activation_function": "gelu_pytorch_tanh", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 2816, "n_head": 22, "n_inner": 11264, "n_layer": 36, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": false, "validate_runner_input": true, "vocab_size": 49153}, "Langboat/mengzi-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.9.2", "use_cache": true, "vocab_size": 32128}, "MBZUAI/LaMini-GPT-774M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/gpt2-large", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": false, "vocab_size": 50258}, "ToddGoldfarb/Cadet-Tiny": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32128}, "TigerResearch/tigerbot-7b-base": {"_name_or_path": "./tigerbot-7b-llama2-base-2h-hf-35000/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 4, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 60928}, "UrukHan/t5-russian-spell": {"_name_or_path": "UrukHan/t5-russian-spell", "_num_labels": 2, "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "max_length": 256, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 32128}, "LinkSoul/Chinese-Llama-2-7b-4bit": {"_name_or_path": "./LinkSoul/Chinese-Llama-2-7b-4bit", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "TheBloke/Vicuna-13B-CoT-fp16": {"_name_or_path": "/mnt/data1/sheshuaijie/Code/Alpaca-CoT/mnt/data1/sheshuaijie/Output/CoT/Trained/vicuna-13b_english-cot+auto-cot_0.0002/merged", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "EleutherAI/pythia-1.4b-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50304}, "MayaPH/GodziLLa2-70B": {"_name_or_path": "GodziLLa2-70B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "TheBloke/wizardLM-13B-1.0-fp16": {"_name_or_path": "/workspace/process/wizardLM-13B-1.0/fp32", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32001}, "Gryphe/MythoBoros-13b": {"_name_or_path": "mythoboros-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "abacusai/Giraffe-v2-13b-32k": {"_name_or_path": "Giraffe-v2-13b-32k/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "jondurbin/airoboros-l2-13b-gpt4-1.4.1": {"_name_or_path": "airoboros-l2-13b-gpt4-1.4.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "razent/SciFive-base-Pubmed": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 1024, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "use_cache": true, "vocab_size": 32128}, "TehVenom/Pygmalion-13b-Merged": {"_name_or_path": "pygmalion-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "badwordsids": [[0]]}, "garage-bAInd/SuperPlatty-30B": {"_name_or_path": "lilloukas/Platypus-30B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-7b-gpt4-2.0": {"_name_or_path": "airoboros-l2-7b-gpt4-2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "Rostlab/ProstT5": {"_name_or_path": "Rostlab/prot_t5_xl_uniref50", "architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "torch_dtype": "float32", "transformers_version": "4.26.0.dev0", "use_cache": true, "vocab_size": 150}, "TheBloke/guanaco-7B-GPTQ": {"_name_or_path": "/workspace/models/huggyllama_llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "JackFram/llama-68m": {"_name_or_path": "JackFram/llama-68m", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 12, "num_hidden_layers": 2, "pad_token_id": 1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "MBZUAI/LaMini-Cerebras-111M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/Cerebras-GPT-111M", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.27.4", "use_cache": false, "vocab_size": 50258}, "ehartford/Wizard-Vicuna-30B-Uncensored": {"_name_or_path": "/workspace/wizard-vicuna-30b-uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "stockmark/gpt-neox-japanese-1.4b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 1024, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 1, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50000}, "TheBloke/MythoMax-L2-13B-GGML": {"model_type": "llama"}, "MBZUAI/LaMini-Cerebras-256M": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/Cerebras-GPT-256M", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": false, "vocab_size": 50258}, "jondurbin/airoboros-l2-13b-gpt4-2.0": {"_name_or_path": "airoboros-l2-13b-gpt4-2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "The-Face-Of-Goonery/Chronos-Beluga-v2-13bfp16": {"_name_or_path": "chronosfp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "lmqg/t5-base-squad-qag": {"_name_or_path": "t5-base-squad-qag", "add_prefix": true, "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.21.2", "use_cache": true, "vocab_size": 32101}, "Voicelab/trurl-2-7b": {"_name_or_path": "/data/models/llama-2-chat-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32001}, "ehartford/Samantha-1.11-13b": {"_name_or_path": "NousResearch/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "clibrain/Llama-2-13b-ft-instruct-es": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "deepse/CodeUp-Llama-2-13b-chat-hf": {"_name_or_path": "meta-llama/Llama-2-13b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "mrm8488/t5-base-finetuned-sarcasm-twitter": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "ToolBench/ToolLLaMA-7b": {"_name_or_path": "/mnt/data/user/tc_agi/qyj/llama-7b/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "marella/gpt-2-ggml": {"model_type": "gpt2"}, "Henk717/airochronos-33B": {"_name_or_path": "/home/mixer/koboldai/models/airoboros-33b-gpt4-1.4", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "stanford-crfm/alias-gpt2-small-x21": {"_name_or_path": "stanford-crfm/alias-gpt2-small-x21", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": true, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.12.5", "use_cache": false, "vocab_size": 50257}, "WizardLM/WizardLM-30B-V1.0": {"_name_or_path": "/workspaceblobstore/qins/llamax/trained_models/30B_sg_alpaca_prefix_mt_350k_2048_e3_2e_5/checkpoint-2049", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": false, "vocab_size": 32001}, "timdettmers/guanaco-33b-merged": {"_name_or_path": "/gscratch/zlab/llama/30B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "xkianteb/alg_ppo_separate_lr_1e-6_n_epochs_10_v_epochs_10_kl_target_1.0_clip_range_0.2": {"_name_or_path": "rajkumarrrk/gpt2-fine-tuned-on-imdb-positive-reviews", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "output_past": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 50257}, "TheBloke/wizard-mega-13B-GPTQ": {"_name_or_path": "/workspace/models/openaccess-ai-collective_wizard-mega-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "bigscience/mt0-xl": {"_name_or_path": "google/mt5-xl", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 5120, "d_kv": 64, "d_model": 2048, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.23.1", "use_cache": true, "vocab_size": 250112}, "luffycodes/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple": {"_name_or_path": "/data/ss164/spock_econ/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-oig-oasst1-256-6_9b": {"_name_or_path": "EleutherAI/pythia-6.9b", "architectures": ["GPTNeoXForCausalLM"], "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "fabiochiu/t5-base-medium-title-generation": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 32128}, "OpenAssistant/falcon-40b-sft-mix-1226": {"_name_or_path": "/home/ubuntu/Open-Assistant/output_dir/oaast-stage2-top2/checkpoint-1226/", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_RW.RWConfig", "AutoModel": "modelling_RW.RWModel", "AutoModelForCausalLM": "modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 65040}, "Writer/palmyra-base": {"activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.01, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 4096, "n_head": 32, "n_inner": 16384, "n_layer": 24, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "bfloat16", "transformers_version": "4.24.0", "use_cache": true, "vocab_size": 50257}, "TheBloke/llama-2-70b-Guanaco-QLoRA-GGML": {"model_type": "llama"}, "Rostlab/prot_t5_base_mt_uniref50": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "use_cache": true, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 3, "min_length": 3, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "predict protein ms : "}}, "vocab_size": 256}, "Lajonbot/Llama-2-13b-hf-instruct-pl-lora_unload": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "WizardLM/WizardLM-7B-V1.0": {"_name_or_path": "/vc_data_blob/caxu/Llama-X/model_weights/checkpoint-2400", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32001}, "pankajmathur/orca_mini_7b": {"_name_or_path": "openlm-research/open_llama_7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000}, "yhyhy3/open_llama_7b_v2_med_instruct": {"_name_or_path": "merged", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": false, "vocab_size": 32000}, "NousResearch/CodeLlama-7b-hf": {"architectures": ["LlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32016}, "OpenBuddy/openbuddy-llama2-13b-v11.1-bf16": {"_name_or_path": "openbuddy-llama2-13b-v11.1-bf16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 37632}, "hf-internal-testing/tiny-random-GPT2ForQuestionAnswering": {"activation_function": "gelu", "architectures": ["GPT2ForQuestionAnswering"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.29.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "explosion-testing/llama2-fewer-kv-heads": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 256, "initializer_range": 0.02, "intermediate_size": 512, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 4, "num_hidden_layers": 5, "num_key_value_heads": 2, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 1024}, "hetpandya/t5-base-tapaco": {"_name_or_path": "t5-base", "architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "transformers_version": "4.8.0", "use_cache": true, "vocab_size": 32128}, "PygmalionAI/pygmalion-2-7b": {"_name_or_path": "PygmalionAI/pygmalion-2-7b", "architectures": ["LlamaForCausalLM"], "badwordsids": "[[29961], [14352], [24630], [29962], [11759], [15974], [5519], [25473], [18899], [25901], [7110], [9341], [13531], [518], [9310], [2636], [3366], [21069], [11970], [23098], [16733], [21298], [18173], [10846], [3816], [28513], [15625], [23192], [28166], [10062], [1385], [11724], [3108], [15555], [10834], [10370], [14330], [1822], [12436], [5262], [17094], [10725], [17077], [11424], [4197], [24406], [13359], [17531], [24566], [23076], [4514], [13192], [19942], [16261], [7072], [6024], [1402], [1839], [2033], [13970], [850], [5913], [28895], [5387], [8308], [24927], [5691], [12940], [19997], [18959], [11287], [16862], [4638], [22322], [29861], [21251], [14704], [17548], [12452], [17288], [23160], [24960], [8219], [18024], [5539], [7464], [27865], [29588], [20068], [19660], [27706], [22896], [24264], [12258], [2314], [4400], [5586], [12622], [6796], [7226], [21939], [18456], [14178], [21540], [21945], [14664], [16215], [10338], [17361], [7503], [13769], [26073], [9601], [26909], [7961], [8999], [20840], [16272], [21545], [3199], [10514], [5159], [22689], [6525], [20526], [27077], [18017]]", "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 32000}, "mrm8488/t5-base-finetuned-imdb-sentiment": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "explosion-testing/falcon-test": {"_name_or_path": "tiiuae/falcon-7b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["FalconForCausalLM"], "attention_dropout": 0.0, "auto_map": {"AutoConfig": "configuration_falcon.FalconConfig", "AutoModel": "modeling_falcon.FalconModel", "AutoModelForSequenceClassification": "modeling_falcon.FalconForSequenceClassification", "AutoModelForTokenClassification": "modeling_falcon.FalconForTokenClassification", "AutoModelForQuestionAnswering": "modeling_falcon.FalconForQuestionAnswering", "AutoModelForCausalLM": "modeling_falcon.FalconForCausalLM"}, "bias": false, "bos_token_id": 11, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_size": 32, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "falcon", "new_decoder_architecture": false, "multi_query": true, "num_attention_heads": 4, "num_hidden_layers": 5, "parallel_attn": true, "torch_dtype": "float32", "transformers_version": "4.28.1", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "ehartford/WizardLM-33B-V1.0-Uncensored": {"_name_or_path": "WizardLM-33B-V1.0-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "TheBloke/StableBeluga-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ": {"_name_or_path": "WizardLM-30B-Uncensored-SuperCOT-30b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "jinaai/jina-embedding-s-en-v1": {"_name_or_path": "tmp/", "architectures": ["T5EncoderModel"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "vocab_size": 32128}, "FelixChao/vicuna-33b-coder": {"_name_or_path": "lmsys/vicuna-33b-v1.3", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "TheBloke/llama-30b-supercot-SuperHOT-8K-fp16": {"_name_or_path": "/workspace/superhot_process/llama-30b-supercot/source", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 8192, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoModel": "modelling_llama.LlamaModel", "AutoModelForCausalLM": "modelling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modelling_llama.LlamaForSequenceClassification"}}, "quantumaikr/llama-2-70b-fb16-orca-chat-10k": {"_name_or_path": "llama-2-70b-fb16-orca-chat-10k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "TheBloke/airoboros-l2-13B-gpt4-1.4.1-GPTQ": {"_name_or_path": "airoboros-l2-13b-gpt4-1.4.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-31m": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 256, "initializer_range": 0.02, "intermediate_size": 1024, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 8, "num_hidden_layers": 6, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "hf-internal-testing/tiny-random-GPT2ForTokenClassification": {"activation_function": "gelu", "architectures": ["GPT2ForTokenClassification"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5, "n_positions": 512, "pad_token_id": 1023, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.25.0.dev0", "type_vocab_size": 16, "use_cache": true, "vocab_size": 1024}, "jondurbin/airoboros-l2-70b-gpt4-1.4.1": {"_name_or_path": "airoboros-l2-70b-gpt4-1.4.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "kimsan0622/gpt2-medium": {"_name_or_path": "gpt_4400", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 64007}, "TheBloke/EverythingLM-13B-16K-GPTQ": {"_name_or_path": "/home/pikal/Coding/everythinglm/LLongMA-2-13b-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "Linly-AI/Chinese-LLaMA-2-13B-hf": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 40076}, "BlackSamorez/rudialogpt3_medium_based_on_gpt2_2ch": {"_name_or_path": "BlackSamorez/rudialogpt3_medium_based_on_gpt2_2ch", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 2048, "n_special": 0, "output_past": true, "predict_special_tokens": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.17.0", "use_cache": true, "vocab_size": 50257}, "EleutherAI/pythia-2.8b-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.24.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50304}, "TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ": {"_name_or_path": "llama2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "google/byt5-xl": {"_name_or_path": "/home/patrick/t5/byt5-xl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 6720, "d_kv": 64, "d_model": 2560, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 32, "num_layers": 36, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "transformers_version": "4.7.0.dev0", "use_cache": true, "vocab_size": 384}, "TheBloke/wizard-vicuna-13B-HF": {"_name_or_path": "/workspace/models/junelee_wizard-vicuna-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "TehVenom/Pygmalion-Vicuna-1.1-7b": {"_name_or_path": "/home/mixer/Venom-Shop/Pygmalion-7b-sharded/", "architectures": ["LlamaForCausalLM"], "badwordsids": [[0]], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32000}, "openaccess-ai-collective/wizard-mega-13b": {"_name_or_path": "huggyllama/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-7b-gpt4-m2.0": {"_name_or_path": "airoboros-l2-7b-gpt4-m2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "openchat/openchat_v3.2_super": {"_name_or_path": "imone/LLaMA2_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0", "use_cache": false, "vocab_size": 32002}, "openaccess-ai-collective/manticore-13b-chat-pyg": {"_name_or_path": "/home/sgugger/tmp/llama/llama-13b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "Neko-Institute-of-Science/pygmalion-7b": {"_name_or_path": "pygmalion-7b", "architectures": ["LlamaForCausalLM"], "badwordsids": [[0]], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "unicamp-dl/ptt5-small-portuguese-vocab": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "vocab_size": 32128}, "hf-internal-testing/tiny-random-T5ForQuestionAnswering": {"architectures": ["T5ForQuestionAnswering"], "bos_token_id": 0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32100}, "microsoft/CodeGPT-small-java-adaptedGPT2": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "vocab_size": 50260}, "unicamp-dl/ptt5-base-portuguese-vocab": {"architectures": ["T5WithLMHeadModel"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "vocab_size": 32128}, "Fredithefish/ScarletPajama-3B-HF": {"_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "hf-internal-testing/tiny-random-T5ForSequenceClassification": {"architectures": ["T5ForSequenceClassification"], "bos_token_id": 0, "classifier_dropout": 0.0, "d_ff": 37, "d_kv": 8, "d_model": 32, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 0.002, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 5, "num_heads": 4, "num_layers": 5, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 8, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32100}, "TheBloke/Nous-Hermes-Llama-2-7B-GPTQ": {"_name_or_path": "output/hermes-llama2-4k/checkpoint-2259", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.1, "desc_act": false, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "calvindoingstuff/DialoGPT-medium-luffy": {"_name_or_path": "output-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"conversational": {"max_length": 1000}}, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 50257}, "lvkaokao/llama2-7b-hf-chat-lora-v2": {"_name_or_path": "/models/llama-v2-latest-20230719/models_hf/Llama-2-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "skt/ko-gpt-trinity-1.2B-v0.5": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 8, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1920, "n_head": 16, "n_inner": 7680, "n_layer": 24, "n_positions": 1024, "pad_token_id": 8, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": null, "transformers_version": "4.7.0", "use_cache": true, "vocab_size": 51200}, "saibo/llama-1B": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 2, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32000}, "vonjack/Qwen-LLaMAfied-HFTok-7B-Chat": {"_name_or_path": "/notebooks/qwen", "architectures": ["LlamaForCausalLM"], "bos_token_id": 151643, "eos_token_id": 151643, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 6144, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 151936}, "TheBloke/CodeLlama-34B-Python-fp16": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "rope_theta": 1000000, "pad_token_id": 0}, "GAIR/rst-all-11b": {"_name_or_path": "pytorch_model.bin", "architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 128, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.16.2", "use_cache": true, "vocab_size": 32128}, "GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct": {"_name_or_path": "bigcode/starcoder", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "validate_runner_input": true, "vocab_size": 49153}, "jondurbin/airoboros-13b": {"_name_or_path": "/workspace/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "aisquared/dlite-v2-1_5b": {"_name_or_path": "gpt2-xl", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48, "n_positions": 1024, "output_past": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float16", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "aiassociates/t5-small-grammar-correction-german": {"_name_or_path": "t5-small", "architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 6, "num_heads": 8, "num_layers": 6, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.18.0", "use_cache": true, "vocab_size": 32128}, "asi/gpt-fr-cased-small": {"activation_function": "gelu_new", "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 2, "pad_token_id": 1, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "vocab_size": 50000}, "csebuetnlp/mT5_m2o_chinese_simplified_crossSum": {"_name_or_path": "google/mt5-base", "architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 250040, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "length_penalty": 0.6, "max_length": 84, "model_type": "mt5", "num_beams": 4, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"langid_map": {"amharic": [35, "\u2581<extra_id_64>"], "arabic": [4, "\u2581<extra_id_95>"], "azerbaijani": [7, "\u2581<extra_id_92>"], "bengali": [42, "\u2581<extra_id_57>"], "burmese": [33, "\u2581<extra_id_66>"], "chinese_simplified": [40, "\u2581<extra_id_59>"], "chinese_traditional": [44, "\u2581<extra_id_55>"], "english": [30, "\u2581<extra_id_69>"], "french": [10, "\u2581<extra_id_89>"], "gujarati": [27, "\u2581<extra_id_72>"], "hausa": [43, "\u2581<extra_id_56>"], "hindi": [21, "\u2581<extra_id_78>"], "igbo": [9, "\u2581<extra_id_90>"], "indonesian": [1, "\u2581<extra_id_98>"], "japanese": [37, "\u2581<extra_id_62>"], "kirundi": [0, "\u2581<extra_id_99>"], "korean": [29, "\u2581<extra_id_70>"], "kyrgyz": [5, "\u2581<extra_id_94>"], "marathi": [13, "\u2581<extra_id_86>"], "nepali": [20, "\u2581<extra_id_79>"], "oromo": [41, "\u2581<extra_id_58>"], "pashto": [34, "\u2581<extra_id_65>"], "persian": [23, "\u2581<extra_id_76>"], "pidgin": [14, "\u2581<extra_id_85>"], "portuguese": [39, "\u2581<extra_id_60>"], "punjabi": [17, "\u2581<extra_id_82>"], "russian": [36, "\u2581<extra_id_63>"], "scottish_gaelic": [24, "\u2581<extra_id_75>"], "serbian_cyrillic": [28, "\u2581<extra_id_71>"], "serbian_latin": [11, "\u2581<extra_id_88>"], "sinhala": [31, "\u2581<extra_id_68>"], "somali": [19, "\u2581<extra_id_80>"], "spanish": [3, "\u2581<extra_id_96>"], "swahili": [18, "\u2581<extra_id_81>"], "tamil": [32, "\u2581<extra_id_67>"], "telugu": [22, "\u2581<extra_id_77>"], "thai": [6, "\u2581<extra_id_93>"], "tigrinya": [16, "\u2581<extra_id_83>"], "turkish": [15, "\u2581<extra_id_84>"], "ukrainian": [2, "\u2581<extra_id_97>"], "urdu": [38, "\u2581<extra_id_61>"], "uzbek": [8, "\u2581<extra_id_91>"], "vietnamese": [12, "\u2581<extra_id_87>"], "welsh": [26, "\u2581<extra_id_73>"], "yoruba": [25, "\u2581<extra_id_74>"]}}, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "transformers_version": "4.10.0.dev0", "use_cache": true, "vocab_size": 250112}, "openthaigpt/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf": {"_name_or_path": "/project/lt200059-openth/.cache/huggingface/hub/models--daryl149--llama-2-7b-chat-hf/snapshots/d8654a4f69178a0c9260cf730241ebac2e72b923", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-13b-2.1": {"_name_or_path": "llama-2-13b-hf-dequantized", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "sentence-transformers/sentence-t5-xl": {"_name_or_path": "models/sentence-t5-3b", "architectures": ["T5EncoderModel"], "d_ff": 16384, "d_kv": 128, "d_model": 1024, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 32, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float16", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 32128}, "OpenBuddy/openbuddy-openllama-3b-v10-bf16": {"_name_or_path": "openbuddy-openllama-3b-v10-bf16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 37120}, "TheBloke/guanaco-33B-GPTQ": {"_name_or_path": "/gscratch/zlab/llama/30B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "h2oai/h2ogpt-oasst1-512-20b": {"_name_or_path": "h2oai/h2ogpt-oasst1-512-20b", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0, "bos_token_id": 0, "custom_pipeline": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "Open-Orca/OpenOrca-Preview1-13B": {"_name_or_path": "/home/sgugger/tmp/llama/llama-13b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "WizardLM/WizardLM-13B-V1.0": {"_name_or_path": "/workspaceblobstore/qins/llamax/trained_models/13B_sg_alpaca_prefix_mt_350k_2048_e3_2e_5/checkpoint-2732", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.0.dev0", "use_cache": false, "vocab_size": 32001}, "garage-bAInd/Camel-Platypus2-13B": {"_name_or_path": "augtoma/qCammel-13", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "wxjiao/alpaca-7b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.27.0.dev0", "use_cache": true, "vocab_size": 32001}, "TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ": {"_name_or_path": "/workspace/superhot_process/wizard-vicuna-13b-uncensored/source", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 8192, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoModel": "modelling_llama.LlamaModel", "AutoModelForCausalLM": "modelling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modelling_llama.LlamaForSequenceClassification"}, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "FelixChao/vicuna-7B-chemical": {"_name_or_path": "CleverShovel/vicuna-7b-v1.3-sharded-bf16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "Arc53/docsgpt-14b": {"_name_or_path": "meta-llama/Llama-2-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "OpenAssistant/llama2-13b-megacode2-oasst": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 32005, "eos_token_id": 32006, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32007}, "TheBloke/Lemur-70B-Chat-v1-GPTQ": {"_name_or_path": "/home/ubuntu/sft-models/lemur-70b-10k/sysdatav1/epoch_1_hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32005, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "EleutherAI/pythia-6.9b-v0": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 16384, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.22.2", "use_cache": true, "vocab_size": 50432}, "grimpep/L2-MythoMax22b-instruct-Falseblock": {"_name_or_path": "mythomix-L2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "Austism/chronos-hermes-13b-v2-GPTQ": {"_name_or_path": "models\\chronos-hermes-13b-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32032}, "UBC-NLP/AraT5v2-base-1024": {"d_ff": 2048, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.30.0", "use_cache": true, "vocab_size": 110208}, "fireballoon/baichuan-vicuna-chinese-7b": {"_name_or_path": "models/baichuan-llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "max_sequence_length": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 64000}, "abeja/gpt2-large-japanese": {"_name_or_path": "checkpoint-huggingface/gpt2-large-japanese", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 1, "embd_pdrop": 0.1, "eos_token_id": 2, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 32000}, "TheBloke/Airoboros-L2-70B-2.1-GPTQ": {"_name_or_path": "llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000, "pretraining_tp": 1, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.1, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "vicgalle/gpt2-alpaca-gpt4": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "flax-community/gpt2-small-indonesian": {"_name_or_path": ".", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.0, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.9.0.dev0", "use_cache": true, "vocab_size": 50257}, "TheBloke/Nous-Hermes-13B-SuperHOT-8K-fp16": {"_name_or_path": "/workspace/process/nous-nermes-13b/source", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32001, "auto_map": {"AutoModel": "modelling_llama.LlamaModel", "AutoModelForCausalLM": "modelling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modelling_llama.LlamaForSequenceClassification"}}, "imone/LLaMA2_13B_with_EOT_token": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": false, "vocab_size": 32002}, "Corianas/111m": {"_name_or_path": "cerebras/Cerebras-GPT-111M", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "bfloat16", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50257}, "The-Face-Of-Goonery/Huginn-v3-13b": {"_name_or_path": "huginn/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "ehartford/Samantha-1.11-7b": {"_name_or_path": "NousResearch/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "heegyu/WizardVicuna-3B-0719": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "acrastt/Griffin-3B": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "totally-not-an-llm/EverythingLM-13b-V2-16k": {"_name_or_path": "/home/pikal/Coding/everythinglm/LLongMA-2-13b-16k", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": {"factor": 4.0, "type": "linear"}, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "use_flash_attention": false, "vocab_size": 32000}, "ikala/bloom-zh-3b-chat": {"_name_or_path": "ckip-joint/bloom-3b-zh", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 2560, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 32, "n_inner": null, "n_layer": 30, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "unk_token_id": 0, "use_cache": true, "vocab_size": 250688}, "Gryphe/MythoLogic-13b": {"_name_or_path": "mythologic-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "AlekseyKorshuk/vicuna-7b": {"_name_or_path": "./", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32001}, "flax-community/gpt2-medium-persian": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 5, "embd_pdrop": 0.1, "eos_token_id": 5, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "min_length": 32, "max_length": 256, "top_k": 50, "top_p": 0.95}}, "transformers_version": "4.9.0.dev0", "use_cache": true, "vocab_size": 50000}, "ehartford/samantha-1.1-llama-33b": {"_name_or_path": "samantha-1.1-llama-33b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "garage-bAInd/Platypus2-70B": {"_name_or_path": "meta-llama/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "OpenLemur/lemur-70b-v1": {"_name_or_path": "checkpoint-30000_new", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32024}, "ausboss/llama-30b-supercot": {"_name_or_path": "huggyllama/llama-30b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-70b-gpt4-m2.0": {"_name_or_path": "airoboros-l2-70b-gpt4-m2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "lmqg/mt5-small-koquad-qg-ae": {"_name_or_path": "mt5-small-koquad-qg-ae", "add_prefix": true, "architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_kv": 64, "d_model": 512, "decoder_start_token_id": 0, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "model_type": "mt5", "num_decoder_layers": 8, "num_heads": 6, "num_layers": 8, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "T5Tokenizer", "torch_dtype": "float32", "transformers_version": "4.21.2", "use_cache": true, "vocab_size": 250101}, "TheBloke/OpenAssistant-SFT-7-Llama-30B-HF": {"_name_or_path": ".saved/llama-30b-sft-7/checkpoint-703/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.0.dev0", "use_cache": true, "vocab_size": 32016}, "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GPTQ": {"_name_or_path": "tiiuae/falcon-40b", "alibi": false, "apply_residual_connection_post_layernorm": false, "architectures": ["RWForCausalLM"], "attention_dropout": 0.0, "attention_probs_dropout_prob": 0.0, "auto_map": {"AutoConfig": "tiiuae/falcon-40b--configuration_RW.RWConfig", "AutoModel": "tiiuae/falcon-40b--modelling_RW.RWModel", "AutoModelForCausalLM": "tiiuae/falcon-40b--modelling_RW.RWForCausalLM", "AutoModelForQuestionAnswering": "tiiuae/falcon-40b--modelling_RW.RWForQuestionAnswering", "AutoModelForSequenceClassification": "tiiuae/falcon-40b--modelling_RW.RWForSequenceClassification", "AutoModelForTokenClassification": "tiiuae/falcon-40b--modelling_RW.RWForTokenClassification"}, "bias": false, "bos_token_id": 11, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 11, "hidden_dropout": 0.0, "hidden_dropout_prob": 0.0, "hidden_size": 8192, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "RefinedWeb", "n_head": 128, "n_head_kv": 8, "n_layer": 60, "parallel_attn": true, "torch_dtype": "float16", "transformers_version": "4.29.0", "use_cache": true, "vocab_size": 65024, "quantization_config": {"bits": 4, "group_size": -1, "damp_percent": 0.01, "desc_act": true, "sym": true, "true_sequential": true, "model_name_or_path": null, "model_file_base_name": "model", "quant_method": "gptq"}}, "GOAT-AI/GOAT-7B-Community": {"_name_or_path": "output/7b-v2-guaass_enh-customloss-512bs/checkpoint-420", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-gm-oasst1-en-1024-20b": {"_name_or_path": "EleutherAI/gpt-neox-20b", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0.0, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "beaugogh/pythia-1.4b-deduped-sharegpt": {"_name_or_path": "EleutherAI/pythia-1.4b-deduped", "architectures": ["GPTNeoXForCausalLM"], "attention_dropout": 0.0, "bos_token_id": 0, "classifier_dropout": 0.1, "end_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout": 0.0, "hidden_size": 2048, "initializer_range": 0.02, "intermediate_size": 8192, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "rope_scaling": null, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50280}, "amurshak/llama-2-7b-miniguanaco": {"_name_or_path": "NousResearch/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "psyche/kollama2-7b-v2": {"_name_or_path": "psyche/kollama2-7b-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "IlyaGusev/fred_t5_ru_turbo_alpaca": {"_name_or_path": "ai-forever/FRED-T5-1.7B", "architectures": ["T5ForConditionalGeneration"], "bos_token_id": 1, "d_ff": 4096, "d_kv": 64, "d_model": 1536, "decoder_start_token_id": 1, "dense_act_fn": "gelu_new", "dropout_rate": 0.1, "eos_token_id": 2, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": true, "layer_norm_epsilon": 1e-06, "max_length": 512, "model_type": "t5", "num_beams": 5, "num_decoder_layers": 24, "num_heads": 24, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 50365}, "potsawee/t5-large-generation-race-Distractor": {"_name_or_path": "t5-large", "architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_kv": 64, "d_model": 1024, "decoder_start_token_id": 0, "dense_act_fn": "relu", "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "initializer_factor": 1.0, "is_encoder_decoder": true, "is_gated_act": false, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 24, "num_heads": 16, "num_layers": 24, "output_past": true, "pad_token_id": 0, "relative_attention_max_distance": 128, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}}, "torch_dtype": "float32", "transformers_version": "4.26.1", "use_cache": true, "vocab_size": 32128}, "heegyu/WizardVicuna-Uncensored-3B-0719": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "TheBloke/openchat_v2_openorca_preview-GPTQ": {"_name_or_path": "/data/one/LLaMA_13B_with_EOT_token", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": true, "vocab_size": 32001, "quantization_config": {"bits": 4, "group_size": 128, "damp_percent": 0.01, "desc_act": false, "sym": true, "true_sequential": true, "model_file_base_name": "model", "quant_method": "gptq"}}, "CalderaAI/13B-Legerdemain-L2": {"_name_or_path": "13B-Legerdemain-l2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "SebastianSchramm/Cerebras-GPT-111M-instruction": {"_name_or_path": "SebastianSchramm/Cerebras-GPT-111M-instruction", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.33.0.dev0", "use_cache": true, "vocab_size": 50258}, "Mikael110/llama-2-7b-guanaco-fp16": {"_name_or_path": "llama2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Locutusque/gpt2-large-conversational": {"_name_or_path": "gpt2-large", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": true, "vocab_size": 50260}, "CalderaAI/13B-Ouroboros": {"_name_or_path": "13B-Ouroboros", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": false, "vocab_size": 32000}, "chaoyi-wu/MedLLaMA_13B": {"_name_or_path": "/home/cs/leijiayu/wuchaoyi/Finetune_LLAMA/LLAMA_Model/llama-13b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "YeungNLP/firefly-llama2-13b": {"_name_or_path": "checkpoint/firefly-llama2-13b-merge", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "garage-bAInd/GPlatty-30B": {"_name_or_path": "lilloukas/Platypus-30B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "pankajmathur/orca_mini_v2_13b": {"_name_or_path": "/workspace/models/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "pankajmathur/model_007_13b_v2": {"_name_or_path": "TheBloke/Llama-2-13B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "chargoddard/Chronorctypus-Limarobormes-13b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "timdettmers/guanaco-65b-merged": {"_name_or_path": "decapoda-research/llama-65b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": -1, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "digitous/13B-HyperMantis": {"_name_or_path": "13B-HyperMantis", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "ckiplab/gpt2-base-chinese": {"activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 101, "embd_pdrop": 0.1, "eos_token_id": 102, "gradient_checkpointing": false, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "resid_pdrop": 0.1, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "tokenizer_class": "BertTokenizerFast", "vocab_size": 21128}, "ehartford/dolphin-llama-13b": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "jphme/orca_mini_v2_ger_7b": {"_name_or_path": "jphme/orca_mini_v2_ger_7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": false, "vocab_size": 32000}, "malhajar/Platypus2-70B-instruct-4bit-gptq": {"_name_or_path": "garage-bAInd/Platypus2-70B-instruct", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0", "use_cache": false, "vocab_size": 32000}, "heegyu/WizardVicuna-open-llama-3b-v2": {"_name_or_path": "openlm-research/open_llama_3b_v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "pankajmathur/model_007": {"_name_or_path": "TheBloke/Llama-2-70B-fp16", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "vicgalle/gpt2-alpaca": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2": {"_name_or_path": "h2o-llmstudio/open_llama_7b_preview_300bt_fix", "architectures": ["LlamaForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 1, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 2, "hidden_act": "silu", "hidden_dropout_prob": 0.0, "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "stabilityai/stablecode-completion-alpha-3b": {"architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 16384, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": true, "vocab_size": 49152}, "aisquared/dlite-v2-355m": {"_name_or_path": "gpt2-medium", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 1024, "n_special": 0, "predict_special_tokens": true, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "google/byt5-xxl": {"_name_or_path": "/home/patrick/t5/byt5-xxl", "architectures": ["T5ForConditionalGeneration"], "d_ff": 12352, "d_kv": 64, "d_model": 4672, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "gated-gelu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "num_decoder_layers": 12, "num_heads": 64, "num_layers": 36, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "tie_word_embeddings": false, "tokenizer_class": "ByT5Tokenizer", "transformers_version": "4.7.0.dev0", "use_cache": true, "vocab_size": 384}, "ehartford/Samantha-1.11-CodeLlama-34b": {"_name_or_path": "/workspace/CodeLlama-34b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-gm-oasst1-multilang-1024-20b": {"_name_or_path": "EleutherAI/gpt-neox-20b", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu_fast", "hidden_dropout_prob": 0.0, "hidden_size": 6144, "initializer_range": 0.02, "intermediate_size": 24576, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 64, "num_hidden_layers": 44, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50432}, "TheBloke/koala-7B-HF": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "vocab_size": 32000}, "ehartford/WizardLM-30B-Uncensored": {"_name_or_path": "/workspace/WizardLM-30B-Uncensored", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32001}, "clibrain/Llama-2-ft-instruct-es": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_length": 4096, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "quantumaikr/llama-2-70b-fb16-guanaco-1k": {"_name_or_path": "upstage/Llama-2-70b-instruct-v2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "psyche/kogpt": {"_name_or_path": "psyche/kogpt", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 2, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 1536, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32002}, "wenge-research/yayi-7b": {"_name_or_path": "yayi-7b", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 4096, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 32, "n_inner": null, "n_layer": 30, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 4, "seq_length": 2048, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "bfloat16", "transformers_version": "4.28.1", "unk_token_id": 0, "use_cache": false, "vocab_size": 250684}, "Aspik101/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload": {"_name_or_path": "heegyu/WizardVicuna-Uncensored-3B-0719", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "rombodawg/LosslessMegaCoder-llama2-7b-mini": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 32006, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32007}, "TurkuNLP/gpt3-finnish-medium": {"apply_residual_connection_post_layernorm": false, "architectures": ["BloomModel"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 1024, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 16, "n_layer": 24, "pad_token_id": 3, "pretraining_tp": 1, "slow_but_exact": false, "transformers_version": "4.26.0.dev0", "use_cache": true, "vocab_size": 131072}, "pankajmathur/orca_mini_13b": {"_name_or_path": "openlm-research/open_llama_13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000}, "Mikael110/llama-2-13b-guanaco-fp16": {"_name_or_path": "/root/llama2-13b-hf/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "totally-not-an-llm/PuddleJumper-13b": {"_name_or_path": "/home/pikal/Coding/everythinglm/OpenOrca-Platypus2-13B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32002}, "jondurbin/airoboros-13b-gpt4": {"_name_or_path": "airoboros-13b-gpt4", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "max_seq_len": 4096, "max_sequence_length": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.2", "use_cache": false, "vocab_size": 32000}, "CobraMamba/mamba-gpt-3b": {"_name_or_path": "openlm-research/open_llama_3b", "architectures": ["LlamaForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_dropout_prob": 0.0, "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}, "zarakiquemparte/zarablend-l2-7b": {"_name_or_path": "zarablend-l2-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "Locutusque/gpt2-conversational-or-qa": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.27.3", "use_cache": true, "vocab_size": 50262}, "frank098/Wizard-Vicuna-13B-juniper": {"_name_or_path": "TheBloke/Wizard-Vicuna-13B-Uncensored-HF", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-gpt-3.5-turbo-100k-7b": {"_name_or_path": "/workspace/llama-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "use_cache": false, "vocab_size": 32000}, "jondurbin/airoboros-33b-gpt4-1.4": {"_name_or_path": "airoboros-33b-gpt4-1.4", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "jondurbin/airoboros-l2-70b-gpt4-2.0": {"_name_or_path": "airoboros-l2-70b-gpt4-2.0", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "MBZUAI/LaMini-Cerebras-1.3B": {"_name_or_path": "/home/awaheed/scratch/InstructTuning/output/Cerebras-GPT-1.3B", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.28.0.dev0", "use_cache": false, "vocab_size": 50258}, "h2oai/h2ogpt-research-oasst1-llama-65b": {"_name_or_path": "decapoda-research/llama-65b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": -1, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.1", "use_cache": true, "vocab_size": 32000}, "nkpz/llama2-22b-daydreamer-v3": {"_name_or_path": "./llama22b_twax", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4098, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "Aspik101/trurl-2-13b-pl-instruct_unload": {"_name_or_path": "Voicelab/trurl-2-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32001}, "OpenAssistant/pythia-12b-pre-v8-12.5k-steps": {"_name_or_path": ".saved/pythia-12b-super-pretrain2/", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0.dev0", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50288}, "breadlicker45/dough-instruct-base-001": {"_name_or_path": "breadlicker45/dough-base-001", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 768, "initializer_range": 0.02, "intermediate_size": 3072, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 12, "num_hidden_layers": 12, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 50402}, "OpenBuddy/openbuddy-llama-30b-v7.1-bf16": {"_name_or_path": "openbuddy-llama-30b-v7.1", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 37632}, "andreaskoepf/llama2-13b-megacode2_min100": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 32005, "eos_token_id": 32006, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32007}, "ehartford/Samantha-1.11-70b": {"_name_or_path": "/workspace/models/Llama-2-70b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "flax-community/t5-recipe-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_kv": 64, "d_model": 768, "decoder_start_token_id": 0, "dropout_rate": 0.1, "eos_token_id": 1, "feed_forward_proj": "relu", "gradient_checkpointing": false, "initializer_factor": 1.0, "is_encoder_decoder": true, "layer_norm_epsilon": 1e-06, "model_type": "t5", "n_positions": 512, "num_decoder_layers": 12, "num_heads": 12, "num_layers": 12, "output_past": true, "pad_token_id": 0, "relative_attention_num_buckets": 32, "task_specific_params": {"summarization": {"early_stopping": true, "length_penalty": 2.0, "max_length": 200, "min_length": 30, "no_repeat_ngram_size": 3, "num_beams": 4, "prefix": "summarize: "}, "translation_en_to_de": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to German: "}, "translation_en_to_fr": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to French: "}, "translation_en_to_ro": {"early_stopping": true, "max_length": 300, "num_beams": 4, "prefix": "translate English to Romanian: "}, "text2text-generation": {"early_stopping": true, "max_length": 512, "repetition_penalty": 1.2, "length_penalty": 1.2, "num_beams": 5, "prefix": "items: "}}, "transformers_version": "4.9.0.dev0", "use_cache": true, "vocab_size": 32128}, "BreadAi/PM_modelV2": {"_name_or_path": "cerebras/Cerebras-GPT-256M", "activation_function": "gelu", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "line_by_line": true, "model_type": "gpt2", "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14, "n_positions": 2048, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.26.1", "use_cache": true, "vocab_size": 50257}, "minlik/chinese-alpaca-33b-merged": {"architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "vocab_size": 49954}, "jordiclive/Llama-2-70b-oasst-1-200": {"_name_or_path": "/mnt/data/llama2/Llama-2-70b-hf-sp", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32016}, "Lajonbot/tableBeluga-7B-instruct-pl-lora_unload": {"_name_or_path": "stabilityai/StableBeluga-7B", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "sia-ai/llama-2-7b-1-percent-open-orca-1000-steps-v0": {"_name_or_path": "meta-llama/Llama-2-7b-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "h2oai/h2ogpt-gm-oasst1-en-1024-12b": {"_name_or_path": "EleutherAI/pythia-12b-deduped", "architectures": ["GPTNeoXForCausalLM"], "attention_probs_dropout_prob": 0.0, "bos_token_id": 0, "custom_pipelines": {"text-generation": {"impl": "h2oai_pipeline.H2OTextGenerationPipeline", "pt": "AutoModelForCausalLM"}}, "eos_token_id": 0, "hidden_act": "gelu", "hidden_dropout_prob": 0.0, "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 20480, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 40, "num_hidden_layers": 36, "rotary_emb_base": 10000, "rotary_pct": 0.25, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.1", "use_cache": true, "use_parallel_residual": true, "vocab_size": 50688}, "jondurbin/airoboros-33b-gpt4-1.2": {"_name_or_path": "airoboros-33b-gpt4-1.2", "architectures": ["LlamaForCausalLM"], "bos_token_id": 0, "eos_token_id": 1, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": -1, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "openchat/openchat_8192": {"_name_or_path": "/dev/shm/LLaMA_13B_with_EOT_token_8192_positions", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.1", "use_cache": true, "vocab_size": 32001}, "TaylorAI/Flash-Llama-3B": {"_name_or_path": "TaylorAI/Flash-Llama-3B", "architectures": ["LlamaForCausalLM"], "auto_map": {"AutoModelForCausalLM": "modeling_flash_llama.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3200, "initializer_range": 0.02, "intermediate_size": 8640, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 26, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "yeontaek/llama-2-13B-ensemble-v1": {"_name_or_path": "/fsx/yeontaek/ties-merge/result", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "Kirili4ik/ruDialoGpt3-medium-finetuned-telegram": {"_name_or_path": "Kirili4ik/ruDialoGpt3-medium-finetuned-telegram", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "gradient_checkpointing": false, "id2label": {"0": "LABEL_0"}, "initializer_range": 0.02, "label2id": {"LABEL_0": 0}, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 2048, "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "n_positions": 2048, "n_special": 0, "output_past": true, "predict_special_tokens": true, "resid_pdrop": 0.1, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float32", "transformers_version": "4.11.3", "use_cache": true, "vocab_size": 50257}, "WangZeJun/bloom-820m-chat": {"_name_or_path": "huggingface_models/bloom-820m-zh/", "apply_residual_connection_post_layernorm": false, "architectures": ["BloomForCausalLM"], "attention_dropout": 0.0, "attention_softmax_in_fp32": true, "bias_dropout_fusion": true, "bos_token_id": 1, "eos_token_id": 2, "hidden_dropout": 0.0, "hidden_size": 1536, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "masked_softmax_fusion": true, "model_type": "bloom", "n_head": 16, "n_inner": null, "n_layer": 24, "offset_alibi": 100, "pad_token_id": 3, "pretraining_tp": 1, "skip_bias_add": true, "skip_bias_add_qkv": false, "slow_but_exact": false, "torch_dtype": "float32", "transformers_version": "4.28.1", "unk_token_id": 0, "use_cache": true, "vocab_size": 46145}, "4bit/Llama-2-70b-chat-hf": {"_name_or_path": "meta-llama/Llama-2-70b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 28672, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "num_key_value_heads": 8, "pad_token_id": 0, "pretraining_tp": 8, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0.dev0", "use_cache": true, "vocab_size": 32000}, "chargoddard/llama2-22b": {"_name_or_path": "/root/llama2-22b/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4098, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": false, "vocab_size": 32000}, "augtoma/qCammel-13": {"_name_or_path": "/workspace/qCammel-13", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_length": 4096, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "NlpHUST/gpt2-vietnamese": {"_name_or_path": "./vi-gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "attn_pdrop": 0.0, "bos_token_id": 50256, "embd_pdrop": 0.0, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.0, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "transformers_version": "4.19.2", "use_cache": true, "vocab_size": 50257}, "Monero/Manticore-13b-Chat-Pyg-Guanaco": {"_name_or_path": "openaccess-ai-collective/manticore-13b-chat-pyg", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.28.0", "use_cache": true, "vocab_size": 32000}, "NousResearch/CodeLlama-34b-hf": {"architectures": ["LlamaForCausalLM"], "auto_map": {"AutoConfig": "configuration_llama.LlamaConfig", "AutoModel": "modeling_llama.LlamaModel", "AutoModelForCausalLM": "modeling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modeling_llama.LlamaForSequenceClassification"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 16384, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 48, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 1000000, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.32.0.dev0", "use_cache": true, "vocab_size": 32000}, "aisquared/dlite-v2-124m": {"_name_or_path": "gpt2", "activation_function": "gelu_new", "architectures": ["GPT2LMHeadModel"], "custom_pipelines": {"text-generation": {"impl": "instruct_pipeline.InstructionTextGenerationPipeline", "pt": "AutoModelForCausalLM", "tf": "TFAutoModelForCausalLM"}}, "attn_pdrop": 0.1, "bos_token_id": 50256, "embd_pdrop": 0.1, "eos_token_id": 50256, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "model_type": "gpt2", "n_ctx": 1024, "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12, "n_positions": 1024, "reorder_and_upcast_attn": false, "resid_pdrop": 0.1, "scale_attn_by_inverse_layer_idx": false, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "task_specific_params": {"text-generation": {"do_sample": true, "max_length": 50}}, "torch_dtype": "float32", "transformers_version": "4.25.1", "use_cache": false, "vocab_size": 50260}, "pankajmathur/orca_mini_v2_7b": {"_name_or_path": "huggyllama/llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.29.1", "use_cache": true, "vocab_size": 32000}, "The-Face-Of-Goonery/Huginn-22b-Prototype": {"_name_or_path": "huginn/", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": false, "vocab_size": 32000}, "DevaMalla/llama7b": {"_name_or_path": "huggyllama/llama-7b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-06, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float32", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "openaccess-ai-collective/manticore-13b": {"_name_or_path": "huggyllama/llama-13b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000}, "nkpz/llama2-22b-chat-wizard-uncensored": {"_name_or_path": null, "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 2048, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 40, "num_key_value_heads": 52, "pad_token_id": 0, "pretraining_tp": 2, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.2", "use_cache": true, "vocab_size": 32000}, "davzoku/cria-llama2-7b-v1.3": {"_name_or_path": "NousResearch/Llama-2-7b-chat-hf", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 4096, "initializer_range": 0.02, "intermediate_size": 11008, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 32, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "TaylorAI/Flash-Llama-13B": {"_name_or_path": "TaylorAI/Flash-Llama-13B", "architectures": ["LlamaForCausalLM"], "auto_map": {"AutoModelForCausalLM": "modeling_flash_llama.LlamaForCausalLM"}, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 5120, "initializer_range": 0.02, "intermediate_size": 13824, "max_position_embeddings": 4096, "model_type": "llama", "num_attention_heads": 40, "num_hidden_layers": 40, "num_key_value_heads": 40, "pad_token_id": 0, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.31.0", "use_cache": true, "vocab_size": 32000}, "Fredithefish/ReasonixPajama-3B-HF": {"_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1", "architectures": ["GPTNeoXForCausalLM"], "bos_token_id": 0, "classifier_dropout": 0.1, "eos_token_id": 0, "hidden_act": "gelu", "hidden_size": 2560, "initializer_range": 0.02, "intermediate_size": 10240, "layer_norm_eps": 1e-05, "max_position_embeddings": 2048, "model_type": "gpt_neox", "num_attention_heads": 32, "num_hidden_layers": 32, "rotary_emb_base": 10000, "rotary_pct": 1.0, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "use_parallel_residual": false, "vocab_size": 50432}, "TheBloke/Platypus-30B-SuperHOT-8K-fp16": {"_name_or_path": "/workspace/superhot_process/platypus-30b/source", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 6656, "initializer_range": 0.02, "intermediate_size": 17920, "max_position_embeddings": 8192, "model_type": "llama", "num_attention_heads": 52, "num_hidden_layers": 60, "pad_token_id": 0, "rms_norm_eps": 1e-06, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "vocab_size": 32000, "auto_map": {"AutoModel": "modelling_llama.LlamaModel", "AutoModelForCausalLM": "modelling_llama.LlamaForCausalLM", "AutoModelForSequenceClassification": "modelling_llama.LlamaForSequenceClassification"}}, "LoupGarou/WizardCoder-Guanaco-15B-V1.1": {"_name_or_path": "bigcode/starcoder", "activation_function": "gelu", "architectures": ["GPTBigCodeForCausalLM"], "attention_softmax_in_fp32": true, "attn_pdrop": 0.1, "bos_token_id": 0, "embd_pdrop": 0.1, "eos_token_id": 0, "inference_runner": 0, "initializer_range": 0.02, "layer_norm_epsilon": 1e-05, "max_batch_size": null, "max_sequence_length": null, "model_type": "gpt_bigcode", "multi_query": true, "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40, "n_positions": 8192, "pad_key_length": true, "pre_allocate_kv_cache": false, "resid_pdrop": 0.1, "scale_attention_softmax_in_fp32": true, "scale_attn_weights": true, "summary_activation": null, "summary_first_dropout": 0.1, "summary_proj_to_labels": true, "summary_type": "cls_index", "summary_use_proj": true, "torch_dtype": "float16", "transformers_version": "4.30.0.dev0", "use_cache": true, "validate_runner_input": true, "vocab_size": 49153}, "TheBloke/guanaco-65B-HF": {"_name_or_path": "/workspace/models/huggyllama_llama-65b", "architectures": ["LlamaForCausalLM"], "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 8192, "initializer_range": 0.02, "intermediate_size": 22016, "max_position_embeddings": 2048, "max_sequence_length": 2048, "model_type": "llama", "num_attention_heads": 64, "num_hidden_layers": 80, "pad_token_id": 0, "rms_norm_eps": 1e-05, "tie_word_embeddings": false, "torch_dtype": "float16", "transformers_version": "4.29.2", "use_cache": true, "vocab_size": 32000}}