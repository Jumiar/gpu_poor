{"NousResearch/Llama-2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "google/flan-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/flan-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PascalNotin/Tranception_Small": {"architectures": ["TranceptionLMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "bigscience/bloom-560m": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "distilgpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 6}, "hf-internal-testing/tiny-random-gpt2": {"intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "tiiuae/falcon-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "bigscience/bloomz-1b1": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "gpt2-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "mrm8488/t5-base-finetuned-common_gen": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmsys/fastchat-t5-3b-v1.0": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "gpt2-xl": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_layer": 48}, "meta-llama/Llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "hf-internal-testing/tiny-random-t5": {"d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "EleutherAI/pythia-6.9b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "databricks/dolly-v2-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "hf-internal-testing/tiny-random-GPTNeoXForCausalLM": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "meta-llama/Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "microsoft/DialoGPT-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "google/mt5-base": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "hf-internal-testing/tiny-random-BloomModel": {"architectures": ["BloomModel"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "google/flan-t5-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "lmsys/vicuna-7b-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "huggyllama/llama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mrm8488/t5-base-finetuned-summarize-news": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/flan-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "tiiuae/falcon-40b-instruct": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "ramsrigouthamg/t5_sentence_paraphraser": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "flexudy/t5-base-multi-sentence-doctor": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lewtun/tiny-random-mt5": {"architectures": ["MT5Model"], "d_ff": 1024, "d_model": 16, "num_heads": 4, "num_layers": 2}, "gpt2-large": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_layer": 36}, "valhalla/t5-base-e2e-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sshleifer/tiny-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2, "n_head": 2, "n_layer": 2}, "fxmarty/tiny-llama-fast-tokenizer": {"architectures": ["LlamaForCausalLM"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "decapoda-research/llama-7b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "stabilityai/StableBeluga2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "syzymon/long_llama_3b": {"architectures": ["LongLlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "NousResearch/Llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Llama-2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "tiiuae/falcon-7b-instruct": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "google/flan-t5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "meta-llama/Llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "petals-team/StableBeluga2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "meta-llama/Llama-2-70b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "EleutherAI/gpt-neox-20b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "hf-internal-testing/tiny-random-GPTBigCodeForCausalLM": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "nferruz/ProtGPT2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "philschmid/flan-t5-xxl-sharded-fp16": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "HuggingFaceM4/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "hf-internal-testing/tiny-random-BloomForCausalLM": {"architectures": ["BloomForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "hf-internal-testing/tiny-random-GPT2LMHeadModel": {"architectures": ["GPT2LMHeadModel"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "Vamsi/T5_Paraphrase_Paws": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmsys/vicuna-7b-v1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "meta-llama/Llama-2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ybelkada/tiny-random-T5ForConditionalGeneration-calibrated": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "prithivida/parrot_paraphraser_on_T5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hf-internal-testing/tiny-random-GPTBigCodeModel": {"architectures": ["GPTBigCodeModel"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "hkunlp/instructor-large": {"architectures": ["T5EncoderModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "lmsys/vicuna-7b-v1.5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "fabiochiu/t5-small-medium-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/Llama-2-7b-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "skt/kogpt2-base-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "google/t5-v1_1-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Maykeye/TinyLLama-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 64, "intermediate_size": 256, "num_attention_heads": 16, "num_hidden_layers": 8}, "TheBloke/Llama-2-13B-chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "tiiuae/falcon-40b": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "sonoisa/t5-base-japanese-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Rostlab/prot_t5_xl_uniref50": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "TheBloke/vicuna-7B-v1.3-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "daryl149/llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "stabilityai/StableBeluga-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "meta-llama/Llama-2-70b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/MythoMax-L2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "czurita/nsql-llama-2-7B-sharded-bf16-2GB": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "vennify/t5-base-grammar-correction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "czearing/story-to-title": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/byt5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3840, "d_model": 1536, "num_heads": 16, "num_layers": 36}, "HuggingFaceH4/starchat-beta": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "codellama/CodeLlama-34b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "openlm-research/open_llama_13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "optimum/t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "t5-3b": {"architectures": ["T5WithLMHeadModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "humarin/chatgpt_paraphraser_on_T5_base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Gustavosta/MagicPrompt-Stable-Diffusion": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "bigscience/bloomz-7b1": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "trl-internal-testing/tiny-random-GPTNeoXForCausalLM": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "NousResearch/Yarn-Llama-2-7b-64k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "khhuang/zerofec-qa2claim-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "khhuang/zerofec-daqa-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "declare-lab/flan-alpaca-gpt4-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "codellama/CodeLlama-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmsys/vicuna-13b-v1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Rostlab/prot_t5_xl_half_uniref50-enc": {"architectures": ["T5EncoderModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "google/mt5-small": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Salesforce/safety-flan-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "patrickvonplaten/t5-tiny-random": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 256, "d_model": 64, "num_heads": 2, "num_layers": 2}, "google/flan-ul2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 4096, "num_heads": 16, "num_layers": 32}, "EleutherAI/pythia-70m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "bigscience/mt0-large": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "stevhliu/my_awesome_billsum_model": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "EleutherAI/pythia-70m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "lmsys/vicuna-13b-v1.5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "PAIXAI/Astrid-1B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "Phind/Phind-CodeLlama-34B-Python-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "pszemraj/flan-t5-large-grammar-synthesis": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Voicelab/vlt5-base-keywords": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "togethercomputer/Llama-2-7B-32K-Instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "huggyllama/llama-65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "ai-forever/ruGPT-3.5-13B": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "Einmalumdiewelt/T5-Base_GNAD": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/t5-v1_1-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "Austism/chronos-hermes-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "upstage/SOLAR-0-70b-16bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "bigscience/bloom-7b1": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "nlpai-lab/kullm-polyglot-12.8b-v2": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "codellama/CodeLlama-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-GPT2Model": {"architectures": ["GPT2Model"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "Gryphe/MythoMax-L2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "openlm-research/open_llama_3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/Llama-2-70B-chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "trl-internal-testing/dummy-GPT2-correct-vocab": {"architectures": ["GPT2LMHeadModel"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "charsiu/g2p_multilingual_byT5_small_100": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "EleutherAI/pythia-160m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "ElnaggarLab/ankh-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 48}, "trl-internal-testing/tiny-random-GPT2LMHeadModel": {"architectures": ["GPT2LMHeadModel"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "openlm-research/open_llama_7b_v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "codellama/CodeLlama-7b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "WizardLM/WizardCoder-Python-34B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "pszemraj/grammar-synthesis-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "openlm-research/open_llama_3b_v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "IDEA-CCNL/Wenzhong-GPT2-110M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "microsoft/DialoGPT-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "trl-internal-testing/tiny-random-BloomForCausalLM": {"architectures": ["BloomForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "trl-internal-testing/tiny-random-T5ForConditionalGeneration": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "hf-internal-testing/tiny-random-onnx-mt5": {"architectures": ["MT5Model"], "d_ff": 1024, "d_model": 16, "num_heads": 4, "num_layers": 2}, "NousResearch/Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "trl-internal-testing/tiny-random-MT5ForConditionalGeneration": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 256, "d_model": 64, "num_heads": 4, "num_layers": 8}, "fxmarty/tiny-testing-gpt2-remote-code": {"architectures": ["GPT2CustomLMHeadModel"], "intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "castorini/monot5-base-msmarco-10k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "microsoft/DialoGPT-large": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_layer": 36}, "bigscience/bloomz-560m": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "Open-Orca/OpenOrca-Platypus2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "google/byt5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "bigscience/bloom-1b7": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "elinas/chronos-13b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "google/t5-efficient-tiny": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 256, "num_heads": 4, "num_layers": 4}, "bigscience/bloom-1b1": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "EleutherAI/polyglot-ko-1.3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "bigscience/bloom-3b": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "TinyPixel/Llama-2-7B-bf16-sharded": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "edumunozsala/llama-2-7b-int4-python-code-20k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yahma/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "beomi/KoAlpaca-Polyglot-12.8B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "stanfordnlp/backpack-gpt2": {"architectures": ["BackpackGPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "prithivida/grammar_error_correcter_v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lvkaokao/llama2-7b-hf-chat-lora-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "google/t5-v1_1-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/gpt4-alpaca-lora_mlp-65B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "google/mt5-large": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "EleutherAI/pythia-2.8b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "cyberagent/open-calm-7b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "lvwerra/gpt2-imdb": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "WizardLM/WizardLM-13B-V1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "KoboldAI/GPT-NeoX-20B-Erebus": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "togethercomputer/RedPajama-INCITE-Instruct-3B-v1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "aditi2222/automatic_title_generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "shibing624/chinese-alpaca-plus-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "optimum/gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "togethercomputer/LLaMA-2-7B-32K": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "amazon/FalconLite": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "EleutherAI/polyglot-ko-5.8b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "databricks/dolly-v2-7b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "snrspeaks/t5-one-line-summary": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmsys/vicuna-33b-v1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/OpenOrca-Platypus2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Llama-2-13B-Chat-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sdadas/mt5-base-translator-pl-en": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama-2-7b-chat-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bigcode/gpt_bigcode-santacoder": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "lmsys/vicuna-13b-v1.5-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigcode/santacoder": {"architectures": ["GPT2LMHeadCustomModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "togethercomputer/RedPajama-INCITE-Chat-3B-v1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "ai-forever/mGPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "openlm-research/open_llama_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "decapoda-research/llama-13b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/codellama-13b-oasst-sft-v10": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "rinna/bilingual-gpt-neox-4b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "KoboldAI/LLaMA2-13B-Holomax-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "MBZUAI/LaMini-T5-61M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "google/t5-v1_1-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "EleutherAI/pythia-1.4b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "stabilityai/StableBeluga-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "tiiuae/falcon-rw-1b": {"architectures": ["FalconForCausalLM"], "hidden_size": 2048, "num_attention_heads": 32, "num_hidden_layers": 24}, "ClueAI/ChatYuan-large-v2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "af1tang/personaGPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "google/t5-large-lm-adapt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "vilsonrodrigues/falcon-7b-instruct-sharded": {"architectures": ["FalconForCausalLM"], "hidden_size": 4544, "num_attention_heads": 71, "num_hidden_layers": 32}, "petals-team/falcon-rw-1b": {"architectures": ["FalconForCausalLM"], "hidden_size": 2048, "num_attention_heads": 32, "num_hidden_layers": 24}, "bigscience/T0_3B": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "TheTravellingEngineer/llama2-7b-hf-guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Salesforce/codet5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "EleutherAI/pythia-2.8b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "The-Face-Of-Goonery/Huginn-13b-v1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "FredZhang7/distilgpt2-stable-diffusion-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "WizardLM/WizardCoder-15B-V1.0": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "EleutherAI/pythia-410m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "huggyllama/llama-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ybelkada/falcon-7b-sharded-bf16": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "MingZhong/unieval-sum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "NousResearch/Nous-Hermes-Llama2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "csebuetnlp/mT5_multilingual_XLSum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hkunlp/instructor-xl": {"architectures": ["T5EncoderModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "h2oai/h2ogpt-4096-llama2-13b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "HuggingFaceH4/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "EleutherAI/polyglot-ko-12.8b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "databricks/dolly-v2-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "mrm8488/t5-base-finetuned-span-sentiment-extraction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "WizardLM/WizardLM-70B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "codellama/CodeLlama-34b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "Salesforce/codet5-base-multi-sum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "MBZUAI/LaMini-T5-738M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "codellama/CodeLlama-13b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-4096-llama2-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "bigscience/bloom": {"architectures": ["BloomForCausalLM"], "n_layer": 70, "num_attention_heads": 112}, "TigerResearch/tigerbot-13b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/airoboros-l2-70B-gpt4-1.4.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Austism/chronos-hermes-13b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "snrspeaks/KeyPhraseTransformer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Rocketknight1/tiny-random-falcon-7b": {"architectures": ["FalconForCausalLM"], "hidden_size": 1136, "num_attention_heads": 71, "num_hidden_layers": 2}, "hf-internal-testing/tiny-random-T5Model": {"architectures": ["T5Model"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "sambanovasystems/BLOOMChat-176B-v1": {"architectures": ["BloomForCausalLM"], "hidden_size": 14336, "n_head": 112, "n_layer": 70}, "huggyllama/llama-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "elyza/ELYZA-japanese-Llama-2-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lcw99/t5-base-korean-text-summary": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "it5/it5-base-news-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "uer/gpt2-chinese-cluecorpussmall": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "t5-11b": {"architectures": ["T5WithLMHeadModel"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "KoboldAI/LLaMA2-13B-Holomax": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Llama-2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bigscience/bloomz-3b": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "lmsys/vicuna-7b-v1.5-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sonoisa/t5-base-japanese": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "line-corporation/japanese-large-lm-3.6b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 3072, "intermediate_size": 12288, "num_attention_heads": 32, "num_hidden_layers": 30}, "TheBloke/Llama-2-7B-32K-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "EleutherAI/pythia-410m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "NousResearch/Llama-2-70b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/falcon-7b-instruct-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "eachadea/vicuna-13b-1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "beomi/llama-2-ko-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/falcon-40b-instruct-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "TheBloke/Llama-2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "garage-bAInd/Platypus2-70B-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "rajkumarrrk/gpt2-fine-tuned-on-imdb-positive-reviews": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "cerebras/Cerebras-GPT-13B": {"architectures": ["GPT2Model"], "n_embd": 5120, "n_head": 40, "n_inner": 20480, "n_layer": 40}, "rinna/japanese-gpt2-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24}, "bigscience/T0pp": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "Phind/Phind-CodeLlama-34B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "beomi/kykim-gpt3-kor-small_based_on_gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Pi3141/DialoGPT-medium-elon-3": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "EleutherAI/pythia-1b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "ai-forever/rugpt3large_based_on_gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "jondurbin/airoboros-l2-13b-gpt4-m2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "codellama/CodeLlama-13b-Python-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "AUTOMATIC/promptgen-lexart": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "Salesforce/codet5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "h2oai/h2ogpt-oig-oasst1-512-6_9b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "rinna/japanese-gpt-neox-3.6b-instruction-ppo": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "prithivida/informal_to_formal_styletransfer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "matsuo-lab/weblab-10b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4864, "intermediate_size": 19456, "num_attention_heads": 38, "num_hidden_layers": 36}, "succinctly/text2image-prompt-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Llama-2-7B-Chat-GGML": {}, "TheBloke/Llama-2-70B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "sentence-transformers/gtr-t5-large": {"architectures": ["T5EncoderModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "togethercomputer/RedPajama-INCITE-Base-3B-v1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "rinna/bilingual-gpt-neox-4b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "TheBloke/Llama-2-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pankajmathur/orca_mini_v3_70b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "OpenAssistant/llama2-13b-orca-8k-3319": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/StableBeluga-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "defog/sqlcoder": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "WizardLM/WizardCoder-Python-13B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "stabilityai/stablelm-tuned-alpha-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 16}, "cyberagent/open-calm-small": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/WizardLM-70B-V1.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "bigscience/bigscience-small-testing": {"architectures": ["BloomModel"], "hidden_size": 64, "n_head": 8, "n_inner": null, "n_layer": 2}, "cyberagent/open-calm-1b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "lamini/lamini_docs_finetuned": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "EnglishVoice/t5-base-uk-to-us-english": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "codellama/CodeLlama-7b-Python-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/WizardLM-13B-V1.2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-160m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "jphme/Llama-2-13b-chat-german": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Salesforce/codet5p-220m": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/mt5-xl": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "cerebras/Cerebras-GPT-111M": {"n_inner": 3072, "n_embd": 768, "n_head": 12, "n_layer": 10}, "google/t5-v1_1-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/vicuna-7B-v1.5-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "chavinlo/alpaca-native": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "kimnt93/kmv-7b-03": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NumbersStation/nsql-llama-2-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "cerebras/Cerebras-GPT-1.3B": {"n_inner": 8192, "n_embd": 2048, "n_head": 16, "n_layer": 24}, "trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "akreal/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "akreal/tiny-random-BloomForCausalLM": {"architectures": ["BloomForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "NousResearch/Nous-Hermes-llama-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ai-forever/rugpt3small_based_on_gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "VMware/open-llama-7b-v2-open-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "robertmyers/targon-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Nous-Hermes-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/WizardLM-7B-uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ramsrigouthamg/t5_paraphraser": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "tinkoff-ai/ruDialoGPT-medium": {"n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "OpenAssistant/falcon-7b-sft-mix-2000": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "bigcode/tiny_starcoder_py": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 20}, "rinna/japanese-gpt-1b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "TheBloke/orca_mini_v3_70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "UBC-NLP/turjuman": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "h2oai/h2ogpt-4096-llama2-70b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Phind/Phind-CodeLlama-34B-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "iarfmoose/t5-base-question-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama-2-7B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mrm8488/t5-base-finetuned-emotion": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hkunlp/instructor-base": {"architectures": ["T5EncoderModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "fxmarty/onnx-tiny-random-gpt2-without-merge": {"architectures": ["GPT2Model"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "fxmarty/onnx-tiny-random-gpt2-with-merge": {"architectures": ["GPT2Model"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "microsoft/GODEL-v1_1-large-seq2seq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "rinna/japanese-gpt-neox-3.6b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "cyberagent/open-calm-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "eachadea/vicuna-7b-1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "beomi/KoAlpaca-Polyglot-5.8B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "grammarly/coedit-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Platypus2-70B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "codellama/CodeLlama-34b-Python-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "noamwies/llama-test-gqa-with-better-transformer": {"architectures": ["LlamaForCausalLM"], "hidden_size": 128, "intermediate_size": 344, "num_attention_heads": 8, "num_hidden_layers": 2}, "bigscience/bloomz-7b1-mt": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "Salesforce/codet5p-770m": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "OpenAssistant/pythia-12b-sft-v8-7k-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "augtoma/qCammel-70-x": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "NousResearch/Llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "plguillou/t5-base-fr-sum-cnndm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "WeOpenML/PandaLM-7B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "VMware/open-llama-7b-open-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "pankajmathur/orca_mini_v3_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "google/t5-xl-lm-adapt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "LinkSoul/Chinese-Llama-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "line-corporation/japanese-large-lm-3.6b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 3072, "intermediate_size": 12288, "num_attention_heads": 32, "num_hidden_layers": 30}, "OpenAssistant/oasst-sft-1-pythia-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "ehartford/WizardLM-7B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "upstage/llama-30b-instruct-2048": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "cyberagent/open-calm-large": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1536, "intermediate_size": 6144, "num_attention_heads": 16, "num_hidden_layers": 24}, "Gryphe/MythoLogic-L2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "eenzeenee/t5-small-korean-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "google/t5-xxl-lm-adapt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "mywateriswet/ShuanBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "hf-internal-testing/tiny-random-bloom": {"architectures": ["BloomModel"], "hidden_size": 64, "n_head": 8, "n_inner": null, "n_layer": 2}, "TheBloke/Llama-2-13B-chat-GGML": {}, "decapoda-research/llama-30b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "lmsys/longchat-7b-v1.5-32k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ziqingyang/chinese-alpaca-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "nlpai-lab/kullm-polyglot-5.8b-v2": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "google/byt5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3968, "d_model": 1536, "num_heads": 12, "num_layers": 18}, "stabilityai/stablelm-tuned-alpha-7b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 48, "num_hidden_layers": 16}, "PygmalionAI/pygmalion-1.3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "stanford-crfm/BioMedLM": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2560, "n_head": 20, "n_inner": null, "n_layer": 32}, "PY007/TinyLlama-1.1B-step-50K-105b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 2048, "intermediate_size": 5632, "num_attention_heads": 32, "num_hidden_layers": 22}, "georgesung/llama2_7b_chat_uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bigscience/mt0-small": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/WizardCoder-15B-1.0-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "google/t5-base-lm-adapt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "OpenAssistant/falcon-40b-sft-top1-560": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "TheBloke/WizardLM-30B-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/WizardCoder-Python-34B-V1.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "garage-bAInd/Camel-Platypus2-70B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "DeepFloyd/t5-v1_1-xxl": {"architectures": ["T5EncoderModel"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "EleutherAI/pythia-1b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "TheBloke/CodeLlama-7B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "kfkas/Llama-2-ko-7b-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "valhalla/t5-small-qa-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "FlagAlpha/Llama2-Chinese-13b-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Open-Orca/OpenOrcaxOpenChat-Preview2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "trl-internal-testing/tiny-random-LlamaForCausalLM": {"architectures": ["LlamaForCausalLM"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "abhishek/llama-2-7b-hf-small-shards": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "togethercomputer/RedPajama-INCITE-7B-Base": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "Salesforce/codegen25-7b-multi": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "fabiochiu/t5-base-tag-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "MBZUAI/LaMini-Flan-T5-248M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "bigscience/bloomz-1b7": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "valhalla/t5-base-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Wi/gptp": {"architectures": ["GPTPModel"], "n_embd": 128, "n_head": 4, "n_inner": null, "n_layer": 4}, "medalpaca/medalpaca-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yentinglin/Taiwan-LLaMa-v1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "rinna/japanese-gpt-neox-small": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "TheBloke/llama2_7b_chat_uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "EleutherAI/pythia-1.4b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "daryl149/llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "flax-community/gpt-2-spanish": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "KoboldAI/LLAMA2-13B-Holodeck-1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-13b-gpt4-1.4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mrm8488/t5-base-finetuned-question-generation-ap": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "OpenBuddy/openbuddy-llama2-13b-v8.1-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-6.9b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "tscholak/3vnuv1vf": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "OpenAssistant/llama2-70b-oasst-sft-v10": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/vicuna-13B-v1.5-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/falcon-7b-sft-top1-696": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "sentence-transformers/sentence-t5-large": {"architectures": ["T5EncoderModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Nous-Hermes-Llama2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mesolitica/finetune-translation-t5-super-super-tiny-standard-bahasa-cased": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 512, "d_model": 128, "num_heads": 6, "num_layers": 2}, "Henk717/spring-dragon": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "openchat/openchat_v3.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "WizardLM/WizardMath-70B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "potsawee/t5-large-generation-squad-QuestionAnswer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Phind-CodeLlama-34B-v2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "pankajmathur/orca_mini_3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "fffrrt/ruGPT-3.5-13B-GPTQ": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "kykim/gpt3-kor-small_based_on_gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "PAIXAI/Astrid-1B-CPU": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "ElnaggarLab/ankh-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3840, "d_model": 1536, "num_heads": 16, "num_layers": 48}, "togethercomputer/RedPajama-INCITE-7B-Chat": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "ramsrigouthamg/t5_squad_v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "KETI-AIR/ke-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sentence-transformers/gtr-t5-base": {"architectures": ["T5EncoderModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ramsrigouthamg/t5-large-paraphraser-diverse-high-quality": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "rinna/japanese-gpt2-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "rinna/bilingual-gpt-neox-4b-instruction-ppo": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "ramsrigouthamg/t5_boolean_questions": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "philschmid/flan-t5-base-samsum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/t5-small-lm-adapt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "matsuo-lab/weblab-10b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4864, "intermediate_size": 19456, "num_attention_heads": 38, "num_hidden_layers": 36}, "stabilityai/stablecode-completion-alpha-3b-4k": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "IDEA-CCNL/Ziya-LLaMA-7B-Reward": {"architectures": ["LlamaRewardModel"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ichitaka/falcon-40b-instruct-8bit": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "TheBloke/WizardCoder-Python-13B-V1.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "togethercomputer/Pythia-Chat-Base-7B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/wizardLM-7B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "AUTOMATIC/promptgen-majinai-unsafe": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "pinkmanlove/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmsys/longchat-13b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "togethercomputer/RedPajama-INCITE-7B-Instruct": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmsys/vicuna-13b-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Salesforce/codet5-large": {"architectures": ["T5WithLMHeadModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "FredZhang7/anime-anything-promptgen-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "Salesforce/xgen-7b-8k-inst": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jojo0217/step3_mk7": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-14m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 128, "intermediate_size": 512, "num_attention_heads": 4, "num_hidden_layers": 6}, "cerebras/Cerebras-GPT-590M": {"n_inner": 6144, "n_embd": 1536, "n_head": 12, "n_layer": 18}, "dbmdz/german-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "KoboldAI/GPT-NeoX-20B-Skein": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "beomi/polyglot-ko-12.8b-safetensors": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "sentence-transformers/sentence-t5-base": {"architectures": ["T5EncoderModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "decapoda-research/llama-65b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "mesolitica/finetune-translation-t5-small-standard-bahasa-cased": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "marcsun13/bloom-1b7_with_lm_head": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "MBZUAI/LaMini-Flan-T5-783M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "medalpaca/medalpaca-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "JulesBelveze/t5-small-headline-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "Michau/t5-base-en-generate-headline": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Falcon-180B-Chat-GPTQ": {"architectures": ["FalconForCausalLM"], "hidden_size": 14848, "num_attention_heads": 232, "num_hidden_layers": 80}, "Salesforce/xgen-7b-8k-base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ai-forever/ruT5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "KRAFTON/KORani-v3-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigscience/mt0-xxl-mt": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "garage-bAInd/Stable-Platypus2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Wizard-Vicuna-13B-Uncensored-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-oasst1-512-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "Parth/result": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "declare-lab/flan-alpaca-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "sdadas/mt5-base-translator-en-pl": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ziqingyang/chinese-llama-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NousResearch/Nous-Hermes-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pragmatic-programs/listener-suffix-idx-300k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "jinaai/jina-embedding-l-en-v1": {"architectures": ["T5EncoderModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "stabilityai/stablelm-base-alpha-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 16}, "razent/SciFive-base-Pubmed_PMC": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "uer/gpt2-chinese-poem": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "openchat/openchat_v3.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IDEA-CCNL/Ziya-LLaMA-13B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Sao10K/Mythical-Destroyer-V2-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "juierror/text-to-sql-with-table-schema": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "MingZhong/unieval-fact": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/vicuna-13B-v1.5-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "cerebras/Cerebras-GPT-256M": {"n_inner": 4352, "n_embd": 1088, "n_head": 17, "n_layer": 14}, "declare-lab/flan-alpaca-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ehartford/WizardLM-1.0-Uncensored-Llama2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "aubmindlab/aragpt2-base": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "valhalla/t5-small-e2e-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "elinas/llama-7b-hf-transformers-4.29": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmsys/vicuna-13b-delta-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "garage-bAInd/Platypus2-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "PKU-Alignment/beaver-7b-v1.0-cost": {"architectures": ["LlamaModelForScore"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "allenai/unifiedqa-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "JackFram/llama-160m": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "daryl149/llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "akreal/tiny-random-t5": {"d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "cyberagent/open-calm-medium": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "The-Face-Of-Goonery/Huginn-13b-FP16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "facebook/tart-full-flan-t5-xl": {"architectures": ["EncT5ForSequenceClassification"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "csebuetnlp/banglat5_banglaparaphrase": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "FlagAlpha/Llama2-Chinese-7b-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jerryjalapeno/Llama-2-1b-0-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "NousResearch/Redmond-Puffin-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigscience/bloomz": {"architectures": ["BloomForCausalLM"], "n_layer": 70, "num_attention_heads": 112}, "allenai/unifiedqa-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "WizardLM/WizardMath-7B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "pragmatic-programs/speaker-prefix-idx-300k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "TheBloke/CodeLlama-13B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Upstage-Llama-2-70B-instruct-v2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "pinkmanlove/llama-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "VietAI/envit5-translation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "cerebras/Cerebras-GPT-2.7B": {"n_inner": 10240, "n_embd": 2560, "n_head": 32, "n_layer": 32}, "Open-Orca/LlongOrca-7B-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "hf-internal-testing/tiny-random-T5ForConditionalGeneration": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "juierror/flan-t5-text2sql-with-schema-v2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "BeIR/query-gen-msmarco-t5-base-v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "conceptofmind/LLongMA-2-13b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/Yarn-Llama-2-13b-128k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "wangrongsheng/MiniGPT-4-LLaMA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-GPT2ForSequenceClassification": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "zenham/wail_m_e4_16h_2k": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "h2oai/h2ogpt-4096-llama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ai-forever/FRED-T5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "FreedomIntelligence/phoenix-inst-chat-7b": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "castorini/monot5-base-msmarco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "minlik/chinese-alpaca-plus-7b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "joaogante/tiny-random-gpt2-with-generation-config": {"intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "neulab/gpt2-finetuned-wikitext103": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "jarradh/llama2_70b_chat_uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TigerResearch/tigerbot-13b-base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "rinna/japanese-gpt-neox-3.6b-instruction-sft-v2": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "bofenghuang/vigogne-2-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/stable-vicuna-13B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "aiplanet/effi-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-33b-gpt4-m2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/orca_mini_v3_13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "HuggingFaceH4/starchat-alpha": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "WizardLM/WizardMath-13B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "upstage/Llama-2-70b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "anushehchaudry/llama-2-tiny-random": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8, "intermediate_size": 32, "num_attention_heads": 2, "num_hidden_layers": 1}, "fangloveskari/ORCA_LLaMA_70B_QLoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "HyperbeeAI/Tulpar-7b-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Llama-2-70B-Chat-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "csebuetnlp/mT5_m2m_crossSum_enhanced": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Genz-70b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "cerebras/Cerebras-GPT-6.7B": {"n_embd": 4096, "n_layer": 32, "n_head": 32, "n_inner": 16384}, "ziqingyang/chinese-alpaca-2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "google/t5-small-ssm-nq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "EleutherAI/polyglot-ko-3.8b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 3072, "intermediate_size": 12288, "num_attention_heads": 24, "num_hidden_layers": 32}, "kashif/stack-llama-2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "line-corporation/japanese-large-lm-1.7b": {"n_embd": 2304, "n_layer": 24, "n_head": 24, "n_inner": 9216, "architectures": ["GPT2LMHeadModel"]}, "microsoft/codereviewer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/guanaco-7B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "circulus/Llama-2-7b-orca-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "FlagAlpha/Atom-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Tap-M/Luna-AI-Llama2-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "K024/mt5-zh-ja-en-trimmed": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "deep-learning-analytics/automatic-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "luodian/llama-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "stabilityai/stablelm-base-alpha-7b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 48, "num_hidden_layers": 16}, "OpenLemur/lemur-70b-chat-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "rahular/varta-t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "rinna/japanese-gpt-neox-3.6b-instruction-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "garage-bAInd/Platypus-30B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "WizardLM/WizardCoder-Python-7B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "chavinlo/gpt4-x-alpaca": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sentence-transformers/gtr-t5-xl": {"architectures": ["T5EncoderModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "wangrongsheng/MiniGPT-4-LLaMA-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "EleutherAI/pythia-12b-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "unicamp-dl/translation-pt-en-t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "bigscience/mt0-base": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Pirr/pythia-13b-deduped-green_devil": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "trl-internal-testing/tiny-random-GPT2Model": {"architectures": ["GPT2Model"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "MBZUAI/LaMini-GPT-1.5B": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "Universal-NER/UniNER-7B-all": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/koala-13B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Rostlab/prot_t5_xl_bfd": {"architectures": ["T5WithLMHeadModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "Voicelab/trurl-2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "explosion-testing/llama2-kv-sharing": {"architectures": ["LlamaForCausalLM"], "hidden_size": 256, "intermediate_size": 512, "num_attention_heads": 4, "num_hidden_layers": 5}, "inpars/monot5-3b-inpars-v2-nq-promptagator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "upstage/llama-65b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "microsoft/CodeGPT-small-py": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "VietAI/vit5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/CodeUp-Llama-2-13B-Chat-HF-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/CodeLlama-34B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "elyza/ELYZA-japanese-Llama-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-7B-Python-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "FlagAlpha/Llama2-Chinese-13b-Chat-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/CodeLlama-13B-Python-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Enoch/llama-65b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "chargoddard/platypus-2-22b-relora": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "togethercomputer/GPT-NeoXT-Chat-Base-20B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "porkorbeef/Llama-2-13b-sf": {"architectures": ["LlamaModel"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/Wizard-Vicuna-13B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "doas/test5": {"architectures": ["LlamaModel"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "klosax/open_llama_3b_350bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "Writer/camel-5b-hf": {"architectures": ["GPT2LMHeadModel"], "n_embd": 4096, "n_head": 32, "n_inner": 16384, "n_layer": 24}, "Filosofas/DialoGPT-medium-PALPATINE2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "nomic-ai/gpt4all-falcon": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "reciprocate/llama2-7b-gsm8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pankajmathur/orca_mini_v3_13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "projecte-aina/aguila-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/WizardLM-13B-V1.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "MBZUAI/LaMini-GPT-124M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "google/mt5-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "MaRiOrOsSi/t5-base-finetuned-question-answering": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "satvikag/chatbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "LMFlow/Robin-7b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "daryl149/llama-2-70b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "acrastt/Puma-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/orca_mini_v3_7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "taeminlee/kogpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "NousResearch/Llama-2-70b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "rinna/japanese-gpt2-xsmall": {"architectures": ["GPT2LMHeadModel"], "n_embd": 512, "n_head": 8, "n_inner": 2304, "n_layer": 6}, "ziqingyang/chinese-llama-2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-t5-v1.1": {"d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "pankajmathur/Lima_Unchained_70b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "chargoddard/llama2-22b-blocktriangular": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "TheBloke/CodeLlama-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "BeIR/query-gen-msmarco-t5-large-v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/CodeLlama-13B-Instruct-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "acrastt/Marx-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "PygmalionAI/pygmalion-2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "shibing624/chinese-alpaca-plus-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "syzymon/long_llama_3b_instruct": {"architectures": ["LongLlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "bofenghuang/vigogne-2-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Gustavosta/MagicPrompt-Dalle": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "muchad/idt5-qa-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/vicuna-13b-v1.3.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TigerResearch/tigerbot-13b-base-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/WizardLM-13B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "clibrain/Llama-2-7b-ft-instruct-es": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "google/t5_xxl_true_nli_mixture": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "unikei/t5-base-split-and-rephrase": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "microsoft/Promptist": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "stas/mt5-tiny-random": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 256, "d_model": 64, "num_heads": 4, "num_layers": 8}, "AIDC-ai-business/Luban-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "microsoft/GODEL-v1_1-base-seq2seq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "CalderaAI/30B-Lazarus": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "acrastt/Marx-3B-V2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "h2oai/h2ogpt-4096-llama2-70b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "ajibawa-2023/scarlett-33b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-l2-70b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "rubentito/vt5-base-spdocvqa": {"architectures": ["HF_VT5"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "aisquared/dlite-v2-774m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "elyza/ELYZA-japanese-Llama-2-7b-fast": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/llama-2-70b-fb16-korean": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/CodeLlama-34B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "microsoft/DialogRPT-updown": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "TheBloke/CodeLlama-34B-Instruct-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "garage-bAInd/Platypus2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "trl-internal-testing/tiny-BloomForCausalLM-correct-vocab": {"architectures": ["BloomForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "TheBloke/Llama-2-7B-GGML": {}, "TheBloke/Wizard-Vicuna-7B-Uncensored-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "wenge-research/yayi-7b-llama2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "coffeeee/nsfw-story-generator2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "jondurbin/airoboros-33b-gpt4-2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "totally-not-an-llm/EverythingLM-13b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "datificate/gpt2-small-spanish": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "mrm8488/t5-base-finetuned-wikiSQL": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "bofenghuang/vigogne-2-13b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/stablelm-7b-sft-v7-epoch-3": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 48, "num_hidden_layers": 16}, "bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "flozi00/codellama-34b-german-assistant-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "WizardLM/WizardCoder-1B-V1.0": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "upstage/llama-30b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "ehartford/dolphin-llama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Open-Orca/LlongOrca-13B-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/Nous-Hermes-Llama2-70b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "ml6team/mt5-small-german-query-generation": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "bigscience/mt0-xxl": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "EleutherAI/pythia-2.8b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/wizardLM-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "conceptofmind/LLongMA-2-7b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmsys/vicuna-7b-delta-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bofenghuang/vigogne-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "csebuetnlp/banglat5_nmt_en_bn": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "trl-internal-testing/tiny-random-T5Model": {"architectures": ["T5Model"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "OpenBuddy/openbuddy-llama2-70b-v10.1-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/wizard-vicuna-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "JosephusCheung/Guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openchat/opencoderplus": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "jacobmorrison/tk-instruct-large-lora-experiments": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "PygmalionAI/metharme-1.3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "TheBloke/orca_mini_13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-70m-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "project-baize/baize-v2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "line-corporation/japanese-large-lm-1.7b-instruction-sft": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2304, "n_head": 24, "n_inner": 9216, "n_layer": 24}, "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/llama-2-70b-Guanaco-QLoRA-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "MBZUAI/LaMini-Flan-T5-77M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "csebuetnlp/banglat5_nmt_bn_en": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Trelis/Llama-2-7b-chat-hf-function-calling-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ehartford/Wizard-Vicuna-7B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "llSourcell/medllama2_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "WizardLM/WizardLM-13B-V1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Gryphe/MythoMix-L2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/StableBeluga2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "VietAI/vit5-large-vietnews-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "adasnew/t5-small-xsum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "Intel/t5-small-xsum-int8-dynamic": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "daspartho/prompt-extend": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "EleutherAI/pythia-160m-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "Ar4ikov/gpt2-650k-stable-diffusion-prompt-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ehartford/WizardLM-Uncensored-Falcon-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "CobraMamba/mamba-gpt-3b-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/llama2_70b_chat_uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "ai-forever/FRED-T5-1.7B": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "MBZUAI/LaMini-Cerebras-590M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 12, "n_inner": 6144, "n_layer": 18}, "mrm8488/llama-2-coder-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "guardrail/llama-2-7b-guanaco-instruct-sharded": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "rinna/bilingual-gpt-neox-4b-8k": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2816, "intermediate_size": 11264, "num_attention_heads": 22, "num_hidden_layers": 36}, "mrm8488/falcoder-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "circulus/Llama-2-13b-orca-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "allenai/tk-instruct-3b-def": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "pierreguillou/gpt2-small-portuguese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "junelee/wizard-vicuna-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "castorini/monot5-3b-msmarco-10k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "TheBloke/Llama-2-70B-Chat-GGML": {}, "TheBloke/CodeLlama-7B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-13B-ensemble-v5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ybelkada/flan-t5-xl-sharded-bf16": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "WizardLM/WizardCoder-3B-V1.0": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2816, "n_head": 22, "n_inner": 11264, "n_layer": 36}, "Langboat/mengzi-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "MBZUAI/LaMini-GPT-774M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "ToddGoldfarb/Cadet-Tiny": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TigerResearch/tigerbot-7b-base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "UrukHan/t5-russian-spell": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "LinkSoul/Chinese-Llama-2-7b-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Vicuna-13B-CoT-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-1.4b-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "MayaPH/GodziLLa2-70B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/wizardLM-13B-1.0-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Gryphe/MythoBoros-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "abacusai/Giraffe-v2-13b-32k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-l2-13b-gpt4-1.4.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "razent/SciFive-base-Pubmed": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TehVenom/Pygmalion-13b-Merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "garage-bAInd/SuperPlatty-30B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-l2-7b-gpt4-2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Rostlab/ProstT5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "TheBloke/guanaco-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "JackFram/llama-68m": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 2}, "MBZUAI/LaMini-Cerebras-111M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10}, "ehartford/Wizard-Vicuna-30B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "stockmark/gpt-neox-japanese-1.4b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "TheBloke/MythoMax-L2-13B-GGML": {}, "MBZUAI/LaMini-Cerebras-256M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14}, "jondurbin/airoboros-l2-13b-gpt4-2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "The-Face-Of-Goonery/Chronos-Beluga-v2-13bfp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lmqg/t5-base-squad-qag": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Voicelab/trurl-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ehartford/Samantha-1.11-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "clibrain/Llama-2-13b-ft-instruct-es": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "deepse/CodeUp-Llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mrm8488/t5-base-finetuned-sarcasm-twitter": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ToolBench/ToolLLaMA-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "marella/gpt-2-ggml": {}, "Henk717/airochronos-33B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "stanford-crfm/alias-gpt2-small-x21": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "WizardLM/WizardLM-30B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "timdettmers/guanaco-33b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "xkianteb/alg_ppo_separate_lr_1e-6_n_epochs_10_v_epochs_10_kl_target_1.0_clip_range_0.2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/wizard-mega-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigscience/mt0-xl": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "luffycodes/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-oig-oasst1-256-6_9b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "fabiochiu/t5-base-medium-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "OpenAssistant/falcon-40b-sft-mix-1226": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "Writer/palmyra-base": {"architectures": ["GPT2LMHeadModel"], "n_embd": 4096, "n_head": 32, "n_inner": 16384, "n_layer": 24}, "TheBloke/llama-2-70b-Guanaco-QLoRA-GGML": {}, "Rostlab/prot_t5_base_mt_uniref50": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Lajonbot/Llama-2-13b-hf-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "WizardLM/WizardLM-7B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "pankajmathur/orca_mini_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yhyhy3/open_llama_7b_v2_med_instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NousResearch/CodeLlama-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "OpenBuddy/openbuddy-llama2-13b-v11.1-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-GPT2ForQuestionAnswering": {"architectures": ["GPT2ForQuestionAnswering"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "explosion-testing/llama2-fewer-kv-heads": {"architectures": ["LlamaForCausalLM"], "hidden_size": 256, "intermediate_size": 512, "num_attention_heads": 4, "num_hidden_layers": 5}, "hetpandya/t5-base-tapaco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PygmalionAI/pygmalion-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mrm8488/t5-base-finetuned-imdb-sentiment": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "explosion-testing/falcon-test": {"architectures": ["FalconForCausalLM"], "hidden_size": 32, "num_attention_heads": 4, "num_hidden_layers": 5}, "ehartford/WizardLM-33B-V1.0-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/StableBeluga-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jinaai/jina-embedding-s-en-v1": {"architectures": ["T5EncoderModel"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "FelixChao/vicuna-33b-coder": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/llama-30b-supercot-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "quantumaikr/llama-2-70b-fb16-orca-chat-10k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/airoboros-l2-13B-gpt4-1.4.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "EleutherAI/pythia-31m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 256, "intermediate_size": 1024, "num_attention_heads": 8, "num_hidden_layers": 6}, "hf-internal-testing/tiny-random-GPT2ForTokenClassification": {"architectures": ["GPT2ForTokenClassification"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "jondurbin/airoboros-l2-70b-gpt4-1.4.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "kimsan0622/gpt2-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/EverythingLM-13B-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Linly-AI/Chinese-LLaMA-2-13B-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BlackSamorez/rudialogpt3_medium_based_on_gpt2_2ch": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "EleutherAI/pythia-2.8b-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "google/byt5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 6720, "d_model": 2560, "num_heads": 32, "num_layers": 36}, "TheBloke/wizard-vicuna-13B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TehVenom/Pygmalion-Vicuna-1.1-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openaccess-ai-collective/wizard-mega-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-l2-7b-gpt4-m2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openchat/openchat_v3.2_super": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "openaccess-ai-collective/manticore-13b-chat-pyg": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Neko-Institute-of-Science/pygmalion-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "unicamp-dl/ptt5-small-portuguese-vocab": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "hf-internal-testing/tiny-random-T5ForQuestionAnswering": {"architectures": ["T5ForQuestionAnswering"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "microsoft/CodeGPT-small-java-adaptedGPT2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "unicamp-dl/ptt5-base-portuguese-vocab": {"architectures": ["T5WithLMHeadModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Fredithefish/ScarletPajama-3B-HF": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "hf-internal-testing/tiny-random-T5ForSequenceClassification": {"architectures": ["T5ForSequenceClassification"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "TheBloke/Nous-Hermes-Llama-2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "calvindoingstuff/DialoGPT-medium-luffy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "lvkaokao/llama2-7b-hf-chat-lora-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "skt/ko-gpt-trinity-1.2B-v0.5": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1920, "n_head": 16, "n_inner": 7680, "n_layer": 24}, "saibo/llama-1B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 2}, "vonjack/Qwen-LLaMAfied-HFTok-7B-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-34B-Python-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "GAIR/rst-all-11b": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "jondurbin/airoboros-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "aisquared/dlite-v2-1_5b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "aiassociates/t5-small-grammar-correction-german": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "asi/gpt-fr-cased-small": {"n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "csebuetnlp/mT5_m2o_chinese_simplified_crossSum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "openthaigpt/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-l2-13b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sentence-transformers/sentence-t5-xl": {"architectures": ["T5EncoderModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "OpenBuddy/openbuddy-openllama-3b-v10-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/guanaco-33B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "h2oai/h2ogpt-oasst1-512-20b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "Open-Orca/OpenOrca-Preview1-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "WizardLM/WizardLM-13B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "garage-bAInd/Camel-Platypus2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "wxjiao/alpaca-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "FelixChao/vicuna-7B-chemical": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Arc53/docsgpt-14b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/llama2-13b-megacode2-oasst": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Lemur-70B-Chat-v1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "EleutherAI/pythia-6.9b-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "grimpep/L2-MythoMax22b-instruct-Falseblock": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "Austism/chronos-hermes-13b-v2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "UBC-NLP/AraT5v2-base-1024": {"d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "fireballoon/baichuan-vicuna-chinese-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "abeja/gpt2-large-japanese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "TheBloke/Airoboros-L2-70B-2.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "vicgalle/gpt2-alpaca-gpt4": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "flax-community/gpt2-small-indonesian": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Nous-Hermes-13B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "imone/LLaMA2_13B_with_EOT_token": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Corianas/111m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10}, "The-Face-Of-Goonery/Huginn-v3-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/Samantha-1.11-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "heegyu/WizardVicuna-3B-0719": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "acrastt/Griffin-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "totally-not-an-llm/EverythingLM-13b-V2-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ikala/bloom-zh-3b-chat": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "Gryphe/MythoLogic-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "AlekseyKorshuk/vicuna-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "flax-community/gpt2-medium-persian": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "ehartford/samantha-1.1-llama-33b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "garage-bAInd/Platypus2-70B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "OpenLemur/lemur-70b-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "ausboss/llama-30b-supercot": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-l2-70b-gpt4-m2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "lmqg/mt5-small-koquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/OpenAssistant-SFT-7-Llama-30B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "GOAT-AI/GOAT-7B-Community": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-gm-oasst1-en-1024-20b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "beaugogh/pythia-1.4b-deduped-sharegpt": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "amurshak/llama-2-7b-miniguanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "psyche/kollama2-7b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "IlyaGusev/fred_t5_ru_turbo_alpaca": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "potsawee/t5-large-generation-race-Distractor": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "heegyu/WizardVicuna-Uncensored-3B-0719": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/openchat_v2_openorca_preview-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CalderaAI/13B-Legerdemain-L2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "SebastianSchramm/Cerebras-GPT-111M-instruction": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10}, "Mikael110/llama-2-7b-guanaco-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Locutusque/gpt2-large-conversational": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "CalderaAI/13B-Ouroboros": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "chaoyi-wu/MedLLaMA_13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "YeungNLP/firefly-llama2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "garage-bAInd/GPlatty-30B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "pankajmathur/orca_mini_v2_13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pankajmathur/model_007_13b_v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "chargoddard/Chronorctypus-Limarobormes-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "timdettmers/guanaco-65b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "digitous/13B-HyperMantis": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ckiplab/gpt2-base-chinese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ehartford/dolphin-llama-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jphme/orca_mini_v2_ger_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "malhajar/Platypus2-70B-instruct-4bit-gptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "heegyu/WizardVicuna-open-llama-3b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "pankajmathur/model_007": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "vicgalle/gpt2-alpaca": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "stabilityai/stablecode-completion-alpha-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "aisquared/dlite-v2-355m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "google/byt5-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 12352, "d_model": 4672, "num_heads": 64, "num_layers": 36}, "ehartford/Samantha-1.11-CodeLlama-34b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "h2oai/h2ogpt-gm-oasst1-multilang-1024-20b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "TheBloke/koala-7B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ehartford/WizardLM-30B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "clibrain/Llama-2-ft-instruct-es": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/llama-2-70b-fb16-guanaco-1k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "psyche/kogpt": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 12, "n_inner": null, "n_layer": 12}, "wenge-research/yayi-7b": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "Aspik101/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "rombodawg/LosslessMegaCoder-llama2-7b-mini": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TurkuNLP/gpt3-finnish-medium": {"architectures": ["BloomModel"], "hidden_size": 1024, "n_head": 16, "n_layer": 24}, "pankajmathur/orca_mini_13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Mikael110/llama-2-13b-guanaco-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "totally-not-an-llm/PuddleJumper-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-13b-gpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CobraMamba/mamba-gpt-3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "zarakiquemparte/zarablend-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Locutusque/gpt2-conversational-or-qa": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "frank098/Wizard-Vicuna-13B-juniper": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-gpt-3.5-turbo-100k-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-33b-gpt4-1.4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-l2-70b-gpt4-2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "MBZUAI/LaMini-Cerebras-1.3B": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "h2oai/h2ogpt-research-oasst1-llama-65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "nkpz/llama2-22b-daydreamer-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "Aspik101/trurl-2-13b-pl-instruct_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/pythia-12b-pre-v8-12.5k-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "breadlicker45/dough-instruct-base-001": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "OpenBuddy/openbuddy-llama-30b-v7.1-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "andreaskoepf/llama2-13b-megacode2_min100": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/Samantha-1.11-70b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "flax-community/t5-recipe-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "BreadAi/PM_modelV2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14}, "minlik/chinese-alpaca-33b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jordiclive/Llama-2-70b-oasst-1-200": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Lajonbot/tableBeluga-7B-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sia-ai/llama-2-7b-1-percent-open-orca-1000-steps-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-gm-oasst1-en-1024-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "jondurbin/airoboros-33b-gpt4-1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "openchat/openchat_8192": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TaylorAI/Flash-Llama-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "yeontaek/llama-2-13B-ensemble-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Kirili4ik/ruDialoGpt3-medium-finetuned-telegram": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "WangZeJun/bloom-820m-chat": {"architectures": ["BloomForCausalLM"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "4bit/Llama-2-70b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "chargoddard/llama2-22b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "augtoma/qCammel-13": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NlpHUST/gpt2-vietnamese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Monero/Manticore-13b-Chat-Pyg-Guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/CodeLlama-34b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "aisquared/dlite-v2-124m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "pankajmathur/orca_mini_v2_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "The-Face-Of-Goonery/Huginn-22b-Prototype": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "DevaMalla/llama7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openaccess-ai-collective/manticore-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "nkpz/llama2-22b-chat-wizard-uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "davzoku/cria-llama2-7b-v1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TaylorAI/Flash-Llama-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Fredithefish/ReasonixPajama-3B-HF": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Platypus-30B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "LoupGarou/WizardCoder-Guanaco-15B-V1.1": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "TheBloke/guanaco-65B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "hakurei/lotus-12B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "bofenghuang/vigogne-33b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "grimpep/llama2-22B-GPLATTY": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "concedo/Pythia-70M-ChatSalad": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "rombodawg/LosslessMegaCoder-llama2-13b-mini": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TaylorAI/Flash-Llama-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenBuddy/openbuddy-llama-65b-v8-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "ajibawa-2023/scarlett-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/medalpaca-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "elinas/chronos-33b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "OpenBuddy/openbuddy-atom-13b-v9-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenAssistant/pythia-12b-sft-v8-rlhf-2k-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "TheTravellingEngineer/llama2-7b-chat-hf-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Ejafa/vicuna_7B_vanilla_1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yulan-team/YuLan-Chat-2-13b-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "huashiyiqike/testmodel": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 10}, "TheBloke/WizardLM-30B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "notstoic/PygmalionCoT-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Vicuna-33B-1-3-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "FelixChao/vicuna-7B-physics": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/tulu-30B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-65b-gpt4-1.4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "uukuguy/speechless-llama2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "digitous/13B-Chimera": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-7b-gpt4-1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "grimpep/llama2-28B-Airo03": {"architectures": ["LlamaForCausalLM"], "hidden_size": 7296, "intermediate_size": 22016, "num_attention_heads": 57, "num_hidden_layers": 40}, "ehartford/CodeLlama-34b-Instruct-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "YeungNLP/firefly-ziya-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheTravellingEngineer/bloom-560m-RLHF-v2": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheTravellingEngineer/llama2-7b-chat-hf-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "uukuguy/speechless-hermes-coig-lite-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BreadAi/gpt-Youtube": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "Aspik101/llama-30b-instruct-2048-PL-lora": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "beaugogh/Llama2-13b-sharegpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gaodrew/gaodrew-gorgonzola-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OpenBuddy/openbuddy-llama2-13b-v11-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/guanaco-13B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/CodeLlama-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BreadAi/MusePy-1-2": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "jondurbin/airoboros-33b-gpt4-1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "YeungNLP/firefly-bloom-7b1": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "grimpep/llama2-22b-wizard_vicuna": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "Fredithefish/Guanaco-3B-Uncensored": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "digitous/Alpacino13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mncai/SGPT-1.3B-insurance-epoch10": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "TheTravellingEngineer/llama2-7b-chat-hf-dpo": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/Platypus2xOpenOrca-13B-LoRa": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/gpt4-alpaca-lora-30b-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "bhenrym14/airophin-13b-pntk-16k-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Kimiko-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "porkorbeef/Llama-2-13b-12_153950": {"architectures": ["LlamaModel"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "PSanni/Deer-3b": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "IGeniusDev/llama13B-quant8-testv1-openorca-customdataset": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Neko-Institute-of-Science/metharme-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "alibidaran/medical_transcription_generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Panchovix/airoboros-33b-gpt4-1.2-SuperHOT-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "digitous/Alpacino30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "lgaalves/gpt2-dolly": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TaylorAI/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "zarakiquemparte/zarafusionex-1.1-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "OpenAssistant/pythia-12b-sft-v8-2.5k-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "TheBloke/airoboros-13B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/robin-33B-v2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Aspik101/trurl-2-7b-pl-instruct_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "llama-anon/petra-13b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFLai/gpt2-turkish-uncased": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "health360/Healix-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/Mythalion-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "pe-nlp/llama-2-13b-vicuna-wizard": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/Platypus2-13B-QLoRa": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "acrastt/OmegLLaMA-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "jslin09/bloom-560m-finetuned-fraud": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "YeungNLP/firefly-bloom-2b6-v2": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "xzuyn/LLaMa-1-MedicWizard-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Azure99/blossom-v2-3b": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "TheBloke/Airoboros-L2-13B-2.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "MetaIX/GPT4-X-Alpasta-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "PocketDoc/Dans-PersonalityEngine-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "vicgalle/alpaca-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Corianas/590m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 12, "n_inner": 6144, "n_layer": 18}, "OpenBuddy/openbuddy-openllama-13b-v7-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gywy/llama2-13b-chinese-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Corianas/Quokka_590m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 12, "n_inner": 6144, "n_layer": 18}, "aisquared/dlite-v1-355m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "aisquared/dlite-v1-774m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "project-baize/baize-v2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Project-Baize-v2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "FabbriSimo01/GPT_Large_Quantized": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "ajibawa-2023/carl-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Azure99/blossom-v1-3b": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "Aspik101/30B-Lazarus-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "yeontaek/Platypus2xOpenOrca-13B-IA3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Enno-Ai/ennodata-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "FabbriSimo01/Cerebras_1.3b_Quantized": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "migtissera/Synthia-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "pe-nlp/llama-2-13b-platypus-vicuna-wizard": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/Platypus2xOpenOrca-13B-IA3-ensemble": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Corianas/1.3b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "Rachneet/gpt2-xl-alpaca": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "Aeala/VicUnlocked-alpaca-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/VicUnlocked-30B-LoRA-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "bavest/fin-llama-33b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "openchat/openchat_v2_w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "FabbriSimo01/Bloom_1b_Quantized": {"architectures": ["BloomForCausalLM"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "Aspik101/tulu-7b-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheTravellingEngineer/llama2-7b-chat-hf-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-70b-IA3-guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Kunhao/pile-7b-250b-tokens": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-13b-QLoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/llama-2-13b-Beluga-QLoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/Platypus2xOpenOrca-13B-IA3-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ausboss/llama7b-wizardlm-unfiltered": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/h2ogpt-oasst1-512-30B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "bofenghuang/vigogne-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NYTK/PULI-GPTrio": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "LLMs/WizardLM-30B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "openaccess-ai-collective/minotaur-13b-fixed": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheTravellingEngineer/bloom-1b1-RLHF-v2": {"architectures": ["BloomForCausalLM"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "yeontaek/Platypus2xOpenOrca-13B-IA3-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BreadAi/DiscordPy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14}, "TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "gaodrew/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "yeontaek/Platypus2xOpenOrca-13B-IA3-v2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/Platypus2xOpenOrca-13B-LoRa-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/oasst-pythia-12b-6000-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "LoupGarou/WizardCoder-Guanaco-15B-V1.0": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "KnutJaegersberg/gpt-2-xl-EvolInstruct": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "Lajonbot/WizardLM-13B-V1.2-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/Platypus2-13B-IA3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "zarakiquemparte/zaraxe-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "BreadAi/gpt-YA-1-1_70M": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "dvruette/oasst-pythia-12b-reference": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "titan087/OpenLlama13B-Guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "w601sxs/b1ade-1b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "Andron00e/YetAnother_Open-Llama-3B-LoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "quantumaikr/QuantumLM": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-65b-gpt4-2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Aspik101/llama-30b-2048-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "yeontaek/Platypus2-13B-LoRa": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "zarakiquemparte/zarafusionix-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "huggingtweets/gladosystem": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "eachadea/legacy-vicuna-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Aeala/GPT4-x-AlpacaDente2-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "shibing624/chinese-llama-plus-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "euclaise/gpt-neox-122m-minipile-digits": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "TheBloke/UltraLM-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lvkaokao/llama2-7b-hf-instruction-lora": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "BreadAi/StoryPy": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "dvruette/oasst-pythia-12b-flash-attn-5000-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "aisquared/dlite-v1-124m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ewof/koishi-instruct-3b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/airoboros-7b-gpt4-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-13b-gpt4-1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/tulu-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yhyhy3/med-orca-instruct-33b": {"architectures": ["LlamaModel"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "heegyu/LIMA-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "abhishek/llama2guanacotest": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "heegyu/LIMA2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Corianas/Quokka_256m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14}, "golaxy/gogpt-560m": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "OptimalScale/robin-7b-v2-delta": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bofenghuang/vigogne-13b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "klosax/pythia-160m-deduped-step92k-193bt": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "golaxy/gogpt2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "YeungNLP/firefly-llama2-13b-v1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "WhoTookMyAmogusNickname/NewHope_HF_not_official": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/CodeLlama-34b-Python-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "KnutJaegersberg/megatron-GPT-2-345m-EvolInstruct": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24}, "Aeala/Alpaca-elina-65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Monero/WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "csitfun/llama-7b-logicot": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "OptimalScale/robin-65b-v2-delta": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "LLMs/WizardLM-13B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CobraMamba/mamba-gpt-3b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "aisquared/dlite-v1-1_5b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "nthngdy/pythia-owt2-70m-100k": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "LLMs/AlpacaGPT4-7B-elina": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-7b-gpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "grantprice/Cerebras-GPT-590M-finetuned-DND": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 12, "n_inner": 6144, "n_layer": 18}, "TheBloke/robin-13B-v2-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/robin-65b-v2-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "FPHam/Free_Sydney_13b_HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "acrastt/RedPajama-INCITE-Chat-Instruct-3B-V1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-65b-gpt4-m2.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "heegyu/LIMA2-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "BreadAi/MuseCan": {"architectures": ["GPT2LMHeadModel"], "n_embd": 960, "n_head": 15, "n_inner": 9, "n_layer": 5}, "ausboss/llama-13b-supercot": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "openaccess-ai-collective/manticore-30b-chat-pyg-alpha": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "OptimalScale/robin-13b-v2-delta": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "quantumaikr/llama-2-7b-hf-guanaco-1k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Fredithefish/RedPajama-INCITE-Chat-3B-ShareGPT-11K": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "CalderaAI/13B-BlueMethod": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "SaylorTwift/gpt2_test": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "WeOpenML/PandaLM-Alpaca-7B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "WeOpenML/Alpaca-7B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sumo43/lora_moe_7b_baseline": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "wenge-research/yayi-13b-llama2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "golaxy/gowizardlm": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "abhiramtirumala/DialoGPT-sarcastic-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Corianas/Quokka_2.7b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2560, "n_head": 32, "n_inner": 10240, "n_layer": 32}, "Corianas/256_5epoch": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1088, "n_head": 17, "n_inner": 4352, "n_layer": 14}, "dvruette/llama-13b-pretrained": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/alpaca-lora-65B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "ashercn97/giraffe-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Aspik101/Vicuzard-30B-Uncensored-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/dromedary-65b-lora-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Yhyu13/chimera-inst-chat-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ehartford/based-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "concedo/Vicuzard-30B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "64bits/LexPodLM-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "MayaPH/GodziLLa-30B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Aspik101/vicuna-7b-v1.3-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "julianweng/Llama-2-7b-chat-orcah": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "heegyu/RedTulu-Uncensored-3B-0719": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "Aspik101/Llama-2-7b-hf-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/QuantumLM-70B-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "BreadAi/gpt-YA-1-1_160M": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "dvruette/oasst-pythia-12b-pretrained-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "Aeala/GPT4-x-AlpacaDente-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TehVenom/Pygmalion_AlpacaLora-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "LLMs/Stable-Vicuna-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "quantumaikr/open_llama_7b_hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Aeala/GPT4-x-Alpasta-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Fredithefish/CrimsonPajama": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "openaccess-ai-collective/hippogriff-30b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "camel-ai/CAMEL-13B-Role-Playing-Data": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/landmark-attention-llama7b-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/robin-33B-v2-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/GPlatty-30B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/Chinese-Alpaca-33B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/CAMEL-33B-Combined-Data-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "klosax/open_llama_13b_600bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Aspik101/Nous-Hermes-13b-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-l2-7b-gpt4-1.4.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "YeungNLP/firefly-llama-30b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "ashercn97/manatee-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lizhuang144/starcoder_mirror": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "Aspik101/vicuna-13b-v1.5-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Aspik101/Redmond-Puffin-13B-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Aspik101/StableBeluga-13B-instruct-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "RoversX/llama-2-7b-hf-small-shards-Samantha-V1-SFT": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Corianas/Quokka_1.3b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "nthngdy/pythia-owt2-70m-50k": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "danielhanchen/open_llama_3b_600bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/VicUnlocked-alpaca-65B-QLoRA-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "kevinpro/Vicuna-13B-CoT": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "wahaha1987/llama_7b_sharegpt94k_fastchat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openaccess-ai-collective/minotaur-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/tulu-7B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "golaxy/gogpt-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Aeala/Enterredaas-33b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "kingbri/chronolima-airo-grad-l2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheTravellingEngineer/bloom-560m-RLHF": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "HWERI/Llama2-7b-sharegpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "l3utterfly/llama2-7b-layla": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-13b-Guanaco-QLoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "duliadotio/dulia-13b-8k-alpha": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/llama-2-13B-ensemble-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/oasst-gpt-neox-20b-3000-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "dvruette/oasst-gpt-neox-20b-1000-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "huggingtweets/jerma985": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Dampish/Dante-2.8B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Planner-7B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "klosax/pythia-70m-deduped-step44k-92bt": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "klosax/open_llama_7b_400bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "golaxy/gogpt2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Lajonbot/Llama-2-7b-chat-hf-instruct-pl-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheTravellingEngineer/llama2-7b-chat-hf-guanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Lajonbot/vicuna-7b-v1.5-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "kingbri/airolima-chronos-grad-l2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/llama-2-70B-ensemble-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "dvruette/oasst-llama-13b-2-epochs": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/llama-13b-pretrained-sft-epoch-1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/llama-13b-pretrained-dropout": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hakurei/instruct-12b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "dvruette/gpt-neox-20b-full-precision": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 6144, "intermediate_size": 24576, "num_attention_heads": 64, "num_hidden_layers": 44}, "Monero/WizardLM-13b-OpenAssistant-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Yhyu13/llama-30B-hf-openassitant": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-65b-gpt4-1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "camel-ai/CAMEL-33B-Combined-Data": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "MBZUAI/bactrian-x-llama-13b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dsvv-cair/alpaca-cleaned-llama-30b-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "YeungNLP/firefly-llama-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "YeungNLP/firefly-llama-13b-v1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "heegyu/WizardVicuna2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/oasst-llama-13b-1000-steps": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dvruette/llama-13b-pretrained-sft-do2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pillowtalks-ai/delta13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "illuin/test-custom-llama": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "MrNJK/gpt2-xl-sft": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "PocketDoc/Dans-PileOfSets-Mk1-llama-13b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-65b-gpt4-1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "frank098/WizardLM_13B_juniper": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "golaxy/goims": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "dvruette/oasst-pythia-6.9b-4000-steps": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "mncai/chatdoctor": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "wannaphong/openthaigpt-0.1.0-beta-full-model_for_open_llm_leaderboard": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "golaxy/gogpt-3b-bloom": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "golaxy/gogpt-7b-bloom": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "jondurbin/airoboros-33b-gpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "jondurbin/airoboros-13b-gpt4-1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-7b-gpt4-1.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-13b-gpt4-1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-7b-gpt4-1.4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-13b-gpt4-1.4-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-7b-gpt4-1.4.1-qlora": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "frank098/orca_mini_3b_juniper": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "Lajonbot/vicuna-13b-v1.3-PL-lora_unload": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jxhong/CAlign-alpaca-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/KoreanLM-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "keyfan/vicuna-chinese-replication-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-7b-gpt4-1.3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jerryjalapeno/nart-100k-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "xzuyn/Alpacino-SuperCOT-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "wahaha1987/llama_13b_sharegpt94k_fastchat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "quantumaikr/QuantumLM-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Undi95/ReMM-SLERP-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "huggingtweets/bladeecity-jerma985": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "pszemraj/pythia-6.9b-HC3": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "CalderaAI/30B-Epsilon": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TFLai/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "alpindale/pygmalion-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/airoboros-c34b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "abacaj/starcoderbase-1b-sft": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "bongchoi/test-llama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TinyPixel/lima-test": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/llama-2-70B-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "yeontaek/llama-2-13B-ensemble-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "cointegrated/rut5-base-absum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "pankajmathur/model_420_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Fredithefish/Guanaco-3B-Uncensored-v2": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-70B-ensemble-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Writer/palmyra-large": {"architectures": ["GPT2LMHeadModel"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 44}, "RobbeD/OpenLlama-Platypus-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TFLai/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NobodyExistsOnTheInternet/PuffedConvo13bLoraE4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Sao10K/Medusa-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFLai/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TFLai/MythoMix-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "chargoddard/llama-2-34b-uncode": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "zarakiquemparte/zaraxls-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TFLai/Stable-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Danielbrdz/Barcenas-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TFLai/Limarp-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFLai/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "The-Face-Of-Goonery/Huginn-13b-v4.5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-l2-7b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "allenai/unifiedqa-v2-t5-large-1363200": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "synapsoft/Llama-2-7b-hf-flan2022-1.2M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/Platypus2-13B-LoRa-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "KES/T5-KES": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "robowaifudev/megatron-gpt2-345m": {"n_embd": 1024, "n_layer": 24, "n_head": 16, "n_inner": 4096, "architectures": ["GPT2LMHeadModel"]}, "Sao10K/Mythical-Destroyer-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-dolphin_20w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "The-Face-Of-Goonery/Huginn-13b-V4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "haining/scientific_abstract_simplification": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "ChanonUtupon/openthaigpt-merge-lora-llama-2-7B-3470k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "chaoyi-wu/PMC_LLAMA_7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "CHIH-HUNG/llama-2-13b-OpenOrca_5w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "clibrain/lince-zero": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/Project-Baize-v2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "uukuguy/speechless-codellama-platypus-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-dolphin_5w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/airoboros-2.1-llama-2-13B-QLoRa": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-llama2-luban-orca-platypus-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Norquinal/llama-2-7b-claude-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TFLai/Luban-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "conceptofmind/Open-LLongMA-3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "Norquinal/llama-2-7b-claude-chat-rp": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lgaalves/llama-2-7b-hf_open-platypus": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ziqingyang/chinese-llama-2-7b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yeontaek/llama-2-13B-ensemble-v6": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/llama-2-70B-ensemble-v7": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "ubikpt/t5-small-finetuned-cnn": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "rajkumarrrk/t5-base-fine-tuned-on-cnn-dm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/t5-efficient-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "TFLai/Airboros2.1-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-4096-llama2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TFLai/Ensemble5-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFLai/Athena-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "4bit/Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TFLai/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFLai/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Salesforce/codegen25-7b-mono": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Sao10K/Stheno-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yeontaek/WizardCoder-Python-13B-LoRa": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-orca-platypus-coig-lite-2k-0.6e-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "casperhansen/vicuna-7b-v1.5-awq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "luffycodes/nash-vicuna-33b-v1dot3-ep2-w-rag-w-simple": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "CHIH-HUNG/llama-2-13b-OpenOrca_20w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "google/t5-efficient-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/orca_mini_v2_7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "tianyil1/denas-llama2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Sao10K/Stheno-Inverted-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "junelee/ko_vicuna_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Seungyoun/codellama-7b-instruct-pad": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Kimiko-v2-13B-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-orca-platypus-coig-lite-4k-0.5e-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-orca-platypus-coig-lite-4k-0.6e-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Undi95/UndiMix-v1-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "yeontaek/llama-2-70B-ensemble-v6": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/vicuna-13B-v1.5-16K-GGML": {}, "KnutJaegersberg/black_goo_recipe_a": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "PKU-Alignment/beaver-7b-v1.0-reward": {"architectures": ["LlamaModelForScore"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "KnutJaegersberg/black_goo_recipe_b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "lgaalves/gpt2_open-platypus": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "cointegrated/rut5-base-multitask": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "Cheng98/llama-160m": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "Andron00e/YetAnother_Open-Llama-3B-LoRA-OpenOrca": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "lgaalves/gpt2_guanaco-dolly-platypus": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "gagan3012/k2t-base": {"architectures": ["T5WithLMHeadModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "marcchew/Platypus-2-7B-LaMini-14K": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lgaalves/gpt2_platypus-dolly-guanaco": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "czearing/article-title-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "luffycodes/mcq-vicuna-13b-v1.5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Universal-NER/UniNER-7B-definition": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Meli/GPT2-Prompt": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "s-nlp/ruT5-base-detox": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "cointegrated/rut5-base-paraphraser": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "DevaMalla/llama7b_alpaca_bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Universal-NER/UniNER-7B-type": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/starchat-beta-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "bigscience/sgpt-bloom-7b1-msmarco": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "4bit/Llama-2-13b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ClueAI/PromptCLUE-base-v1-5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "budecosystem/genz-13b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/LlongOrca-13B-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ozcangundes/mt5-multitask-qa-qg-turkish": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "EleutherAI/pythia-410m-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "CHIH-HUNG/llama-2-13b-FINETUNE2_3w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sonoisa/t5-base-japanese-v1.1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "bolbolzaban/gpt2-persian": {"n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24, "architectures": ["GPT2LMHeadModel"]}, "google/t5-large-ssm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "DeepPavlov/rudialogpt3_medium_based_on_gpt2_v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Mikivis/xuanxuan": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "uukuguy/speechless-llama2-hermes-orca-platypus-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "KnutJaegersberg/black_goo_recipe_c": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "beaugogh/Llama2-7b-sharegpt4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Salesforce/codet5p-770m-py": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "codefuse-ai/CodeFuse-CodeLlama-34B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "AUTOMATIC/promptgen-majinai-safe": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "reciprocate/shepherd-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Devio/test-22B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "acrastt/Bean-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/L2-MythoMax22b-Instruct-Falseblock-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "vihangd/smartplat-3b-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "jinaai/jina-embedding-b-en-v1": {"architectures": ["T5EncoderModel"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "yahma/llama-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uukuguy/speechless-codellama-orca-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE1_17w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "VMware/open-llama-13b-open-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ToolBench/ToolLLaMA-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "luffycodes/mcq-hal-vicuna-13b-v1.5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/BigTranslate-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "PeanutJar/LLaMa-2-PeanutButter_v18_A-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "openbmb/UltraLM-65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Mikivis/gpt2-large-lora-sft": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "Devio/test-3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 16}, "akhooli/gpt2-small-arabic": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "Rardilit/Panther_v1": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ValiantLabs/ShiningValiant": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Devio/test100": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "Devio/testC": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "TheBloke/Chronoboros-33B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "vihangd/smartplat-3b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "laituan245/t5-v1_1-small-smiles2caption-ft-from-pretrained-c4": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "4bit/Llama-2-7b-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "CHIH-HUNG/llama-2-13b-FINETUNE2_3w-gate_up_down_proj": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE2_3w-q_k_v_o_proj": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/vicuna-33B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Devio/test-1400": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "TheBloke/gpt4-alpaca-lora-30B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "notstoic/pygmalion-13b-4bit-128g": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/Yarn-Llama-2-7b-128k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Den4ikAI/FRED-T5-LARGE_text_qa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "valhalla/t5-base-qa-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Undi95/ReMM-L2-13B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Zarablend-L2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "KnutJaegersberg/black_goo_recipe_d": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "ckip-joint/bloom-1b1-zh": {"architectures": ["BloomModel"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "seonglae/llama-2-13b-chat-hf-gptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Trelis/Llama-2-7b-chat-hf-sharded-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "KnutJaegersberg/LLongMA-3b-LIMA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "h2oai/h2ogpt-gm-oasst1-en-xgen-7b-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "PeanutJar/LLaMa-2-PeanutButter_v18_B-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ehartford/WizardLM-1.0-Uncensored-CodeLlama-34b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "uukuguy/speechless-codellama-orca-platypus-13b-0.10e": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "DeepESP/gpt2-spanish": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "paust/pko-flan-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "ThomasNLG/t5-qa_squad2neg-en": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PharMolix/BioMedGPT-LM-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "eenzeenee/t5-base-korean-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "porkorbeef/Llama-2-13b-public": {"architectures": ["LlamaModel"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "dahara1/weblab-10b-instruction-sft-GPTQ": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4864, "intermediate_size": 19456, "num_attention_heads": 38, "num_hidden_layers": 36}, "CHIH-HUNG/llama-2-13b-FINETUNE2_TEST_2.2w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mrm8488/t5-small-finetuned-emotion": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "gurgutan/saiga2-13b-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IlyaGusev/rut5_base_sum_gazeta": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gaodrew/OpenOrca-Platypus2-13B-thera-1250": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "minlik/chinese-llama-7b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Stable-Platypus2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Luna-AI-Llama2-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "allenai/t5-small-squad2-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "microsoft/bloom-deepspeed-inference-fp16": {"architectures": ["BloomModel"], "n_layer": 70, "num_attention_heads": 112}, "csebuetnlp/banglat5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "potsawee/t5-large-generation-race-QuestionAnswer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "grammarly/coedit-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "Narrativaai/bloom-560m-finetuned-totto-table-to-text": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "jjaaaww/posi_13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Undi95/Nous-Hermes-13B-Code": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "paust/pko-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "learnanything/llama-7b-huggingface": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "weiren119/Taiwan-LLaMa-v1.0-4bits-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ml6team/keyphrase-generation-t5-small-inspec": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/CodeLlama-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Undi95/MLewd-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "tscholak/cxmefzzi": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "Gaivoronsky/ruGPT-3.5-13B-8bit": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "SatoruDano/llama-2-7b-finetuned_v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ClueAI/PromptCLUE-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "uukuguy/speechless-codellama-orca-airoboros-13b-0.10e": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "anonymous-german-nlp/german-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "fxmarty/gpt2-tiny-onnx": {"intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "prakharz/DIAL-FLANT5-XL": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "h2oai/h2ogpt-oasst1-falcon-40b": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Undi95/ReMM-L2-13B-PIPPA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE1_17w-gate_up_down_proj": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Fredithefish/Guanaco-7B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "conceptofmind/Yarn-Llama-2-13b-128k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Undi95/LewdEngine": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/CodeLlama-7B-Instruct-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE1_17w-q_k_v_o_proj": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jondurbin/airoboros-33b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Salesforce/codet5p-220m-py": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Danielbrdz/CodeBarcenas-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "SJ-Ray/Re-Punctuate": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "EasthShin/Youth_Chatbot_Kogpt2-base": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ThomasNLG/t5-qg_squad1-en": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "EleutherAI/pythia-160m-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "MBZUAI/LaMini-T5-223M": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "HooshvareLab/gpt2-fa": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TFLai/Nova-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "conceptofmind/LLongMA-2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TDC2023/trojan-base-pythia-1.4b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "phpaiola/ptt5-base-summ-xlsum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TFLai/SpeechlessV1-Nova-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/stablecode-instruct-alpha-3b-GPTQ": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "stanford-crfm/music-small-800k": {"n_embd": 768, "n_layer": 12, "n_head": 12, "n_inner": null, "architectures": null}, "TFLai/EnsembleV5-Nova-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "declare-lab/flan-alpaca-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "jpwahle/t5-large-word-sense-disambiguation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "lizhuang144/flan-t5-large-factual-sg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "DKYoon/mt5-base-lm-adapt": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/guanaco-65B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Salesforce/codegen25-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bigscience-data/sgpt-bloom-1b7-nli": {"architectures": ["BloomModel"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "TurkuNLP/gpt3-finnish-small": {"architectures": ["BloomModel"], "hidden_size": 768, "n_head": 12, "n_layer": 12}, "jordiclive/flan-t5-3b-summarizer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "marblyso/DialoGPT-small-what-the-fuck": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "retrieva-jp/t5-small-short": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "codeparrot/codeparrot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Rocketknight1/falcon-rw-1b": {"architectures": ["FalconForCausalLM"], "hidden_size": 2048, "num_attention_heads": 32, "num_hidden_layers": 24}, "TaylorAI/Flash-Llama-30M-20001": {"architectures": ["LlamaForCausalLM"], "hidden_size": 384, "intermediate_size": 1024, "num_attention_heads": 12, "num_hidden_layers": 4}, "castorini/t5-base-canard": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "r3dhummingbird/DialoGPT-medium-joshua": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "IDEA-CCNL/Wenzhong2.0-GPT2-110M-BertTokenizer-chinese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "TigerResearch/tigerbot-13b-chat-8bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pranavpsv/gpt2-genre-story-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "Photolens/llama-2-7b-langchain-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ck46/t5-base-hotpot-qa-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "castorini/monot5-small-msmarco-10k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "yujiepan/llama-2-tiny-random": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8, "intermediate_size": 32, "num_attention_heads": 2, "num_hidden_layers": 1}, "castorini/doc2query-t5-base-msmarco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "oliverguhr/spelling-correction-multilingual-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "allenai/unifiedqa-t5-11b": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "TheBloke/CodeLlama-34B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "snorkelai/sdnet": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "SiberiaSoft/SiberianFRED-T5-XL": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "sultan/ArabicT5-Base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 20}, "nikaashpuri/gpt-expt-sp-v3-K-600-MA-Mac-actions-kmeans-v16": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Yarn-Llama-2-13B-128K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "allenai/cosmo-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "flax-community/gpt2-bengali": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "EleutherAI/pythia-410m-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "Writer/palmyra-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "LukasStankevicius/t5-base-lithuanian-news-summaries-175": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "laituan245/molt5-large-caption2smiles": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "google/ul2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 4096, "num_heads": 16, "num_layers": 32}, "Suva/uptag-keyphrase-model": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/orca_mini_7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TusharJoshi89/title-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "juierror/flan-t5-text2sql-with-schema": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hf-tiny-model-private/tiny-random-T5ForConditionalGeneration": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "stacked-summaries/flan-t5-large-stacked-samsum-1024": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "persiannlp/mt5-base-parsinlu-opus-translation_fa_en": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "gurgutan/ruGPT-13B-4bit": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "TheBloke/upstage-llama-30b-instruct-2048-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "sdadas/polish-gpt2-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24}, "aubmindlab/aragpt2-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "SEBIS/code_trans_t5_large_source_code_summarization_python_multitask_finetune": {"architectures": ["T5Model"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "maximxls/text-normalization-ru-terrible": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 256, "num_heads": 4, "num_layers": 3}, "TheBloke/llama-2-13B-Guanaco-QLoRA-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ziqingyang/chinese-alpaca-2-13b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "KETI-AIR/ke-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "ibm/qcpg-sentences": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "tiiuae/falcon-rw-7b": {"architectures": ["FalconForCausalLM"], "hidden_size": 4096, "num_attention_heads": 64, "num_hidden_layers": 36}, "timdettmers/guanaco-13b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-oig-oasst1-falcon-40b": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "coffeeee/nsfw-story-generator": {"architectures": ["GPT2Model"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "zpn/llama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "crumb/bloom-560m-RLHF-SD2-prompter-aesthetic": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "kalpeshk2011/dipper-paraphraser-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "TheBloke/WizardLM-13B-V1.0-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "allenai/unifiedqa-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "oliverguhr/spelling-correction-german-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ThomasSimonini/t5-end2end-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "asi/gpt-fr-cased-base": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1792, "n_head": 14, "n_inner": null, "n_layer": 24}, "lora-x/backpack-gpt2": {"architectures": ["BackpackGPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Vigogne-2-13B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ai-forever/ruT5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "ml6team/keyphrase-generation-t5-small-openkp": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "mrm8488/t5-base-finetuned-e2m-intent": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "nikaashpuri/gpt-expt-sp-v3-K-600-MA-Mac-actions-kmeans-v14": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Marx-3b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/Dolphin-Llama2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "oscorrea/scores-falcon40b-sm-merged": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "lmqg/t5-small-squad-qag": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "ehartford/WizardLM-Uncensored-Falcon-40b": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "persiannlp/mt5-base-parsinlu-sentiment-analysis": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "VietAI/vit5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "thanathorn/mt5-cpe-kmutt-thai-sentence-sum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Blackroot/Hermes-Kimiko-13B-f16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CarperAI/stable-vicuna-13b-delta": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "j5ng/kullm-12.8b-GPTQ-8bit": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/ReMM-SLERP-L2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Weni/WeniGPT-L-70": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "valhalla/t5-small-qg-hl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "retrieva-jp/t5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "TheBloke/Wizard-Vicuna-30B-Superhot-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "openllmplayground/openalpaca_3b_600bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "ArmelR/starcoder-gradio-v0": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "chanind/frame-semantic-transformer-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "akreal/tiny-random-gpt2": {"intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "Neko-Institute-of-Science/LLaMA-7B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Writer/palmyra-med-20b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 44}, "SiberiaSoft/SiberianPersonaFred": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "mrm8488/spanish-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "unicamp-dl/translation-en-pt-t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "OFA-Sys/gsm8k-rft-llama7b-u13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "liuhaotian/LLaVA-13b-delta-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "huggingface/falcon-40b-gptq": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "Ravi07bec/llama-qlora-65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "PKU-Alignment/alpaca-7b-reproduced": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Unbabel/gec-t5_small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "MIIB-NLP/Arabic-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/t5-large-ssm-nq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "stanford-crfm/arwen-gpt2-medium-x21": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "sentence-transformers/gtr-t5-xxl": {"architectures": ["T5EncoderModel"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "TheBloke/Nous-Hermes-Llama2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "paust/pko-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "allenai/tk-instruct-11b-def": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "amphora/FinABSA": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TurkuNLP/gpt3-finnish-13B": {"architectures": ["BloomModel"], "hidden_size": 5120, "n_head": 40, "n_layer": 40}, "PAIXAI/Astrid-LLama-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Aalaa/opt-125m-wikitext2": {"architectures": ["OPTForCausalLM"], "hidden_size": 768, "num_attention_heads": 12, "num_hidden_layers": 12}, "hf-internal-testing/tiny-random-GPTNeoXForQuestionAnswering": {"architectures": ["GPTNeoXForQuestionAnswering"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "cssupport/t5-small-awesome-text-to-sql": {"d_model": 512, "d_ff": 2048, "num_layers": 6, "num_heads": 8, "architectures": ["T5ForConditionalGeneration"]}, "TheBloke/MythoMix-L2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "conceptofmind/Hermes-LLongMA-2-13b-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lysandre/arxiv-nlp": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "Pcik/DialoGPT-medium-Kirby": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "PY007/SLM_1-4B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 2048, "intermediate_size": 5632, "num_attention_heads": 16, "num_hidden_layers": 24}, "ceshine/t5-paraphrase-paws-msrp-opinosis": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "microsoft/bloom-deepspeed-inference-int8": {"architectures": ["BloomModel"], "n_layer": 70, "num_attention_heads": 112}, "TheBloke/PuddleJumper-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gorilla-llm/gorilla-falcon-7b-hf-v0": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/starcoder-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "lmsys/longchat-7b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DAMO-NLP-MT/polylm-1.7b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "Salesforce/xgen-7b-4k-base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DAMO-NLP-MT/polylm-13b": {"architectures": ["PolyLMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": 20480, "n_layer": 40}, "dbddv01/gpt2-french-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "EleutherAI/pythia-70m-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "algolet/mt5-base-chinese-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hf-internal-testing/tiny-random-BloomForQuestionAnswering": {"architectures": ["BloomForQuestionAnswering"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "hf-internal-testing/tiny-random-BloomForTokenClassification": {"architectures": ["BloomForTokenClassification"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "flax-community/t5-base-cnn-dm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hf-internal-testing/tiny-random-BloomForSequenceClassification": {"architectures": ["BloomForSequenceClassification"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "tau/t5-v1_1-large-rss": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/airoboros-l2-13b-gpt4-m2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-GPTNeoXForSequenceClassification": {"architectures": ["GPTNeoXForSequenceClassification"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "allegro/plt5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/stable-vicuna-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "hf-internal-testing/tiny-random-GPTNeoXForTokenClassification": {"architectures": ["GPTNeoXForTokenClassification"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "HuggingFaceH4/tiny-random-LlamaForSequenceClassification": {"architectures": ["LlamaForSequenceClassification"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "hf-internal-testing/tiny-random-GPTNeoXModel": {"architectures": ["GPTNeoXModel"], "hidden_size": 32, "intermediate_size": 37, "num_attention_heads": 4, "num_hidden_layers": 5}, "IlyaGusev/rut5_base_headline_gen_telegram": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lgaalves/gpt2_camel_physics-platypus": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lightonai/alfred-40b-0723": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "KETI-AIR/ke-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "ibm/regen-disambiguation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "vihangd/smartplat-3b-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/OpenBuddy-Llama2-13B-v11.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BlinksFly/Harry_Potter-Ai": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "conceptofmind/Yarn-Llama-2-7b-128k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "liujch1998/vera": {"architectures": ["T5EncoderModel"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "kaist-ai/CoT-T5-11B": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "lintang/t5-v1_1-base-flan": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sentence-transformers/sentence-t5-xxl": {"architectures": ["T5EncoderModel"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "TheBloke/vicuna-7B-v1.5-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "retrieva-jp/t5-large-long": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "retrieva-jp/t5-base-long": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "upstage/SOLAR-0-70b-8bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "jerteh/gpt2-vrabac": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Parth/boolean": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "hf-internal-testing/tiny-random-GPTBigCodeForSequenceClassification": {"architectures": ["GPTBigCodeForSequenceClassification"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "hf-internal-testing/tiny-random-GPTBigCodeForTokenClassification": {"architectures": ["GPTBigCodeForTokenClassification"], "n_embd": 32, "n_head": 4, "n_inner": 37, "n_layer": 5}, "megagonlabs/t5-base-japanese-web": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "MisguidedKerbal/DialoGPT-kerbalV3": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "praeclarum/cuneiform": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "uw-hai/polyjuice": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "reciprocate/tiny-llama": {"architectures": ["LlamaForCausalLM"], "hidden_size": 64, "intermediate_size": 64, "num_attention_heads": 1, "num_hidden_layers": 1}, "luqh/ClinicalT5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "stanford-crfm/celebrimbor-gpt2-medium-x81": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/CodeLlama-13B-Python-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "laituan245/molt5-large-smiles2caption": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TurkuNLP/gpt3-finnish-8B": {"architectures": ["BloomModel"], "hidden_size": 4096, "n_head": 32, "n_layer": 32}, "NeuML/t5-small-txtsql": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "malteos/bloom-6b4-clp-german": {"hidden_size": 4096, "n_head": 32, "n_layer": 30}, "GT4SD/multitask-text-and-chemistry-t5-base-augm": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "allenai/open-instruct-stanford-alpaca-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "CarperAI/randomwalks": {"architectures": ["GPT2LMHeadModel"], "n_embd": 144, "n_head": 6, "n_inner": null, "n_layer": 6}, "unicamp-dl/mt5-13b-mmarco-100k": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "lmqg/t5-small-squad-qg-ae": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "naltukhov/joke-generator-rus-t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "hf-internal-testing/tiny-random-UMT5Model": {"architectures": ["UMT5Model"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "rentcarsAI/falcon-7b-codegenerator-qlora-merged": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "panggi/t5-base-indonesian-summarization-cased": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hf-internal-testing/tiny-random-UMT5ForQuestionAnswering": {"architectures": ["UMT5ForQuestionAnswering"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "UBC-NLP/AraT5-base": {"d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "kmewhort/stable-diffusion-prompt-bolster": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Llama-2-13B-GGML": {}, "gaussalgo/T5-LM-Large-text2sql-spider": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "DAMO-NLP-MT/polylm-multialpaca-13b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": 20480, "n_layer": 40}, "hf-internal-testing/tiny-random-UMT5ForSequenceClassification": {"architectures": ["UMT5ForSequenceClassification"], "d_ff": 37, "d_model": 32, "num_heads": 4, "num_layers": 5}, "tinkoff-ai/ruDialoGPT-small": {"n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "indonesian-nlp/gpt2-medium-indonesian": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Salesforce/mixqg-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "EleutherAI/pythia-1b-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "NinedayWang/PolyCoder-2.7B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "zanchat/falcon-1b": {"architectures": ["RWForCausalLM"], "hidden_size": 2048, "n_head": 32, "n_layer": 24}, "Goodnoway/DialoGPT-nerbalV2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "crumb/llama2-7b-shard-bf16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sagawa/ReactionT5-retrosynthesis": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "DKYoon/mt5-large-lm-adapt": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "lintang/t5-v1_1-xl-flan": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "castorini/monot5-large-msmarco-10k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Ichsan2895/Merak-7B-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "stanford-crfm/caprica-gpt2-small-x81": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "vicgalle/gpt2-open-instruct-v1": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "philschmid/llama-2-7b-instruction-generator": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "aubmindlab/aragpt2-large": {"architectures": ["GPT2LMHeadModel"], "intermediate_size": 5120, "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "NonzeroCornet34/DialoGPT-small-philbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "deep-learning-analytics/wikihow-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "JDBN/t5-base-fr-qg-fquad": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "stanford-crfm/durin-gpt2-medium-x343": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "abjbpi/Dwight_Schrute": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Spico/Humback-Myx": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "T-Systems-onsite/mt5-small-sum-de-en-v2": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "kaiyuy/leandojo-lean3-tacgen-byt5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "pinkmanlove/llama-33b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "lintang/t5-v1_1-large-flan": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Naseej/noon-7b": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "chizhikchi/sci-five-radsum23": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "impyadav/GPT2-FineTuned-Hinglish-Song-Generation": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "elinas/llama-13b-hf-transformers-4.29": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/GodziLLa2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/Llama-2-70B-OASST-1-200-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "jacobmorrison/tk-instruct-base-lora-experiments": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "ingen51/DialoGPT-medium-GPT4": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "cointegrated/rut5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PocketDoc/Dans-CreepingSenseOfDoom": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "tsmatz/mt5_summarize_japanese": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "domenicrosati/QA2D-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "gorkemgoknar/gpt2chatbotenglish": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "DeliveryBoy/DiabloGPT-medium-Kurisu": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "philschmid/instruct-igel-001": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_layer": 30}, "xDAN2099/xDAN_13B_Zh_Base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "codeparrot/codeparrot-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "paust/pko-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "flozi00/Llama-2-13b-german-assistant-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "doc2query/msmarco-t5-base-v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "microsoft/DialogRPT-depth": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "nomic-ai/gpt4all-13b-snoozy": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NousResearch/Yarn-Llama-2-13b-64k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mrm8488/t5-base-e2e-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "postbot/gpt2-medium-emailgen": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "vanilladucky/Friends_chatting_bot_redefined": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/LlongOrca-7B-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mutamuta/DialoGPT-spongebob-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Ar4ikov/gpt2-medium-650k-stable-diffusion-prompt-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/HermesLimaRP-L2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "clibrain/Llama-2-7b-ft-instruct-es-gptq-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Yarn-Llama-2-7B-128K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmqg/mt5-small-jaquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "allenai/tk-instruct-base-def-pos": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "davidkim205/komt-Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "tangy0/llama-2-7b-dtlpy_v0.4chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TigerResearch/tigerbot-70b-base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "hadifar/eventextraction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TintinMeimei/NousResearch-Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Nekochu/Llama-2-13B-fp16-french": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "minhtoan/t5-translation-vietnamese-nom": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 300, "num_heads": 8, "num_layers": 6}, "BELLE-2/BELLE-Llama2-13B-chat-0.4M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigscience/T0": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "andreaskoepf/pythia-1.4b-gpt4all-pretrain": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "Salesforce/codet5-base-codexglue-clone": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Chae/scottbot_med": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/LLaMA-7b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sagard21/python-code-explainer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "stanfordnlp/SteamSHP-flan-t5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "MarinHinawa/DialoGPT-medium-Ene": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "aiautomationlab/german-news-title-gen-mt5": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/vicuna-13B-1.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Chronos-Hermes-13B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "microsoft/DialogRPT-human-vs-machine": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "uer/gpt2-distil-chinese-cluecorpussmall": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "davidkim205/komt-Llama-2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ibm/qcpg-questions": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "gavin124/gpt2-finetuned-cnn-summarization-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "hogru/MolReactGen-GuacaMol-Molecules": {"architectures": ["GPT2LMHeadModel"], "n_embd": 144, "n_head": 12, "n_inner": null, "n_layer": 12}, "stanford-crfm/darkmatter-gpt2-small-x343": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "conceptofmind/Yarn-Llama-2-7b-64k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Radicalkiddo/DialoGPT-small-Radical": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Ninja5000/DialoGPT-medium-HarryPotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "theblackcat102/alpaca-title-generator-mt0-large": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "transfaeries/Twilight-Sparkle-GPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Vigogne-2-7B-Instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "markofhope/DialoGPT-medium-HarringtonBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "seeksery/DialoGPT-calig3": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "beomi/kcgpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "vilm/vietcuna-3b": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "IDEA-CCNL/Randeng-T5-784M": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "vwxyzjn/starcoderbase-triviaqa": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 3072, "n_head": 32, "n_inner": 12288, "n_layer": 30}, "TheBloke/Llama-2-7b-Chat-GGUF": {}, "MingZhong/unieval-dialog": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "OpenAssistant/falcon-40b-megacode2-oasst": {"architectures": ["FalconForCausalLM"], "hidden_size": 8192, "num_attention_heads": 128, "num_hidden_layers": 60}, "axiong/PMC_LLaMA_13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "codeparrot/codeparrot-small-multi": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "EleutherAI/pythia-6.9b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "Riiid/sheep-duck-llama-2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "alibaba-pai/pai-bloom-1b1-text2prompt-sd": {"architectures": ["BloomForCausalLM"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Chronos-Beluga-v2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "malmarjeh/t5-arabic-text-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "GarfExit/DialogGPT-medium-707": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "audreycl/DialoGPT-RPF": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "florentiino/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "JazzyLucas/DialoGPT-small-TonyStark": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "marblyso/DialoGPT-medium-marina": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "polandball/GPT-Polen": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "GarrisonBot/DialoGPT-medium-herbertgarrison": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "XuYipei/kw-cutegpt-13b-ift": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Pygmalion-7B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "timothykim04/DialoGPT-medium-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "allegro/plt5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lengoctuong/gpt2-finetuned-wikitext2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "PyaeSoneK/Fine_Tuned_Pythia_smallest_140_legal": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "psyche/KoT5-paraphrase-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "microsoft/DialogRPT-width": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "Dahoas/pythia-1B-static-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "jerteh/gpt2-orao": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "TheBloke/LosslessMegaCoder-Llama2-13B-Mini-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Ngao/DialoGPT-small-ngao": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "4i-ai/Llama-2-7b-alpaca-es": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "asifhugs/open_llama_7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "RajuKandasamy/tamillama_tiny_30m": {"architectures": ["LlamaForCausalLM"], "hidden_size": 256, "intermediate_size": 786, "num_attention_heads": 8, "num_hidden_layers": 16}, "stabilityai/StableBeluga1-Delta": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "Linly-AI/Chinese-LLaMA-2-7B-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "flax-community/gpt2-base-thai": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "shalomma/llama-7b-embeddings": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "KhanAdeeb/model-tony-stark": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "spy24/autonlp-UK-to-US-600416931": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "DKYoon/mt5-small-lm-adapt": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/Llama-2-70B-GGML": {}, "TheBloke/model_007-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "abhi-8/DialoGPT-medium-Joshua-twevy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "camenduru/MiniGPT4-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "paripi/Malishka": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "SiberiaSoft/SiberianPersonaFred_large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Alred/t5-small-finetuned-summarization-cnn": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "Leomas/DialoGPT-medium-Leomas": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TehVenom/Pygmalion-7b-Merged-Safetensors": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "marblyso/DialoGPT-medium-pearl": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "lmqg/mt5-small-dequad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/WizardLM-Uncensored-Falcon-40B-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "NlpHUST/t5-small-vi-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Elucia/Diluc_Bot_1.3": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "h2oai/h2ogpt-16k-codellama-34b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "microsoft/CodeGPT-small-java": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "Starry/COUNTNARC": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "OpenMEDLab/PULSE-7bv5": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "marblyso/DialoGPT-medium-aubrey": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Ashypaws/DialoGPT-medium-Ashybot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "YTTD/DialoGPT-medium-sou": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "marblyso/DialoGPT-medium-hero": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Trelis/Llama-2-7b-chat-hf-function-calling-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NousResearch/CodeLlama-7b-hf-flash": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-34B-Python-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "musabgultekin/functionary-7b-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "amasand/gpt2-imdb-pos-ppo": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "bigscience/bloomz-7b1-p3": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "rirv938/wizard-vicuna-13b-uncensored-awq-4bit-g128": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "marblyso/DialoGPT-medium-marblesbagel": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "vilm/vietcuna-7b-v3": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "stas/t5-very-small-random": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 256, "d_model": 64, "num_heads": 4, "num_layers": 8}, "KeLiu/Title-Gen": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "vampiregirl/DialoGPT-medium-lennoxram": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "sharpbai/Llama-2-7b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sam2ai/openllama_odia_3b_base": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "lmqg/mt5-small-esquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "stanfordnlp/SteamSHP-flan-t5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "allenai/tulu-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "JNDankwah/DialoGPT-small-ThorCB": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-ruquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "lmqg/mt5-small-ruquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Dinocroth/DialoGPT-medium-Trevor-PhilipsV2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Speedemon/jake-peralta-ai": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "chanind/frame-semantic-transformer-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "stanford-crfm/music-medium-800k": {"n_embd": 1024, "n_layer": 24, "n_head": 16, "n_inner": null, "architectures": null}, "h2oai/h2ogpt-16k-codellama-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Pygmalion-2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "huggingface-course/codeparrot-ds": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "KakoSi/AcciGPT-smol": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-itquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "EggsInAJar/DialoGPT-small-MerrickBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "razent/SciFive-large-Pubmed": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "armandnlp/gpt2-TOD_finetuned_SGD": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "RuterNorway/Llama-2-13b-chat-norwegian": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "AIDC-ai-business/Marcoroni-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "deep-learning-analytics/GrammarCorrector": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "redrussianarmy/gpt2-turkish-cased": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-frquad-qg-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "psyche/KoT5-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "loitran/DialoGPT-medium-peppapig": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "openchat/openchat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "saikatc/NatGen": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Coderhuynin/DialoGPT-large-TonyStark": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "declare-lab/flan-sharegpt-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "usvsnsp/pythia-6.9b-rm-full-hh-rlhf": {"architectures": ["GPTNeoXForSequenceClassification"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "yujiepan/llama-2-tiny-3layers-random": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8, "intermediate_size": 32, "num_attention_heads": 2, "num_hidden_layers": 3}, "allenai/unifiedqa-v2-t5-3b-1363200": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "gsarti/it5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "simple2312/DialoGPT-Ellie": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "kashif/llama-7b_stack-exchange_RM_peft-adapter-merged": {"architectures": ["LlamaForSequenceClassification"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "larryvrh/mt5-translation-ja_zh": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "j5ng/et5-typos-corrector": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "vilsonrodrigues/falcon-7b-sharded": {"architectures": ["FalconForCausalLM"], "hidden_size": 4544, "num_attention_heads": 71, "num_hidden_layers": 32}, "felinecity/ScaraBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "persiannlp/mt5-base-parsinlu-translation_en_fa": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Jonesy/HomersNightOut": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "conceptofmind/LLongMA-2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/LoKuS-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "shibing624/mengzi-t5-base-chinese-correction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Lamia/DialoGPT-small-Sundrop": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Blizzchor/DialoGPT-medium-gamora": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "jlsalty9999/DialoGPT-medium-Riddle": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "uer/gpt2-chinese-lyric": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "LMFlow/Full-Robin-7b-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "llm-book/t5-base-long-livedoor-news-corpus": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "nuggster/DialoGPT-small-ianbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Tristan/gpt2_reward_summarization": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "MysteriousAmazon/DialoGPT-medium-freddy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "wdidfau/Pygmalion-13b-Landmark-Attention-Merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "kaiyuy/leandojo-lean3-retriever-byt5-small": {"architectures": ["T5EncoderModel"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "kz919/ntk_scaled_open_llama_3b_32k": {"architectures": ["NTKScaledLlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "abhi-8/DialoGPT-medium-Rick": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "polymath707/llama-2-13b-miniguanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Langboat/bloom-389m-zh": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Techcs002/DialoGPT-medium-AboTalkTest": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "AIDC-ai-business/Marcoroni-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ybelkada/t5-3b-sharded": {"architectures": ["T5WithLMHeadModel"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "benjamin/gerpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "abhi-8/DialoGPT-medium-Michael": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "cahya/gpt2-small-indonesian-522M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "marianna13/flan-t5-base-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Lakoc/fisher_dec_6_layers": {"architectures": ["GPT2Model"], "n_embd": 512, "n_head": 4, "n_inner": null, "n_layer": 6}, "simple2312/DialoGPT-nayeon": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "sjrhuschlee/flan-t5-base-squad2": {"architectures": ["T5ForQuestionAnswering"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "eqhylxx/full-vicuna-160m": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "Ashypaws/DialoGPT-medium-Kitaibot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "NHStudios/DialoGPT-small-jake": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ZipperXYZ/DialoGPT-medium-TheWorldMachineExpressive2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "IIC/mt5-spanish-mlsum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "mattymchen/gense-base-plus": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "DAMO-NLP/SeqGPT-560M": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "AMHR/T5-for-Adversarial-Paraphrasing": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Blizzchor/DialoGPT-medium-HarryBotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "brianveebee/DialoGPT-medium-bender": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "YTTD/DialoGPT-medium-keiji": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Pcik/DialoGPT-medium-Dante": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "mHossain/bangla-para-v3-500000": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama-2-7B-GGUF": {}, "diwas7777/HarryBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "seduerr/t5-small-pytorch": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "felinecity/DioloGPT-small-KaeyaBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "lmsys/vicuna-7b-delta-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "inu-ai/dolly-japanese-gpt-1b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "Dahoas/pythia-125M-static-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "Blizzchor/DialoGPT-medium-QuillLord": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "beomi/KoAlpaca-llama-1-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "patrickNLP/Graphix-3B": {"architectures": ["Model"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "Starry/HELLORUKAS": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "keans/DialoGPT-small-highjacker": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "DoesNoPro/DialoGPT-small-RaidenG": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ThatSkyFox/DialoGPT-medium-whatsapp": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "EnterNameBros/Senko-san-medium-scl": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/CodeLlama-7B-Python-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mrm8488/t5-small-finetuned-quora-for-paraphrasing": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "NonzeroCornet34/DialoGPT-small-hansolo": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "d0rj/rut5-base-summ": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "CAIRE-CedarsSinai/falcon-7b-qlora-chat-support-bot-faq-alzkb-version-2": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "el-profesor/code_t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Soumyajit1008/DialoGPT-small-harryPotterssen": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "malteos/bloom-1b5-clp-german": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_layer": 24}, "yesuns/DialoGPT-small-yesun": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Stevo/DiagloGPT-medium-spamton": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Vision-CAIR/vicuna-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/airoboros-33B-gpt4-1-4-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "tanishqvashisht/DialoGPT-small-Joshua": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TurkuNLP/gpt3-finnish-3B": {"architectures": ["BloomModel"], "hidden_size": 2560, "n_head": 32, "n_layer": 32}, "lizhuang144/flan-t5-base-VG-factual-sg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Athena-v1-GGUF": {}, "xxyyy123/test-28b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pastlecry/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "DiscordRequestsAPI/NurDeeps-Bot-2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "channashi/DialoGPT-small-rocket": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ritog/bangla-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Redmond-Puffin-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Shakerlicious/DialoGPT-small-raquelbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-base-jaquad-qag": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "anon8231489123/vicuna-13b-GPTQ-4bit-128g": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jacobmorrison/tk-instruct-small-lora-experiments": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/open-llama-13b-open-instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "cedpsam/chatbot_fr": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Photolens/llama-2-13b-langchain-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "avinashshrangee/DialoGPT-small-Ricky": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "YeungNLP/firefly-llama2-7b-pretrain": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "efederici/it5-efficient-small-fanpage": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 32}, "saikiranmaddukuri/chat_to_sql0.17": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama2-28B-Air03-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 7296, "intermediate_size": 22016, "num_attention_heads": 57, "num_hidden_layers": 40}, "crodri/falcon_aguila_meteocat": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "Narsil/starcoder-gptq": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "CobraMamba/mamba-gpt-3b-v4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "YeungNLP/firefly-llama2-13b-pretrain": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/airoboros-l2-7b-gpt4-1.4.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DecafNosebleed/DialoGPT-small-ScaraBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "yazdipour/text-to-sparql-t5-small-qald9": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "ClassCat/gpt2-base-french": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/airoboros-33B-GPT4-m2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "quantumaikr/KoreanLM-1.5b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 1024, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "toyfreak/DialoGPT-small-addy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "spursyy/mT5_multilingual_XLSum_rust": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lengoctuong/gpt2-finetuned-chatbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "KnutJaegersberg/megatron-gpt2-345m-evol_instruct_v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24}, "zkdtckk/falcon40-instruct-qlora-tta-v1": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "TheBloke/Nous-Hermes-13B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Nous-Hermes-Llama2-GGML": {}, "IkariDev/Athena-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/llama-2-13B-German-Assistant-v2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "cahya/gpt2-large-indonesian-522M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "VietAI/envit5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "kam1run/DialoGPT-large-kami": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "uukuguy/speechless-codellama-dolphin-orca-platypus-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "aluserhuggingface/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/gpt4-x-vicuna-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Pcik/DialoGPT-medium-Ruby": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/LLaMA-30b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "sdadas/polish-gpt2-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "ahxt/llama2_xs_460M_experimental": {"architectures": ["LlamaForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "lemon234071/t5-base-Chinese": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "4bit/pyg-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "squarelike/Gugugo-koen-1.3B-V1.0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "lvwerra/t5-imdb": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "psymon/KoLlama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Maxwere/DiabloGPT-medium-maxbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "nafisehNik/mt5-persian-summary": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "nams/nams-bot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-esquad-qag": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "mattbit/gpt2wb": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ghazikhanihamed/TooT-PLM-P2S": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 48}, "lonewanderer27/YoshinoriBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "VinVanGogh/Llama-2-7b-Aixiety-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "GroNLP/gpt2-medium-italian-embeddings": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "IDEA-CCNL/Randeng-T5-784M-QA-Chinese": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "kingbri/airo-llongma-2-13B-16k-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lvwerra/starcoderbase-gsm8k": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "mofawzy/gpt2-arabic-sentence-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-itquad-qag": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "sharpbai/Llama-2-13b-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lomahony/eleuther-pythia70m-hh-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "Salesforce/codet5-large-ntp-py": {"architectures": ["T5WithLMHeadModel"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Samantha-1.11-CodeLlama-34B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "Lenza/DialoGPT-medium-Kobayashi": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "davidviriato/DialoGPT-small-joshua": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Shakerlicious/DialoGPT-small-descentbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TurkuNLP/gpt3-finnish-xl": {"architectures": ["BloomModel"], "hidden_size": 2064, "n_head": 24, "n_layer": 24}, "TheBloke/starcoderplus-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "TheBloke/Airoboros-L2-7B-2.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Mikivis/gpt2-large-lora-sft1": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "gagan3012/k2t": {"architectures": ["T5WithLMHeadModel"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "MerlynMind/merlyn-education-safety": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "explosion-testing/refined-web-model-test": {"architectures": ["RWForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "explosion-testing/falcon-no-parallel-attn-test": {"architectures": ["RWForCausalLM"], "hidden_size": 32, "n_head": 4, "n_layer": 5}, "Marxav/frpron": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "AmbricJohnson5888/claura": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/CodeLlama-7B-Instruct-GGUF": {}, "felinecity/DioloGPT-small-LisaBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-frquad-qag": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "RobiKenobi/DialoGPT-medium-pete": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Vicuna-13B-CoT-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/airoboros-33B-gpt4-1.4-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "SEBIS/code_trans_t5_base_code_documentation_generation_java_multitask": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "retrieva-jp/t5-base-medium": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "elinas/chronos-13b-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "abhinavkulkarni/meta-llama-Llama-2-7b-chat-hf-w4-g128-awq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Luban-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "uer/t5-base-chinese-cluecorpussmall": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ClueAI/ChatYuan-large-v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "helenai/gpt2-ov": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "fireballoon/baichuan-vicuna-chinese-7b-gptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Trelis/Llama-2-7b-chat-hf-hosted-inference-8bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Starry/KARENTRIES": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "umm-maybe/SportsFanGhost": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/airoboros-13B-gpt4-1.4-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TabbyML/StarCoder-1B": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "TFLai/Nova-13B-50-step": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Mikivis/gpt2-large-lora-sft2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BlackSamorez/falcon-40b-tiny-testing": {"architectures": ["RWForCausalLM"], "hidden_size": 256, "n_head": 4, "n_layer": 2}, "Rocketknight1/tiny-random-falcon-40b": {"architectures": ["FalconForCausalLM"], "hidden_size": 1024, "num_attention_heads": 128, "num_hidden_layers": 2}, "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGML": {}, "TheBloke/Zarafusionex-1.1-L2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmqg/t5-large-squad-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "casperhansen/falcon-7b-awq": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "Azure99/blossom-v2-llama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DeepESP/gpt2-spanish-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "StudentLLM/Alpagasus-2-13b-QLoRA-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Weni/WeniGPT": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "niicovila/llama-v2-tst-law": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Undi95/CreativityEngine": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "DB13067/Peterbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "EleutherAI/pythia-12b-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "allenai/tulu-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/airoboros-l2-13b-gpt4-m2.0-GGML": {}, "TheBloke/Griffin-3B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "imthanhlv/vigpt2medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "m3hrdadfi/gpt2-persian-qa": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/MythoMax-L2-Kimiko-v2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ppn/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-base-ruquad-qag": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Firefly-Llama2-13B-v1.2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "persiannlp/mt5-large-parsinlu-opus-translation_fa_en": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "simple2312/DialoGPT-Twice": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "declare-lab/flan-alpaca-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "ChanceFocus/finma-7b-nlp": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "osunlp/attrscore-flan-t5-xl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "likenneth/honest_llama2_chat_7B": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Hugherinit/hi": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "vaibhav9/GPT2-qa": {"architectures": ["GPT2ModelForQuestionAnswering"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "st3rl4nce/t5-small-finetuned-pubmed": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "uonlp/okapi-ro-llama": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ThomasNLG/t5-weighter_cnndm-en": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "google/t5-11b-ssm-tqa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "lizhuang144/flan-t5-small-VG-factual-sg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "hyunjae/skt-kogpt2-kullm-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Mirage-Studio/llama-gaan-2-7b-chat-hf-dutch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/LosslessMegaCoder-Llama2-7B-Mini-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lmqg/t5-small-squad-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "castorini/doc2query-t5-large-msmarco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/manticore-13b-chat-pyg-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "22h/open-cabrita3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "alzoubi36/priva_t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/vicuna-7B-v0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/falcon-7b-instruct-GGML": {}, "Rozi05/QuoteVibes_Model_Trained": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Tidum/DialoGPT-large-Michael": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "valhalla/t5-small-qg-prepend": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "lmqg/t5-large-squad-qag": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "abhiramtirumala/DialoGPT-sarcastic": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Salesforce/dialogstudio-t5-base-v1.0": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "allenai/unifiedqa-v2-t5-base-1363200": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "kleinay/qanom-seq2seq-model-joint": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "puugz/DialoGPT-small-spiderman": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "UrukHan/t5-russian-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "HuggingFaceH4/tiny-random-LlamaForSeqClass": {"architectures": ["LlamaForSequenceClassification"], "hidden_size": 16, "intermediate_size": 64, "num_attention_heads": 4, "num_hidden_layers": 2}, "JosephusCheung/Qwen-LLaMAfied-7B-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Abzu/orca-mini-v3-70b-gptq-q4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "wnic00/t5-small-finetune-bilingual-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "ChukSamuels/DialoGPT-small-Dr.FauciBot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "macavaney/doc2query-t5-base-msmarco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "nlp-waseda/comet-t5-base-japanese": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "stjiris/t5-portuguese-legal-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Icaruas/V2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "imxly/t5-pegasus": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "stefan-it/german-gpt2-larger": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "noahkim/KoT5_news_summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "hoskinson-center/proofGPT-v0.1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "TheBloke/WizardMath-7B-V1.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "FieldSu/distil_student_24": {"architectures": ["RWForCausalLM"], "hidden_size": 1136, "n_head": 71, "n_layer": 8}, "shyamsn97/Mario-GPT2-700-context-length": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "dgnk007/eagle": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "sharpbai/Llama-2-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "jackyv/DialoGPT-small-pinocchio": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "felinecity/DioloGPT-small-KaeyaBot2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "toyfreak/DialoGPT-small-shy": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "chavinlo/alpaca-13b": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "georgesung/open_llama_7b_qlora_uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ostorc/rick-sanchez-chatbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "polymath707/llama-2-7b-miniguanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "KBlueLeaf/guanaco-7b-leh-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Chronos-Hermes-13B-v2-GGML": {}, "approach0/mathy-vicuna-13B-FFT-phase2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gorilla-llm/gorilla-7b-hf-delta-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "j5ng/kullm-5.8b-GPTQ-8bit": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "bitadin/checkpoint-230167": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "NekoPunchBBB/Llama2-13b-hf-Open-Platypus-QLoRA-att": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mrm8488/t5-small-finetuned-wikiSQL": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "ozcangundes/T5-base-for-BioQA": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "AriakimTaiyo/gpt2-chat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_layer": 36}, "TheBloke/WizardLM-13B-V1.2-GGML": {}, "TheBloke/Trurl-2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ccore/opt-125-smart-test": {"architectures": ["OPTForCausalLM"], "hidden_size": 768, "num_attention_heads": 12, "num_hidden_layers": 12}, "James-WYang/BigTranslate": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Trelis/Llama-2-7b-chat-hf-function-calling": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Wikidepia/IndoT5-base-paraphrase": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "csebuetnlp/mT5_m2m_crossSum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "seanmor5/tiny-llama-test": {"architectures": ["LlamaForCausalLM"], "hidden_size": 32, "intermediate_size": 64, "num_attention_heads": 2, "num_hidden_layers": 2}, "explosion-testing/refined-web-model-new-decoder-test": {"architectures": ["RWModel"], "hidden_size": 256, "n_head": 4, "n_layer": 5}, "jondurbin/airocoder-34b-2.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "lmqg/t5-base-squad-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PORTULAN/gervasio-ptpt-base": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "UWB-AIR/barticzech-1.0": {"architectures": ["MBartForConditionalGeneration"], "d_model": 1024, "num_hidden_layers": 12}, "TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Voicelab/trurl-2-7b-8bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Llama-2-13B-chat-GGUF": {}, "VietAI/vit5-base-vietnews-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmqg/t5-small-squad-ae": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "retrieva-jp/t5-base-short": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "grammarly/coedit-xxl": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "heack/HeackMT5-ZhSum100k": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/LLaMA-13b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TFMC/ELYZA-japanese-Llama-2-7b-instruct-GPTQ-4bit-64g": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mxmax/Chinese_Chat_T5_Base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "elinas/chronos-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "kajdun/iubaris-13b-v3_GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jmeadows17/MathT5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Kimiko-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "nlp-waseda/gpt2-small-japanese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "rshrott/description-together-ai": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "noah-ai/mt5-base-question-generation-vi": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "AI4PD/ZymCTRL": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "bitadin/gpt-4-long-titles-v2-flan-t5-base-llm-12": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "shorthillsai/flan-t5-large-absa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/CodeLlama-13B-oasst-sft-v10-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "prithivida/active_to_passive_styletransfer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "lcw99/t5-large-korean-text-summary": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "EleutherAI/pythia-1.4b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "sdadas/polish-gpt2-large": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": 5120, "n_layer": 36}, "uonlp/okapi-vi-bloom": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "cenkersisman/gpt2-turkish-900m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "IlyaGusev/rugpt_large_turbo_instructed": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "Waterhorse/chessgpt-base-v1": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "jondurbin/spicyboros-13b-2.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "echarlaix/t5-small-openvino": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "mrm8488/santacoder-finetuned-the-stack-bash-shell": {"architectures": ["GPT2LMHeadCustomModel"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "ckip-joint/bloom-3b-zh": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "Dawnstarhunter/DialoGPT-medium-Eveline": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "lmqg/t5-base-squad-ae": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "mesolitica/finetune-translation-t5-small-standard-bahasa-cased-v2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "liuhaotian/LLaVA-7b-delta-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yzhuang/autotree_llama_small_snxor_l1_2_vit": {"architectures": ["LlamaForAutoTree"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 6}, "mrm8488/t5-base-finetuned-wikiSQL-sql-to-en": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "aleksickx/llama-7b-hf": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "yongzx/pythia-70m-sft-hh": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "sonoisa/t5-base-english-japanese": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "BramVanroy/Llama-2-13b-chat-dutch": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Bhuvana/t5-base-spellchecker": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "PlanTL-GOB-ES/gpt2-base-bne": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "lmqg/mt5-small-jaquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Mirage-Studio/llama-gaan-2-7b-chat-hf-dutch-epoch-5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "microsoft/DialogRPT-human-vs-rand": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_layer": 24}, "aubmindlab/aragpt2-mega": {"architectures": ["GPT2LMHeadModel"], "intermediate_size": 6144, "n_embd": 1536, "n_head": 24, "n_inner": null, "n_layer": 48}, "liyuesen/druggpt": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "conceptofmind/Hermes-LLongMA-2-7b-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/scarlett-33B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/EverythingLM-13b-V2-16K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sartmis1/starcoder-v2-openapi-special-tokens": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "TheBloke/Phind-CodeLlama-34B-v1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "TheBloke/Yarn-Llama-2-7B-64K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Dolphin-Llama-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "kfkas/Legal-Llama-2-ko-7b-Chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Ichsan2895/Merak-7B-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "allenai/unifiedqa-v2-t5-base-1251000": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sagawa/ReactionT5-product-prediction": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmqg/mt5-small-jaquad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Narrativa/mT5-base-finetuned-tydiQA-xqa": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "allenai/macaw-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "gagan3012/k2t-new": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "google/t5-efficient-tiny-nl2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 256, "num_heads": 4, "num_layers": 2}, "sam2ai/open_llama_3b_odia_gptq_128_4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "lmqg/mt5-small-dequad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "mrm8488/mT5-small-finetuned-tydiqa-for-xqa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "zjunlp/knowlm-13b-zhixi": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "h2oai/h2ogpt-16k-codellama-13b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mymusise/gpt2-medium-chinese": {"architectures": ["TFGPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "ai-forever/mGPT-13B": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "TinaLiHF/fined-tuned-T5small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/airoboros-l2-7B-gpt4-2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Mihakram/AraT5-base-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "fjungstedt/t5-criteria-text-to-json": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "luqh/ClinicalT5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "h2oai/h2ogpt-16k-codellama-13b-python": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "masakhane/afri-mt5-base": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "YeungNLP/bloom-1b4-zh": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "shekharchatterjee/temp-model-174": {}, "TheBloke/Kimiko-v2-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jeffwan/vicuna-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "kz919/ntk_scaled_open_llama_13b_32k": {"architectures": ["NTKScaledLlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lmqg/t5-base-squad-qg-ae": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "r3dhummingbird/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "camenduru/MiniGPT4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/open-llama-7b-open-instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "MoinFaisal/Llama-2-7b-chat-finetune": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-13B-Instruct-GGUF": {}, "fbellame/llama2-pdf-to-quizz-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "fractalego/fact-checking": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "michelecafagna26/gpt2-medium-finetuned-sst2-sentiment": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Arc53/docsgpt-7b-falcon": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "alenusch/mt5large-ruparaphraser": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "ApoTro/slovak-t5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "microsoft/dolly-v2-7b-olive-optimized": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "huggingtweets/gordonramsay": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "prithivida/formal_to_informal_styletransfer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "model-attribution-challenge/gpt2-xl": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_layer": 48}, "saiful9379/Bangla_GPT2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "deepse/CodeUp-Llama-2-7b-chat-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ziqingyang/chinese-llama-2-13b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ChandlerU11/t5_fine": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/Guanaco-3B-Uncensored-v2-GPTQ": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "mamiksik/T5-commit-message-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "conceptofmind/Yarn-Llama-2-13b-64k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mesolitica/llama-13b-hf-16384-fpf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Sao10K/Stheno-1.2-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "gsarti/it5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Den4ikAI/FRED-T5-XL-interpreter": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "TheBloke/WizardCoder-Guanaco-15B-V1.1-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "seonglae/llama-2-7b-chat-hf-gptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/llama2_7b_chat_uncensored-GGML": {}, "ecosumit/gpt-model": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "allegro/plt5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "cointegrated/rut5-small": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "it5/it5-large-question-answering": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "tscholak/1zha5ono": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "optible/unifiedqa-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "CleverShovel/falcon-7b-instruct-sharded-bf16": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/Pygmalion-13B-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "BlackSamorez/llama-2-tiny-testing": {"architectures": ["LlamaForCausalLM"], "hidden_size": 128, "intermediate_size": 11008, "num_attention_heads": 8, "num_hidden_layers": 2}, "ianagra/Llama-2-7b-ALLM-virtual-sales-assistant": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/KoreanLM-3B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 2048, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "quantumaikr/llama-2-70B-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Deniskin/gpt3_medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "ozcangundes/mt5-small-turkish-summarization": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "EleutherAI/pythia-1b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "flozi00/Llama-2-7b-german-assistant-v3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Mikivis/gpt2-large-lora-stf4": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "AK270802/DialoGPT-small-harrypotter": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "EleutherAI/pythia-12b-deduped-v0": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "EricPeter/Llama-2-multilingual": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Pygmalion-2-7B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "miguelvictor/python-gpt2-large": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_layer": 36}, "h2oai/h2ogpt-16k-codellama-7b-python": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ammarinjtkrbh/llama-2-7b-food-search": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "GroNLP/gpt2-small-dutch": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "pszemraj/opt-350m-email-generation": {"architectures": ["OPTForCausalLM"], "hidden_size": 1024, "num_attention_heads": 16, "num_hidden_layers": 24}, "caffsean/t5-small-finetuned-keyword-to-text-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "lmqg/mt5-small-dequad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "yuyijiong/T5-large-sentiment-analysis-Chinese-MultiTask": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "sonoisa/t5-qiita-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "YeungNLP/firefly-bloom-1b4": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "samwit/koala-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Vicuna-13B-1-3-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Photolens/OpenOrcaxOpenChat-2-13b-langchain-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Ichsan2895/Merak-7B-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "flozi00/Llama-2-7b-german-assistant-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ss1612/loki-chat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "OpenBuddy/openbuddy-falcon-7b-v5-fp16": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "wellecks/llmstep-mathlib4-pythia2.8b": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "dariolopez/llama-2-7b-oasst1-es": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/WizardLM-1.0-Uncensored-CodeLlama-34B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "brad1141/gpt2-finetuned-comp2": {"architectures": ["GPT2ForTokenClassification"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/chronos-hermes-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lizhuang144/flan-t5-large-VG-factual-sg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "nivos/pythia-410m-deduped-finetuned-final-activity-text-10epoch": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "HamidRezaAttar/gpt2-product-description-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/ORCA_LLaMA_70B_QLoRA-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "lmsys/vicuna-13b-delta-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jacobmorrison/tk-instruct-xl-lora-experiments": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "GroNLP/gpt2-small-italian": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "yihsuan/mt5_chinese_small": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "YTTD/DialoGPT-medium-souv2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "arubenruben/ptt5-portuguese-cnn-dailymail-azure-pt-pt": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "localmodels/Llama-2-7B-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "lgaalves/llama-2-13b-chat-platypus": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "it5/it5-large-question-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "psyche/KoT5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Llama2-70B-OASST-SFT-v10-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "deepparag/Aeona": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "lmqg/mt5-small-koquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "lmqg/mt5-small-esquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "NinedayWang/PolyCoder-0.4B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "ConvLab/t5-small-nlu-multiwoz21": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "SIC98/GPT2-python-code-generator": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-itquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "kaiyuy/leandojo-lean4-tacgen-byt5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "usvsnsp/pythia-6.9b-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "PlanTL-GOB-ES/gpt2-large-bne": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_layer": 36}, "jordiclive/flan-t5-11b-summarizer-filtered": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "Jordine/scpoo": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "behnamsh/gpt2_camel_physics": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-esquad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "MerlynMind/merlyn-education-teacher-assistant": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 36}, "mesolitica/llama-7b-hf-16384-fpf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "MatthisHoules/rat-t5-qdmr-grounded-with-db": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "persiannlp/mt5-small-parsinlu-qqp-query-paraphrasing": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "lmqg/mt5-small-koquad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "lmqg/mt5-small-itquad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "openthaigpt/openthaigpt-gpt2-instructgpt-poc-0.0.4": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ChanceFocus/finma-7b-full": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "vivekraina/Llama-2-7b-hf-8bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "dpml/vicuna_mt_450s": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "burberg92/resume_summary": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "Monero/Pygmalion-Metharme-7b-4bit-TopScore": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Icaruas/7bill8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Yarn-Llama-2-13B-64K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "prithivida/passive_to_active_styletransfer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "lmqg/mt5-small-frquad-qg": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "JamesStratford/PLord-bot-DialoGPT-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "yizhangliu/prompt-extend": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lmqg/mt5-small-frquad-ae": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "Beltenebros/DialoGPT-small-PerionOfGaul": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "sominw/rel23_conll": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "mncai/SGPT-5.8B-wiki-mirae-bank_securities-epoch5": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "MickyMike/VulRepair": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ybelkada/t5-11b-sharded": {"architectures": ["T5WithLMHeadModel"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "Einmalumdiewelt/T5-Base_GNAD_MaxSamples": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "digitous/13B-HyperMantis_GPTQ_4bit-128g": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "weqweasdas/hh_rlhf_rm_open_llama_3b": {"architectures": ["LlamaForSequenceClassification"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/WizardMath-13B-V1.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ziqingyang/chinese-alpaca-2-7b-16k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "valhalla/t5-base-squad": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ELiRF/mt5-base-dacsa-es": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "abhitopia/question-answer-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TurkuNLP/gpt3-finnish-large": {"architectures": ["BloomModel"], "hidden_size": 1536, "n_head": 16, "n_layer": 24}, "Abyss-fyf/DialoGPT-small-discord": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/OpenOrca-Platypus2-13B-GGML": {}, "TheBloke/Airoboros-L2-7B-2.1-GGUF": {}, "huggingtweets/googleai": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "it5/it5-base-question-answering": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "woodmtaylor/DialoGPT-medium-Heej": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "kimdwan/t5-base-korean-summarize-LOGAN": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Narrativa/mT5-base-finetuned-tydiQA-question-generation": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "huggingtweets/normmacdonald": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "r3dhummingbird/DialoGPT-medium-neku": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "yhavinga/t5-v1.1-base-dutch-cnn-test": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "kennethhendricks/DialoGPT-medium-jared-hendricks-gen1": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "retrieva-jp/t5-small-long": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/Vigogne-2-7B-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TigerResearch/tigerbot-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Fredithefish/Guanaco-13B-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "consciousAI/question-answering-generative-t5-v1-base-s-q-c": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/open-llama-7B-v2-open-instruct-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mosama/Llama-2-Medical-Merged-LoRA": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "bullmount/quanIta_t5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "YeungNLP/bloomz-396m-zh": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "GreenBitAI/LLaMA-7B-2bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "chgk13/decicoder-1b-openvino-int8": {"architectures": ["DeciCoderForCausalLM"], "hidden_size": 2048, "intermediate_size": 5888, "num_attention_heads": 32, "num_hidden_layers": 20}, "bigscience/bloomz-mt": {"architectures": ["BloomForCausalLM"], "n_layer": 70, "num_attention_heads": 112}, "LarkAI/codet5p-770m_nl2sql_oig": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Linly-AI/Chinese-Falcon-7B": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "ckip-joint/bloom-3b-zh-instruct": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "sgr23/llama2-fine-tuned-dolly-15k-dto": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "edbeeching/gpt2-imdb": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "cardiffnlp/flan-t5-small-tweet-emotion": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/airoboros-7B-gpt4-1.4-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/CodeLlama-7B-GGUF": {}, "TheBloke/Airoboros-c34B-2.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "byeongal/Ko-DialoGPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ismaelfaro/gpt2-poems.en": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "tuner007/t5_abs_qa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "kennethhendricks/DialoGPT-medium-PowPowGaming": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "DunnBC22/flan-t5-base-text_summarization_data": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "zarakiquemparte/hermeslimarp-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "MagicLEMP/llamavocat_13B_mixed_16K": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "4bit/ELYZA-japanese-Llama-2-7b-instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "EnglishVoice/t5-base-us-to-uk-english": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "devanshipatel/t5-gec-english-125k": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "helloollel/vicuna-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "nferroukhi/WizardLM-Uncensored-Falcon-7b-sharded-bf16": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "dacorvo/tiny-random-gpt2-neuronx": {"intermediate_size": 37, "n_embd": 32, "n_head": 4, "n_inner": null, "n_layer": 5}, "JamesStratford/Pidrow-bot-DialoGPT-Large-Feb2023": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "tsuyuan/Llama-2-7b-unit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "OFA-Sys/gsm8k-rft-llama7b2-u13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "uer/gpt2-chinese-ancient": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "YTTD/DialoGPT-medium-safv3": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Neko-Institute-of-Science/LLaMA-65B-HF": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/Spicyboros-13B-2.2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IDEA-CCNL/Randeng-T5-77M-MultiTask-Chinese": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "coreml-projects/Llama-2-7b-chat-coreml": {"architectures": ["LlamaForCausalLM"]}, "oscorrea/scores-lince-sm": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "morzecrew/FRED-T5-RefinedPersonaChat": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "anjakuzev/harry_7": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Mythalion-13B-GGUF": {}, "Kryptone/monikAI": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Luna-AI-Llama2-Uncensored-GGML": {}, "mlabonne/llama-2-7b-miniguanaco": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Hermes-LLongMA-2-7B-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "zlsl/l_erotic_kink_chat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "Sao10K/Stheno-Inverted-1.2-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "castorini/duot5-base-msmarco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "mrm8488/t5-base-finetuned-qasc": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "entropy/gpt2_zinc_87m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "MarkyMarx/DialoGPT-medium-jimmybot2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "stefan-it/secret-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Narrativa/byt5-base-tweet-hate-detection": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3968, "d_model": 1536, "num_heads": 12, "num_layers": 18}, "nicholasKluge/Aira-2-124M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/Samantha-1.11-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "castorini/monot5-large-msmarco": {"architectures": ["T5Model"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "PoloHuggingface/French_grammar_error_corrector": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "cambridgeltl/magic_mscoco": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Gatozu35/tortoise-tts": {"architectures": ["GPT2InferenceModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 30}, "abacusai/Giraffe-v1-delta-13b-scaled-16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "flozi00/Llama-2-13B-german-assistant-v3-4bit-autogptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "HAERAE-HUB/tulu_13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "doc2query/msmarco-14langs-mt5-base-v1": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Maciel/T5Corrector-base-v2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "vilm/vietcuna-3b-v2": {"architectures": ["BloomForCausalLM"], "hidden_size": 2560, "n_head": 32, "n_inner": null, "n_layer": 30}, "TitanML/ct2-int8-falcon-7b-instruct": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "ybelkada/llama-7b-GPTQ-test": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "h2oai/h2ogpt-16k-codellama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TigerResearch/tigerbot-70b-chat-v1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "Supiri/t5-base-conversation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "msterbentz/t5-base-break-high": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "igorktech/rut5-small-chit-chat-intelligent": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "kuleshov/llama-7b-4bit": {"architectures": ["LLaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "hipnologo/gpt2-imdb-finetune": {"architectures": ["GPT2ForSequenceClassification"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "qwopqwop/danbooru-llama-gptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "t-dai-con/gpt-fine-tuned-v2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Platypus2-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "KETI-AIR/ke-t5-base-ko": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "doc2query/all-t5-base-v1": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "GT4SD/multitask-text-and-chemistry-t5-base-standard": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "uer/gpt2-medium-chinese-cluecorpussmall": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "UBC-NLP/AraT5-base-title-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "dsivakumar/text2sql": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "power-greg/super-fast-llm": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": 2048, "n_layer": 4}, "AlexWortega/instruct_rugptMedium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "hiyouga/Llama-2-Chinese-13b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "frank098/llama2-13b-8k-vnf-virtualization": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "OFA-Sys/gsm8k-rft-llama7b-sample100": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "EnterNameBros/Senko-ai-medium": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "PeanutJar/LLaMa-2-PeanutButter_v19_R8-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Sao10K/Medusa-1.1-L2-7B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ChrisVCB/DialoGPT-medium-cmjs": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "indonesian-nlp/gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "persiannlp/mt5-small-parsinlu-squad-reading-comprehension": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "stmnk/codet5-small-code-summarization-python": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "emozilla/LLongMA-2-13b-16k-flash": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bigscience/bloom-petals": {"architectures": ["BloomForCausalLM"], "hidden_size": 14336, "n_head": 112, "n_layer": 70}, "procesaur/gpt2-srlat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "ashwinR/CodeExplainer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Chirayu/nl2pandas": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "OpenBuddy/openbuddy-falcon-7b-v6-bf16": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "swbaek/tulu_65b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 80}, "huggingtweets/wallstreetbets": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Sultannn/gpt2-ft-id-puisi": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "sonoisa/sentence-t5-base-ja-mean-tokens": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sdadas/polish-gpt2-xl": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": 6400, "n_layer": 48}, "sjrhuschlee/flan-t5-large-squad2": {"architectures": ["T5ForQuestionAnswering"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Hnabil/t5-address-standardizer": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Gryphe/MythoLogic-Mini-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Athena-v1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Undi95/MythoMax-L2-Kimiko-v2-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "magnifi/llama-augmented-contextual-2-epoch-6-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "doc2query/msmarco-chinese-mt5-base-v1": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Sakuna/t5_grammar_checker": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Dahoas/pythia-1B-response-full-static-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 16, "num_hidden_layers": 24}, "localmodels/Vicuna-7B-v1.3-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Sao10K/Stheno-1.1-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "mlabonne/drllama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "IlyaGusev/rugpt3medium_sum_gazeta": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "describeai/gemini": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "mojians/E2E-QA-Mining": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "dnagpt/human_gpt2-v1": {"architectures": ["GPT2Model"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "heegyu/WizardVicuna-Uncensored-pythia-160m-deduped": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "maximuslee07/llama-2-7b-rockwell": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DylanJHJ/fidt5-base-nq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "laituan245/molt5-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "DancingIguana/music-generation": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "Qiliang/flan-t5-large-summarization-finetuned-xsum": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/Vicuna-7B-CoT-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "hpcaitech/openmoe-base": {"architectures": ["OpenMoeForCausalLM"], "hidden_size": 768, "intermediate_size": 2048, "num_attention_heads": 12, "num_hidden_layers": 12}, "CalderaAI/13B-Thorns-l2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IlyaGusev/rugpt_medium_turbo_instructed": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "pankajmathur/orca_alpaca_3b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 3200, "intermediate_size": 8640, "num_attention_heads": 32, "num_hidden_layers": 26}, "TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "abacusai/Giraffe-v1-delta-13b-scaled-4": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Huginn-v3-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "bloom-testing/test-bloomd-350m-main": {"architectures": ["BloomModel"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "AI-Sweden/gpt-sw3-356m": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": 4096, "n_layer": 24}, "raymondho/DialoGPT-small-harry": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "TheBloke/airochronos-33B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "TheBloke/OpenChat_v3.2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ahnyeonchan/OpenOrca-AYT-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "stanford-crfm/expanse-gpt2-small-x777": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "doc2query/msmarco-german-mt5-base-v1": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ku-nlp/gpt2-medium-japanese-char": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "llm-blender/gen_fuser_3b": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "lomahony/eleuther-pythia2.8b-hh-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Llama2-22B-GPLATTY-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "grammarly/coedit-xl-composite": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "imuncomfortable/DiabloGPT-small-CocoAtarashi": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "kaiyuy/leandojo-lean3-retriever-tacgen-byt5-small": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3584, "d_model": 1472, "num_heads": 6, "num_layers": 12}, "michaelwzhu/Chinese-LlaMA2-13B-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Xenova/llama2.c-stories110M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 768, "intermediate_size": 2048, "num_attention_heads": 12, "num_hidden_layers": 12}, "Youngwoo9/T5_Pyeongsan": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "literallywood/DialoGPT-small-ekansh": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "jondurbin/spicyboros-7b-2.2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "indobenchmark/indogpt": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "it5/it5-efficient-small-el32-news-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 32}, "mesolitica/finetune-translation-t5-base-standard-bahasa-cased-v2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Den4ikAI/FRED-T5-XL_instructor": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1536, "num_heads": 24, "num_layers": 24}, "mlabonne/gpt2-GPTQ-4bit": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "steerapi/Llama-2-7b-chat-hf-onnx": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Langboat/bloom-1b4-zh": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "neulab/docprompting-codet5-python-doc-retriever": {"architectures": ["BERTScorerForCL"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "AI-Sweden/gpt-sw3-20b": {"architectures": ["GPT2LMHeadModel"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 44}, "syndi-models/article-title-generator": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "vgaraujov/Dummy5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TFLai/Orca-Nova-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "allenai/tk-instruct-11b-def-pos": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "aspis/gpt2-genre-story-generation": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "lcw99/t5-base-korean-paraphrase": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Celestinian/TopicGPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "TheBloke/Redmond-Hermes-Coder-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "abhinavkulkarni/meta-llama-Llama-2-13b-chat-hf-w4-g128-awq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "jypppp/llama-2-7b-manual_GPT_ver2": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Llama-2-7B-32K-Instruct-GGML": {}, "TheBloke/Yarn-Llama-2-7B-128K-GGML": {}, "quantumaikr/KoreanLM-llama-2-7B-finetuned": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "google/t5-xl-ssm-nq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "nikokons/gpt2-greek": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "NYTK/PULI-GPT-3SX": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_hidden_layers": 32}, "Futyn-Maker/rugpt3small_based_on_gpt2-finetuned_teachers_quotes_small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "localmodels/Llama-2-13B-Chat-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "SebastianSchramm/UniNER-7B-all-GPTQ-4bit-128g-actorder_True": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Airoboros-L2-70B-2.1-Creative-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "casperhansen/vicuna-7b-v1.5-awq-gemv": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "IDEA-CCNL/Wenzhong-GPT2-3.5B": {"architectures": ["GPT2LMHeadModel"], "n_embd": 3072, "n_head": 32, "n_inner": 12288, "n_layer": 30}, "antoinelouis/belgpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "atkh6673/DialoGPT-small-trump": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "huggingface-course/mt5-small-finetuned-amazon-en-es": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "malteos/gpt2-xl-wechsel-german": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": 6400, "n_layer": 48}, "KES/caribe-capitalise": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "pszemraj/flan-t5-large-instruct-dolly_hhrlhf": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "Tanmay09516/StableBeluga-7B-sharded-bf16-5GB": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "abhinavkulkarni/codellama-CodeLlama-7b-Python-hf-w4-g128-awq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Spicyboros-7B-2.2-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "huggingtweets/elonmusk": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "BelleGroup/BELLE-7B-2M": {"architectures": ["BloomModel"], "n_inner": null, "n_layer": 30, "num_attention_heads": 32}, "snoop2head/Gomoku-GPT2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 256, "n_head": 4, "n_inner": null, "n_layer": 4}, "AnimusOG/pygmalion-7b-4bit-128g-cuda-2048Token": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/airoboros-l2-7B-gpt4-m2.0-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Juniplayground/Mist_LLaMA-2-7B-1024_V3": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "DataLinguistic/DataLinguistic-34B-V1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "erikycd/chatbot_hadita": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "medicalai/ClinicalGPT-base-zh": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "TheBloke/orca_mini_v2_13b-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "NIRVANA/T5_academic_paraphraser": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "josmunpen/mt5-small-spanish-summarization": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "shahp7575/gpt2-horoscopes": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "yihsuan/best_model_0427_small_long": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "malteos/bloom-6b4-clp-german-oasst-v0.1": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_layer": 30}, "openllmplayground/openalpaca_7b_700bt_preview": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Gaivoronsky/ruGPT-3.5-13B-fp16": {"architectures": ["GPT2LMHeadModel"], "n_embd": 5120, "n_head": 40, "n_inner": null, "n_layer": 40}, "universeTBD/astrollama": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "gorkemgoknar/gpt2-small-turkish": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "huggingtweets/joejoinerr": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Prarabdha/T5-Transformer-RickBot": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "beomi/kollama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "mohammadtaghizadeh/flan-t5-base-imdb-text-classification": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "nicholasKluge/Aira-Instruct-774M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1280, "n_head": 20, "n_inner": null, "n_layer": 36}, "bhenrym14/airoboros-7b-gpt4-1.4.1-lxctx-PI-16384-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Alireza1044/michael_bert_lm": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "shibing624/gpt2-dialogbot-base-chinese": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 10}, "mesolitica/finetune-summarization-ms-t5-base-standard-bahasa-cased": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lmqg/flan-t5-large-squad-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "circulus/alpaca-7b": {"architectures": ["LlaMAForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "reeducator/vicuna-13b-free": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "flozi00/Llama-2-13b-german-assistant-v6-4bit-autogptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "SasnayaLetovka/tinkoff-zhientaev-model": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "mesolitica/t5-base-standard-bahasa-cased": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "EllyPony/flutterbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "pszemraj/flan-t5-xl-grammar-synthesis": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 5120, "d_model": 2048, "num_heads": 32, "num_layers": 24}, "jinxuewen/vicuna-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "fireballoon/baichuan-llama-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "scural/arxiv_model": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "Undi95/CodeEngine": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Codexister/DialoGPT-medium-KafkaBotV1": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "google/t5-xxl-ssm-nq": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 10240, "d_model": 4096, "num_heads": 64, "num_layers": 24}, "uer/gpt2-chinese-couplet": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "nicholasKluge/Aira-Instruct-355M": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "HIT-SCIR/huozi-7b-sft": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "NousResearch/CodeLlama-13b-Instruct-hf-flash": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Enno-Ai/vigogne2-enno-13b-sft-lora-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "sonoisa/t5-base-japanese-article-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "Kyrmasch/t5-kazakh-qa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "TheBloke/airoboros-13b-gpt4-1.4-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TheBloke/Kimiko-13B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "arya555/vicuna-7b-v1.5-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Geo/gpt2_custom_c_q_and_a": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "laituan245/molt5-small-smiles2caption": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "cloudqi/cqi_brain_memory_summarizer_large_pt_v0": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "ybelkada/bloom-1b7-8bit": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "snipaid/snip-igel-500-v2-adapter-merged": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_layer": 30}, "TabbyML/SantaCoder-1B": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 2048, "n_head": 16, "n_inner": 8192, "n_layer": 24}, "TheBloke/Guanaco-33B-SuperHOT-8K-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 60}, "hanseokhyeon/kullm-polyglot-5.8b-v2-GPTQ": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 16, "num_hidden_layers": 28}, "CAIRE-CedarsSinai/falcon-7b-qlora-chat-support-bot-faq-alzkb-version-1": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "pranavpsv/genre-story-generator-v2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "nandakishormpai/t5-small-machine-articles-tag-generation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "ITG/DialoGPT-medium-spanish-chitchat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "4bit/falcon-7b-instruct-GPTQ": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "OpenBuddy/openbuddy-openllama-7b-v5-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "papahawk/keya-560m": {"architectures": ["BloomForCausalLM"], "n_inner": null, "n_layer": 24, "num_attention_heads": 16}, "abhinavkulkarni/tiiuae-falcon-40b-instruct-w4-g128-awq": {"architectures": ["RWForCausalLM"], "hidden_size": 8192, "n_head": 128, "n_layer": 60}, "funstoryai/immersiveL-exp": {"architectures": ["BloomForCausalLM"], "hidden_size": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "Benson/llama-2-7b-miniguanaco-hf": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "clancystudios/DialoGPT-medium-Morty": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "huggingtweets/realdonaldtrump": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "charanhu/text_to_sql_2": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "beomi/kollama-13b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "IDEA-CCNL/Ziya-LLaMA-13B-v1.1": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "nicholasKluge/Aira-Instruct-PT-1B7": {"architectures": ["BloomForCausalLM"], "hidden_size": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "TheBloke/Llama2-22B-Daydreamer-v3-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 6656, "intermediate_size": 17920, "num_attention_heads": 52, "num_hidden_layers": 40}, "yongzx/pythia-160m-sft-hh": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 768, "intermediate_size": 3072, "num_attention_heads": 12, "num_hidden_layers": 12}, "h2oai/h2ogpt-16k-codellama-34b-python": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "nedima68/author_articles_GPT2_textgen_TR": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "IronChef/MascotAI_Open_LLaMA_FINAL": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "unionai/pythia-1B-deduped-wikipedia-8bit": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2560, "intermediate_size": 10240, "num_attention_heads": 32, "num_hidden_layers": 32}, "Chirayu/nl2cql": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Nous-Puffin-70B-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/Llama-2-70B-Orca-200k-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_hidden_layers": 80}, "TheBloke/Llama-2-70B-chat-GGUF": {}, "sartmis1/CodeLlama-34b-instruct-openapi": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "flax-community/bengali-t5-base": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "csebuetnlp/mT5_m2o_hindi_crossSum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "huggingtweets/fabrizioromano": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "yshen99/ZhiGuoLiZheng-GPT2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "malalejandra/putinspeaks": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "Intel/fid_flan_t5_base_nq": {"architectures": ["FusionInDecoderForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "sjrhuschlee/flan-t5-base-mnli": {"architectures": ["T5ForSequenceClassification"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "TheBloke/Codegen25-7B-mono-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "frank098/starcoder-vyatta": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "Xenova/llama2.c-stories42M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 512, "intermediate_size": 1376, "num_attention_heads": 8, "num_hidden_layers": 8}, "flozi00/Llama-2-13b-german-assistant-v5": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Andrei-Alex/Fine-Tuned-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/vicuna-7B-1.1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "sharpbai/alpaca-7b-merged": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Clakmann/t5-base-Clakmann-thesis": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "RuterNorway/Llama-2-13b-chat-norwegian-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "osieosie/bloom-560m-4bit": {"architectures": ["BloomForCausalLM"], "hidden_size": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "paulowoicho/t5-podcast-summarisation": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "liujch1998/rainier-large": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "gsdas/qct5": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "nicholasKluge/Aira-Instruct-1B5": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1600, "n_head": 25, "n_inner": null, "n_layer": 48}, "kajdun/iubaris-13b-v3_GGML": {}, "csebuetnlp/mT5_m2o_english_crossSum": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "dehio/german-qg-t5-quad": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "emil2000/dialogpt-for-french-language": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "khalidsaifullaah/bengali-lyricist-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "thinhda/chatbot": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "Finnish-NLP/llama-7b-finnish": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "ehartford/WizardLM-7B-V1.0-Uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ": {"architectures": ["GPTBigCodeForCausalLM"], "n_embd": 6144, "n_head": 48, "n_inner": 24576, "n_layer": 40}, "DUOMO-Lab/TransGPT-v0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Platypus2-70B-Instruct-GGUF": {}, "lmqg/t5-large-squad-qg-ae": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "rubentito/hivt5-base-mpdocvqa": {"architectures": ["HiVT5"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "cosimoiaia/Loquace-70m": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "metamyth/jennyNew": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "AlexWortega/LLama2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "totally-not-an-llm/AlpacaCielo2-7b-8k": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/orca_mini_v3_7B-GGML": {}, "zjunlp/knowlm-13b-base-v1.0": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ticoAg/gpt2-tigerbot-pt-zh": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "akshat3492/mT5": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TheBloke/Falcon-180B-Chat-GGUF": {}, "unicamp-dl/mt5-base-mmarco-v2": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "malteos/gpt2-wechsel-german-ds-meg": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": 3072, "n_layer": 12}, "phpaiola/ptt5-base-summ-temario": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "mesolitica/finetune-translation-t5-super-tiny-standard-bahasa-cased": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 256, "num_heads": 6, "num_layers": 2}, "ademfatnassi/bonjourGPT-small": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "pr1me/llama2_13b_eros_instruct": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Xenova/llama2.c-stories15M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 288, "intermediate_size": 768, "num_attention_heads": 6, "num_hidden_layers": 6}, "sekarmulyani/gpt2-ulasan-beauty-products-gen": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "akhooli/gpt2-small-arabic-poetry": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "mrm8488/spanish-t5-small-sqac-for-qa": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "flozi00/falcon-7b-german-assistant-v2": {"architectures": ["RWForCausalLM"], "hidden_size": 4544, "n_head": 71, "n_layer": 32}, "TheBloke/llama-2-13B-chat-limarp-v2-merged-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ticoAg/gpt2-tiger-sft-zh": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "thiagomf/Llama-2-7b-hf-sharded-bf16-1GB": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "THUMT/mGPT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_layer": 24, "n_head": 16, "n_inner": 4096}, "lmqg/flan-t5-base-squad-qg": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-700bt": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "TheBloke/Phind-CodeLlama-34B-Python-v1-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 8192, "intermediate_size": 22016, "num_attention_heads": 64, "num_hidden_layers": 48}, "arogov/llama2_13b_chat_uncensored": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "ai-forever/mGPT-1.3B-bulgarian": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2048, "n_head": 16, "n_inner": null, "n_layer": 24}, "davesoma/SageBeluga13": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "pssubitha/llama-2-7b-sales-force-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "PyaeSoneK/pythia_70m_legalQA": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 512, "intermediate_size": 2048, "num_attention_heads": 8, "num_hidden_layers": 6}, "hidude562/OpenMusenet-2.1-L": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1024, "n_head": 16, "n_inner": null, "n_layer": 24}, "abeiler/huggingface-goatLora-goatV9-testData-morePushes": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "abinayam/gpt-2-tamil": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "persiannlp/mt5-base-parsinlu-squad-reading-comprehension": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "pierreguillou/t5-base-qa-squad-v1.1-portuguese": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "lchaloupsky/czech-gpt2-oscar": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_layer": 12}, "OpenHust/viet-gpt2": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "tiansz/ChatYuan-7B-merge": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "voidful/llama-v2-unit-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "taaredikahan23/Llama-2-7b-chat-finetune": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "deutsche-telekom/mt5-small-sum-de-en-v1": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "hetpandya/t5-small-tapaco": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "sunhao666/chi-sum2": {"architectures": ["T5Model"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "smartik/mt5-small-finetuned-gec-0.2": {"architectures": ["MT5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "PORTULAN/gervasio-ptbr-base": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "clibrain/Llama-2-13b-ft-instruct-es-gptq-4bit": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "soketlabs/bhasha-7b-2k-hi": {"architectures": ["MPTForCausalLM"], "d_model": 4096}, "codefuse-ai/CodeFuse-13B": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 5120, "intermediate_size": 20480, "num_attention_heads": 40, "num_hidden_layers": 40}, "Sentdex/GPyT": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "it5/it5-large-news-summarization": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2816, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "FredZhang7/distilgpt2-stable-diffusion": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 6}, "Rostlab/ProstT5_fp16": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 16384, "d_model": 1024, "num_heads": 32, "num_layers": 24}, "approach0/mathy-vicuna-13B-FFT": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "lighteternal/gpt2-finetuned-greek": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "stanford-crfm/battlestar-gpt2-small-x49": {"architectures": ["GPT2LMHeadModel"], "n_embd": 768, "n_head": 12, "n_inner": null, "n_layer": 12}, "stacked-summaries/flan-t5-small-stacked-samsum-1024": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 1024, "d_model": 512, "num_heads": 6, "num_layers": 8}, "TigerResearch/tigerbot-7b-base-v1": {"architectures": ["BloomForCausalLM"], "hidden_size": 4096, "n_head": 32, "n_inner": null, "n_layer": 30}, "Chang-Su/llama-2-13b-chat-ko": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "Clakmann/t5-base-Clakmann-thesis-epoch10": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "yekaraoglann/results": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 512, "num_heads": 8, "num_layers": 6}, "bitadin/gpt-4-medium-titles-v2-flan-t5-base-llm-6": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 2048, "d_model": 768, "num_heads": 12, "num_layers": 12}, "google/t5_11b_trueteacher_and_anli": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 65536, "d_model": 1024, "num_heads": 128, "num_layers": 24}, "TaylorAI/Flash-Llama-30M": {"architectures": ["LlamaForCausalLM"], "hidden_size": 384, "intermediate_size": 1024, "num_attention_heads": 12, "num_hidden_layers": 4}, "flax-community/t5-base-wikisplit": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 3072, "d_model": 768, "num_heads": 12, "num_layers": 12}, "razent/SciFive-large-Pubmed_PMC": {"architectures": ["T5ForConditionalGeneration"], "d_ff": 4096, "d_model": 1024, "num_heads": 16, "num_layers": 24}, "inkoziev/rugpt_chitchat": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "lomahony/eleuther-pythia410m-hh-sft": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 1024, "intermediate_size": 4096, "num_attention_heads": 16, "num_hidden_layers": 24}, "TheBloke/Vicuna-13B-v1.3-German-GPTQ": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "emozilla/LLongMA-2-13b-storysummarizer": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "yongzx/pythia-1b-sft-hh": {"architectures": ["GPTNeoXForCausalLM"], "hidden_size": 2048, "intermediate_size": 8192, "num_attention_heads": 8, "num_hidden_layers": 16}, "TheBloke/airoboros-13b-gpt4-1.4-SuperHOT-8K-fp16": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "heegyu/llama-2-ko-7b-chat": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "flozi00/Llama-2-7b-german-assistant-v3-4bit-autogptq": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "zarakiquemparte/zararp-l2-7b": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}, "Sao10K/Stheno-1.3-L2-13B": {"architectures": ["LlamaForCausalLM"], "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_hidden_layers": 40}, "TsinghuaAI/CPM-Generate": {"architectures": ["GPT2LMHeadModel"], "n_embd": 2560, "n_head": 32, "n_inner": null, "n_layer": 32}, "AlexWortega/instruct_rugptlarge": {"architectures": ["GPT2LMHeadModel"], "n_embd": 1536, "n_head": 16, "n_inner": null, "n_layer": 24}, "tatsu-lab/alpaca-7b-wdiff": {"architectures": ["LlamaForCausalLM"], "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_hidden_layers": 32}}